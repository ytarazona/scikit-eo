<<<<<<< Updated upstream
{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to","text":""},{"location":"#scikit-eo-a-python-package-for-remote-sensing-tools","title":"scikit-eo: A Python package for Remote Sensing Tools","text":""},{"location":"#links-of-interest","title":"Links of interest:","text":"<ul> <li>GitHub repo: https://github.com/ytarazona/scikit-eo</li> <li>Documentation: https://ytarazona.github.io/scikit-eo/</li> <li>PyPI: https://pypi.org/project/scikeo/</li> <li>Notebooks examples: https://github.com/ytarazona/scikit-eo/tree/main/examples</li> <li>Google Colab examples: https://github.com/ytarazona/scikit-eo/tree/main/examples</li> <li>Free software: Apache 2.0</li> </ul>"},{"location":"#introduction","title":"Introduction","text":"<p>Now a day, remotely sensed data has increased dramatically. Microwaves and optical images with different spatial and temporal resolutions are available and are using to monitor a variety of environmental issues such as deforestation, land degradation, crop classifications, among other. Although there are efforts (i.e., Python packages, forums, communities, etc.) to make available line-of-code tools for pre-processing, processing and analysis of satellite imagery, there is still a gap that needs to be filled. In other words, too much time is still spent by many users in developing Python lines of code. Algorithms for mapping land degradation through linear trend of vegetation indices, fusion optical and radar images to classify vegetation cover, calibration of machine learning algorithms, among others, are not available yet.</p> <p>Therefore, scikit-eo is a Python package that provides tools for remote sensing. This package was developed to fill the gaps in remotely sensed data processing tools. Most of the tools are based on scientific publications, and others are useful algorithms that will allow processing to be done in a few lines of code. With these tools, the user will be able to invest time in analyzing the results of their data and not spend time on elaborating lines of code, which can sometimes be stressful.</p>"},{"location":"#tools-for-remote-sensing","title":"Tools for Remote Sensing","text":"Name of functions Description <code>mla</code> Machine Learning <code>calmla</code> Calibrating supervised classification in Remote Sensing <code>rkmeans</code> K-means classification <code>calkmeans</code> This function allows to calibrate the kmeans algorithm. It is possible to obtain the best k value and the best embedded algorithm in kmeans. <code>pca</code> Principal Components Analysis <code>atmosCorr</code> Atmospheric Correction of satellite imagery <code>deepLearning</code> Deep Learning algorithms <code>linearTrend</code> Linear trend is useful for mapping forest degradation or land degradation <code>fusionrs</code> This algorithm allows to fusion images coming from different spectral sensors (e.g., optical-optical, optical and SAR or SAR-SAR). Among many of the qualities of this function, it is possible to obtain the contribution (%) of each variable in the fused image <code>sma</code> Spectral Mixture Analysis - Classification sup-pixel <code>tassCap</code> The Tasseled-Cap Transformation <p>You will find more algorithms!.</p>"},{"location":"#installation","title":"Installation","text":"<p>To use scikit-eo it is necessary to install it. There are two options:</p>"},{"location":"#1-from-pypi","title":"1. From PyPI","text":"<pre><code>pip install scikeo\n</code></pre>"},{"location":"#2-installing-from-source","title":"2. Installing from source","text":"<p>It is also possible to install the latest development version directly from the GitHub repository with:</p> <pre><code>pip install git+https://github.com/ytarazona/scikit-eo\n</code></pre>"},{"location":"atmosCorr/","title":"atmosCorr module","text":""},{"location":"atmosCorr/#module-atmoscorr","title":"module <code>atmosCorr</code>","text":""},{"location":"atmosCorr/#class-atmoscorr","title":"class <code>atmosCorr</code>","text":"<p>Atmospheric Correction in Optical domain </p> <p></p>"},{"location":"atmosCorr/#method-__init__","title":"method <code>__init__</code>","text":"<pre><code>__init__(path, nodata=-99999)\n</code></pre> <p>Parameter: </p> <p>path: String. The folder in which the satellite bands are located. This images could be Landsat  Collection 2 Level-1. For example: path = r'/folder/image/raster'.  </p> <p>nodata: The NoData value to replace with -99999. </p> <p></p>"},{"location":"atmosCorr/#method-dos","title":"method <code>DOS</code>","text":"<pre><code>DOS(sat='LC08', mindn=None)\n</code></pre> <p>The Dark Object Subtraction Method was proposed by Chavez (1988). This image-based  atmospheric correction method considers absolutely critical and valid the existence  of a dark object in the scene, which is used in the selection of a minimum value in  the haze correction. The most valid dark objects in this kind of correction are areas  totally shaded or otherwise areas representing dark water bodies. </p> <p>Parameters:</p> <ul> <li> <p><code>sat</code>:  Type of Satellite. It could be Landsat-5 TM, Landsat-8 OLI or Landsat-9 OLI-2. </p> </li> <li> <p><code>mindn</code>:  Min of digital number for each band in a list. </p> </li> </ul> <p>Return: An array with Surface Reflectance values with 3d, i.e. (rows, cols, bands). </p> <p>References:      - Chavez, P.S. (1988). An Improved Dark-Object Subtraction Technique for Atmospheric  Scattering Correction of Multispectral Data. Remote Sensing of Envrironment, 24(3), 459-479. </p> <p></p>"},{"location":"atmosCorr/#method-rad","title":"method <code>RAD</code>","text":"<pre><code>RAD(sat='LC08')\n</code></pre> <p>Conversion to TOA Radiance. Landsat Level-1 data can be converted to TOA spectral radiance  using the radiance rescaling factors in the MTL file: </p> <p>L\u03bb = MLQcal + AL  </p> <p>where: </p> <p>L\u03bb = TOA spectral radiance (Watts/(m2srad\u03bcm)) ML = Band-specific multiplicative rescaling factor from the metadata (RADIANCE_MULT_BAND_x, where x is the band number) AL = Band-specific additive rescaling factor from the metadata (RADIANCE_ADD_BAND_x, where x is the band number) Qcal =  Quantized and calibrated standard product pixel values (DN)  </p> <p>Parameters:</p> <ul> <li><code>sat</code>:  Type of Satellite. It could be Landsat-5 TM, Landsat-8 OLI or Landsat-9 OLI-2. </li> </ul> <p>Return: An array with radiance values with 3d, i.e. (rows, cols, bands). </p> <p></p>"},{"location":"atmosCorr/#method-toa","title":"method <code>TOA</code>","text":"<pre><code>TOA(sat='LC08')\n</code></pre> <p>A reduction in scene-to-scene variability can be achieved by converting the at-sensor  spectral radiance to exoatmospheric TOA reflectance, also known as in-band planetary albedo. </p> <p>Equation to obtain TOA reflectance: </p> <p>\u03c1\u03bb\u2032 = M\u03c1*DN + A\u03c1 </p> <p>\u03c1\u03bb = \u03c1\u03bb\u2032/sin(theta) </p> <p>Parameters:</p> <ul> <li><code>sat</code>:  Type of Satellite. It could be Landsat-5 TM, Landsat-8 OLI or Landsat-9 OLI-2. </li> </ul> <p>Return: An array with TOA values with 3d, i.e. (rows, cols, bands). </p> <p>This file was automatically generated via lazydocs.</p>"},{"location":"calkmeans/","title":"calkmeans module","text":""},{"location":"calkmeans/#module-calkmeans","title":"module <code>calkmeans</code>","text":""},{"location":"calkmeans/#function-calkmeans","title":"function <code>calkmeans</code>","text":"<pre><code>calkmeans(\n    image,\n    k=None,\n    algo=('auto', 'elkan'),\n    max_iter=300,\n    n_iter=10,\n    nodata=-99999,\n    **kwargs\n)\n</code></pre> <p>Calibrating kmeans </p> <p>This function allows to calibrate the kmeans algorithm. It is possible to obtain the best 'k' value and the best embedded algorithm in KMmeans.  </p> <p>Parameters:</p> <pre><code> - &lt;b&gt;`image`&lt;/b&gt;:  Optical images. It must be rasterio.io.DatasetReader with 3d.\n\n\n - &lt;b&gt;`k`&lt;/b&gt;:  k This argument is None when the objective is to obtain the best 'k' value.  If the objective is to select the best algorithm embedded in kmeans, please specify a 'k' value.\n\n\n - &lt;b&gt;`max_iter`&lt;/b&gt;:  The maximum number of iterations allowed. Strictly related to KMeans. Please see \n - &lt;b&gt;`https`&lt;/b&gt;: //scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html\n\n\n - &lt;b&gt;`algo`&lt;/b&gt;:  It can be \"auto\" and 'elkan'. \"auto\" and \"full\" are deprecated and they will be  removed in Scikit-Learn 1.3. They are both aliases for \"lloyd\".\n\n\n - &lt;b&gt;`Changed in version 1.1`&lt;/b&gt;:  Renamed \u201cfull\u201d to \u201clloyd\u201d, and deprecated \u201cauto\u201d and \u201cfull\u201d.  Changed \u201cauto\u201d to use \u201clloyd\u201d instead of \u201celkan\u201d.\n\n\n - &lt;b&gt;`n_iter`&lt;/b&gt;:  Iterations number to obtain the best 'k' value. 'n_iter' must be greater than the  number of classes expected to be obtained in the classification. Default is 10.\n\n\n - &lt;b&gt;`nodata`&lt;/b&gt;:  The NoData value to replace with -99999.\n\n\n - &lt;b&gt;`**kwargs`&lt;/b&gt;:  These will be passed to scikit-learn KMeans, please see full lists at: \n - &lt;b&gt;`https`&lt;/b&gt;: //scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html.\n</code></pre> <p>Return: </p> <p>Labels of classification as numpy object with 2d. </p> <p>Note:</p> <p>If the idea is to find the optimal value of 'k' (clusters or classes), k = None as an argument of the function must be put, because the function find 'k' for which the intra-class inertia is stabilized. If the 'k' value is known and the idea is to find the best algorithm embedded in kmeans (that maximizes inter-class distances), k = n, which 'n' is a specific class number, must be put. It can be greater than or equal to 0.  </p> <p>This file was automatically generated via lazydocs.</p>"},{"location":"calmla/","title":"calmla module","text":""},{"location":"calmla/#module-calmla","title":"module <code>calmla</code>","text":""},{"location":"calmla/#class-calmla","title":"class <code>calmla</code>","text":"<p>Calibrating supervised classification in Remote Sensing </p> <p>This module allows to calibrate supervised classification in satellite images through various algorithms and using approaches such as Set-Approach,  Leave-One-Out Cross-Validation (LOOCV), Cross-Validation (k-fold) and  Monte Carlo Cross-Validation (MCCV) </p> <p></p>"},{"location":"calmla/#method-__init__","title":"method <code>__init__</code>","text":"<pre><code>__init__(endmembers)\n</code></pre> <p>Parameter:  </p> <p>endmembers: Endmembers must be a matrix (numpy.ndarray) and with more than one endmember.   Rows represent the endmembers and columns represent the spectral bands.  The number of bands must be equal to the number of endmembers.  E.g. an image with 6 bands, endmembers dimension should be $n*6$, where $n$   is rows with the number of endmembers and 6 is the number of bands   (should be equal).  In addition, Endmembers must have a field (type int or float) with the names   of classes to be predicted.  </p> <p></p>"},{"location":"calmla/#method-cv","title":"method <code>CV</code>","text":"<pre><code>CV(\n    split_data,\n    models=('svm', 'dt'),\n    k=5,\n    n_iter=10,\n    random_state=None,\n    **kwargs\n)\n</code></pre> <p>This module allows to calibrate supervised classification in satellite images  through various algorithms and using Cross-Validation. </p> <p>Parameters:</p> <ul> <li> <p><code>split_data</code>:  A dictionary obtained from the splitData method of this package. </p> </li> <li> <p><code>models</code>:  Models to be used such as Support Vector Machine ('svm'), Decision Tree ('dt'), Random Forest ('rf'), Naive Bayes ('nb') and Neural Networks ('nn'). This parameter can be passed like models = ('svm', 'dt', 'rf', 'nb', 'nn'). </p> </li> <li> <p><code>cv</code>:  For splitting samples into two subsets, i.e. training data and  for testing data. Following Leave One Out Cross-Validation. </p> </li> <li> <p><code>n_iter</code>:  Number of iterations, i.e number of times the analysis is executed. </p> </li> <li> <p><code>**kwargs</code>:  These will be passed to SVM, DT, RF, NB and NN, please see full lists at: </p> </li> <li><code>https</code>: //scikit-learn.org/stable/supervised_learning.html#supervised-learning </li> </ul> <p>Return: </p> <p></p>"},{"location":"calmla/#method-loocv","title":"method <code>LOOCV</code>","text":"<pre><code>LOOCV(split_data, models=('svm', 'dt'), cv=LeaveOneOut(), n_iter=10, **kwargs)\n</code></pre> <p>This module allows to calibrate supervised classification in satellite images  through various algorithms and using Leave One Out Cross-Validation. </p> <p>Parameters:</p> <ul> <li> <p><code>split_data</code>:  A dictionary obtained from the splitData method of this package. </p> </li> <li> <p><code>models</code>:  Models to be used such as Support Vector Machine ('svm'), Decision Tree ('dt'), Random Forest ('rf'), Naive Bayes ('nb') and Neural Networks ('nn'). This parameter can be passed like models = ('svm', 'dt', 'rf', 'nb', 'nn'). </p> </li> <li> <p><code>cv</code>:  For splitting samples into two subsets, i.e. training data and  for testing data. Following Leave One Out Cross-Validation. </p> </li> <li> <p><code>n_iter</code>:  Number of iterations, i.e number of times the analysis is executed. </p> </li> <li> <p><code>**kwargs</code>:  These will be passed to SVM, DT, RF, NB and NN, please see full lists at: </p> </li> <li><code>https</code>: //scikit-learn.org/stable/supervised_learning.html#supervised-learning </li> </ul> <p>Return:  </p> <p></p>"},{"location":"calmla/#method-mccv","title":"method <code>MCCV</code>","text":"<pre><code>MCCV(\n    split_data,\n    models='svm',\n    train_size=0.5,\n    n_splits=5,\n    n_iter=10,\n    random_state=None,\n    **kwargs\n)\n</code></pre> <p>This module allows to calibrate supervised classification in satellite images  through various algorithms and using Cross-Validation. </p> <p>Parameters:</p> <ul> <li> <p><code>split_data</code>:  A dictionary obtained from the splitData method of this package. </p> </li> <li> <p><code>models</code>:  Models to be used such as Support Vector Machine ('svm'), Decision Tree ('dt'), Random Forest ('rf'), Naive Bayes ('nb') and Neural Networks ('nn'). This parameter can be passed like models = ('svm', 'dt', 'rf', 'nb', 'nn'). </p> </li> <li> <p><code>cv</code>:  For splitting samples into two subsets, i.e. training data and  for testing data. Following Leave One Out Cross-Validation. </p> </li> <li> <p><code>n_iter</code>:  Number of iterations, i.e number of times the analysis is executed. </p> </li> <li> <p><code>**kwargs</code>:  These will be passed to SVM, DT, RF, NB and NN, please see full lists at: </p> </li> <li><code>https</code>: //scikit-learn.org/stable/supervised_learning.html#supervised-learning </li> </ul> <p>Return:  </p> <p></p>"},{"location":"calmla/#method-sa","title":"method <code>SA</code>","text":"<pre><code>SA(split_data, models=('svm', 'dt', 'rf'), train_size=0.5, n_iter=10, **kwargs)\n</code></pre> <p>This module allows to calibrate supervised classification in satellite images  through various algorithms and using approaches such as Set-Approach. </p> <p>Parameters:</p> <ul> <li> <p><code>split_data</code>:  A dictionary obtained from the splitData method of this package. </p> </li> <li> <p><code>models</code>:  Models to be used such as Support Vector Machine ('svm'), Decision Tree ('dt'), Random Forest ('rf'), Naive Bayes ('nb') and Neural Networks ('nn'). This parameter can be passed like models = ('svm', 'dt', 'rf', 'nb', 'nn'). </p> </li> <li> <p><code>train_size</code>:  For splitting samples into two subsets, i.e. training data and  for testing data. </p> </li> <li> <p><code>n_iter</code>:  Number of iterations, i.e number of times the analysis is executed. </p> </li> <li> <p><code>**kwargs</code>:  These will be passed to SVM, DT, RF, NB and NN, please see full lists at: </p> </li> <li><code>https</code>: //scikit-learn.org/stable/supervised_learning.html#supervised-learning </li> </ul> <p>Return:   </p> <p></p>"},{"location":"calmla/#method-splitdata","title":"method <code>splitData</code>","text":"<pre><code>splitData(random_state=None)\n</code></pre> <p>This method is to separate the dataset in predictor variables and the variable  to be predicted </p> <p>Parameter:  </p> <p>self: Attributes of class calmla. </p> <p>Return:  A dictionary with X and y. </p> <p>This file was automatically generated via lazydocs.</p>"},{"location":"changelog/","title":"Changelog","text":""},{"location":"changelog/#v001-date","title":"v0.0.1 - Date","text":"<p>Improvement:</p> <ul> <li>TBD</li> </ul> <p>New Features:</p> <ul> <li>TBD</li> </ul>"},{"location":"contributing/","title":"Contributing","text":"<p>Contributions are welcome, and they are greatly appreciated! Every little bit helps, and credit will always be given.</p> <p>You can contribute in many ways:</p>"},{"location":"contributing/#types-of-contributions","title":"Types of Contributions","text":""},{"location":"contributing/#report-bugs","title":"Report Bugs","text":"<p>Report bugs at https://github.com/ytarazona/scikit-eo/issues.</p> <p>If you are reporting a bug, please include:</p> <ul> <li>Your operating system name and version.</li> <li>Any details about your local setup that might be helpful in troubleshooting.</li> <li>Detailed steps to reproduce the bug.</li> </ul>"},{"location":"contributing/#fix-bugs","title":"Fix Bugs","text":"<p>Look through the GitHub issues for bugs. Anything tagged with <code>bug</code> and <code>help wanted</code> is open to whoever wants to implement it.</p>"},{"location":"contributing/#implement-features","title":"Implement Features","text":"<p>Look through the GitHub issues for features. Anything tagged with <code>enhancement</code> and <code>help wanted</code> is open to whoever wants to implement it.</p>"},{"location":"contributing/#write-documentation","title":"Write Documentation","text":"<p>scikit-eo could always use more documentation, whether as part of the official scikit-eo docs, in docstrings, or even on the web in blog posts, articles, and such.</p>"},{"location":"contributing/#submit-feedback","title":"Submit Feedback","text":"<p>The best way to send feedback is to file an issue at https://github.com/ytarazona/scikit-eo/issues.</p> <p>If you are proposing a feature:</p> <ul> <li>Explain in detail how it would work.</li> <li>Keep the scope as narrow as possible, to make it easier to implement.</li> <li>Remember that this is a volunteer-driven project, and that contributions are welcome :)</li> </ul>"},{"location":"contributing/#get-started","title":"Get Started!","text":"<p>Ready to contribute? Here's how to set up scikit-eo for local development.</p> <ol> <li> <p>Fork the scikit-eo repo on GitHub.</p> </li> <li> <p>Clone your fork locally:</p> <pre><code>$ git clone git@github.com:your_name_here/scikit-eo.git\n</code></pre> </li> <li> <p>Install your local copy into a virtualenv. Assuming you have     virtualenvwrapper installed, this is how you set up your fork for     local development:</p> <pre><code>$ mkvirtualenv scikit-eo\n$ cd scikit-eo/\n$ python setup.py develop\n</code></pre> </li> <li> <p>Create a branch for local development:</p> <pre><code>$ git checkout -b name-of-your-bugfix-or-feature\n</code></pre> <p>Now you can make your changes locally.</p> </li> <li> <p>When you're done making changes, check that your changes pass flake8     and the tests, including testing other Python versions with tox:</p> <pre><code>$ flake8 scikit-eo tests\n$ python setup.py test or pytest\n$ tox\n</code></pre> <p>To get flake8 and tox, just pip install them into your virtualenv.</p> </li> <li> <p>Commit your changes and push your branch to GitHub:</p> <pre><code>$ git add .\n$ git commit -m \"Your detailed description of your changes.\"\n$ git push origin name-of-your-bugfix-or-feature\n</code></pre> </li> <li> <p>Submit a pull request through the GitHub website.</p> </li> </ol>"},{"location":"contributing/#pull-request-guidelines","title":"Pull Request Guidelines","text":"<p>Before you submit a pull request, check that it meets these guidelines:</p> <ol> <li>The pull request should include tests.</li> <li>If the pull request adds functionality, the docs should be updated.     Put your new functionality into a function with a docstring, and add     the feature to the list in README.rst.</li> <li>The pull request should work for Python 3.5, 3.6, 3.7 and 3.8, and     for PyPy. Check https://github.com/ytarazona/scikit-eo/pull_requests and make sure that the tests pass for all     supported Python versions.</li> </ol>"},{"location":"deeplearning/","title":"deeplearning module","text":""},{"location":"deeplearning/#module-deeplearning","title":"module <code>deeplearning</code>","text":""},{"location":"deeplearning/#class-dl","title":"class <code>DL</code>","text":"<p>Deep Learning classification in Remote Sensing </p> <p></p>"},{"location":"deeplearning/#method-__init__","title":"method <code>__init__</code>","text":"<pre><code>__init__(image, endmembers, nodata=-99999)\n</code></pre> <p>Parameter: </p> <p>image: Optical images. It must be rasterio.io.DatasetReader with 3d.  </p> <p>endmembers: Endmembers must be a matrix (numpy.ndarray) and with more than one endmember.   Rows represent the endmembers and columns represent the spectral bands.  The number of bands must be equal to the number of endmembers.  E.g. an image with 6 bands, endmembers dimension should be $n*6$, where $n$   is rows with the number of endmembers and 6 is the number of bands   (should be equal).  In addition, Endmembers must have a field (type int or float) with the names   of classes to be predicted.  </p> <p>nodata: The NoData value to replace with -99999.  </p> <p></p>"},{"location":"deeplearning/#method-cnn","title":"method <code>CNN</code>","text":"<pre><code>CNN()\n</code></pre>"},{"location":"deeplearning/#method-fullyconnected","title":"method <code>FullyConnected</code>","text":"<pre><code>FullyConnected(\n    hidden_layers=3,\n    hidden_units=[64, 32, 16],\n    output_units=10,\n    input_shape=(6,),\n    epochs=300,\n    batch_size=32,\n    training_split=0.8,\n    random_state=None\n)\n</code></pre> <p>This algorithm consiste of a network with a sequence of Dense layers, which area densely  connnected (also called fully connected) neural layers. This is the simplest of deep  learning. </p> <p>Parameters:</p> <ul> <li> <p><code>hidden_layers</code>:  Number of hidden layers to be used. 3 is for default. </p> </li> <li> <p><code>hidden_units</code>:  Number of units to be used. This is related to 'neurons' in each hidden   layers.  </p> </li> <li> <p><code>output_units</code>:  Number of clases to be obtained. </p> </li> <li> <p><code>input_shape</code>:  The input shape is generally the shape of the input data provided to the   Keras model while training. The model cannot know the shape of the   training data. The shape of other tensors(layers) is computed automatically. </p> </li> <li> <p><code>epochs</code>:  Number of iteration, the network will compute the gradients of the weights with  regard to the loss on the batch, and update the weights accordingly. </p> </li> <li> <p><code>batch_size</code>:  This break the data into small batches. In deep learning, models do not   process antire dataset at once. </p> </li> <li> <p><code>training_split</code>:  For splitting samples into two subsets, i.e. training data and for testing  data. </p> </li> <li> <p><code>random_state</code>:  Random state ensures that the splits that you generate are reproducible.  </p> </li> <li><code>Please, see for more details https</code>: //scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html </li> </ul> <p>Return: </p> <p>A dictionary with Labels of classification as numpy object, overall accuracy,  among others results. </p> <p>This file was automatically generated via lazydocs.</p>"},{"location":"faq/","title":"FAQ","text":""},{"location":"fusionrs/","title":"fusionrs module","text":""},{"location":"fusionrs/#module-fusionrs","title":"module <code>fusionrs</code>","text":""},{"location":"fusionrs/#function-fusionrs","title":"function <code>fusionrs</code>","text":"<pre><code>fusionrs(optical, radar, stand_varb=True, nodata=-99999, **kwargs)\n</code></pre> <p>Fusion of images with different observation geometries through Principal Component Analysis (PCA). </p> <p>This algorithm allows to fusion images coming from different spectral sensors  (e.g., optical-optical, optical and SAR, or SAR-SAR). It is also possible to obtain the contribution (%) of each variable in the fused image. </p> <p>Parameters:</p> <ul> <li> <p><code>optical</code>:  Optical image. It must be rasterio.io.DatasetReader with 3d. </p> </li> <li> <p><code>radar</code>:  Radar image. It must be rasterio.io.DatasetReader with 3d. </p> </li> <li> <p><code>stand_varb</code>:  Logical. If <code>stand.varb = True</code>, the PCA is calculated using the correlation   matrix (standardized variables) instead of the covariance matrix   (non-standardized variables).  </p> </li> <li> <p><code>nodata</code>:  The NoData value to replace with -99999. </p> </li> <li> <p><code>**kwargs</code>:  These will be passed to scikit-learn PCA, please see full lists at: </p> </li> <li><code>https</code>: //scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html </li> </ul> <p>Return: </p> <p>A dictionary. </p> <p>Note:</p> <p>Before executing the function, it is recommended that images coming from different sensors or from the same sensor have a co-registration.  The contributions of variables in accounting for the variability in a given principal component are expressed in percentage. Variables that are correlated with PC1 (i.e., Dim.1) and PC2 (i.e., Dim.2) are the most important in explaining the variability in the data set. Variables that do not correlated with any PC or correlated with the last dimensions are variables with low contribution and might be removed to simplify the overall analysis. The contribution is a scaled version of the squared correlation between variables and component axes (or the cosine, from a geometrical point of view) --- this is used to assess the quality of the representation of the variables of the principal component, and it is computed as (cos(variable,axis)^2/total cos2 of the component)\u00d7100.  References: - Tarazona, Y., Zabala, A., Pons, X., Broquetas, A., Nowosad, J., and Zurqani, H.A. Fusing Landsat and SAR data for mapping tropical deforestation through machine learning classification and the PVts-\u03b2 non-seasonal detection approach, Canadian Journal of Remote Sensing., vol. 47, no. 5, pp. 677\u2013696, Sep. 2021. </p> <p>This file was automatically generated via lazydocs.</p>"},{"location":"get-started/","title":"Get Started","text":"<p>In these lines, a machine learning approach will be used to classify a satellite image. </p>"},{"location":"get-started/#example","title":"Example","text":""},{"location":"get-started/#1-applying-machine-learning","title":"1. Applying Machine Learning","text":"<p>Libraries to be used:</p> <pre><code>import rasterio\nimport numpy as np\nfrom scikeo.mla import MLA\nimport matplotlib.pyplot as plt\nfrom dbfread import DBF\nimport matplotlib as mpl\nimport pandas as pd\n</code></pre>"},{"location":"get-started/#20-optical-image","title":"2.0 Optical image","text":"<p>Landsat-8 OLI (Operational Land Imager) will be used to obtain in order to classify using Random Forest (RF). This image, which is in surface reflectance with bands: - Blue -&gt; B2 - Green -&gt; B3  - Red -&gt; B4 - Nir -&gt; B5 - Swir1 -&gt; B6 - Swir2 -&gt; B7</p> <p>The image and signatures to be used can be downloaded here:</p>"},{"location":"get-started/#30-supervised-classification-using-random-forest","title":"3.0 Supervised Classification using Random Forest","text":"<p>Image and endmembers</p> <pre><code>path_raster = r\"C:\\data\\ml\\LC08_232066_20190727_SR.tif\"\nimg = rasterio.open(path_raster)\n\npath_endm = r\"C:\\data\\ml\\endmembers.dbf\"\nendm = DBF(path_endm)\n</code></pre> <pre><code># endmembers\ndf = pd.DataFrame(iter(endm))\ndf.head()\n</code></pre> <p> </p> <p>Instance of <code>mla()</code>:</p> <pre><code>inst = MLA(image = img, endmembers = endm)\n</code></pre> <p>Applying Random Forest:</p> <pre><code>rf_class = inst.SVM(training_split = 0.7)\n</code></pre>"},{"location":"get-started/#40-results","title":"4.0 Results","text":"<p>Dictionary of results</p> <pre><code>rf_class.keys()\n</code></pre> <p>Overall accuracy</p> <pre><code>rf_class.get('Overall_Accuracy')\n</code></pre> <p>Kappa index</p> <pre><code>rf_class.get('Kappa_Index')\n</code></pre> <p>Confusion matrix or error matrix</p> <pre><code>rf_class.get('Confusion_Matrix')\n</code></pre> <p> </p> <p>Preparing the image before plotting</p> <pre><code># Let's define the color palette\npalette = mpl.colors.ListedColormap([\"#2232F9\",\"#F922AE\",\"#229954\",\"#7CED5E\"])\n</code></pre> <p>Applying the <code>plotRGB()</code> algorithm is easy:</p> <pre><code># Let\u00b4s plot\nfig, axes = plt.subplots(nrows = 1, ncols = 2, figsize = (15, 9))\n\n# satellite image\nplotRGB(img, title = 'Image in Surface Reflectance', ax = axes[0])\n\n# class results\naxes[1].imshow(svm_class.get('Classification_Map'), cmap = palette)\naxes[1].set_title(\"Classification map\")\naxes[1].grid(False)\n</code></pre> <p> </p>"},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#stable-release","title":"Stable release","text":"<p>To install scikit-eo, run this command in your terminal:</p> <pre><code>pip install scikit-eo\n</code></pre> <p>This is the preferred method to install scikit-eo, as it will always install the most recent stable release.</p> <p>If you don't have pip installed, this Python installation guide can guide you through the process.</p>"},{"location":"installation/#from-sources","title":"From sources","text":"<p>The sources for scikit-eo can be downloaded from the Github repo.</p> <p>You can clone the public repository:</p> <pre><code>git clone git://github.com/ytarazona/scikit-eo\n</code></pre> <pre><code>\n</code></pre>"},{"location":"linearTrend/","title":"linearTrend module","text":""},{"location":"linearTrend/#module-lineartrend","title":"module <code>linearTrend</code>","text":""},{"location":"linearTrend/#class-lineartrend","title":"class <code>linearTrend</code>","text":"<p>Linear Trend in Remote Sensing </p> <p></p>"},{"location":"linearTrend/#method-__init__","title":"method <code>__init__</code>","text":"<pre><code>__init__(image, nodata=-99999)\n</code></pre> <p>Parameters:</p> <ul> <li> <p><code>image</code>:  Optical images. It must be rasterio.io.DatasetReader with 3d. </p> </li> <li> <p><code>nodata</code>:  The NoData value to replace with -99999. </p> </li> </ul> <p></p>"},{"location":"linearTrend/#method-ln","title":"method <code>LN</code>","text":"<pre><code>LN(**kwargs)\n</code></pre> <p>Linear trend is useful for mapping forest degradation, land degradation, etc. This algorithm is capable of obtaining the slope of an ordinary least-squares  linear regression and its reliability (p-value). </p> <p>Parameters:</p> <ul> <li><code>**kwargs</code>:  These will be passed to LN, please see full lists at: </li> <li><code>https</code>: //docs.scipy.org/doc/scipy/reference/generated/scipy.stats.linregress.html </li> </ul> <p>Return: a dictionary with slope, intercept and p-value obtained. All of them in numpy.ndarray  with 2d. </p> <p>Note:</p> <p>Linear regression is widely used to analyze forest degradation or land degradation. Specifically, the slope and its reliability are used as main parameters and they can be obtained with this function. On the other hand, logistic regression allows obtaining a degradation risk map, in other words, it is a probability map.  References: - Tarazona, Y., Maria, Miyasiro-Lopez. (2020). Monitoring tropical forest degradation using remote sensing. Challenges and opportunities in the Madre de Dios region, Peru. Remote Sensing Applications: Society and Environment, 19, 100337.  - Wilkinson, G.N., Rogers, C.E., 1973. Symbolic descriptions of factorial models for analysis of variance. Appl. Stat. 22, 392-399.  - Chambers, J.M., 1992. Statistical Models in S. CRS Press.  </p> <p>Note:</p> <p>Linear regression is widely used to analyze forest degradation or land degradation. Specifically, the slope and its reliability are used as main parameters and they can be obtained with this function. On the other hand, logistic regression allows obtaining a degradation risk map, in other words, it is a probability map. </p> <p></p>"},{"location":"linearTrend/#method-lr","title":"method <code>LR</code>","text":"<pre><code>LR(col_pos=0, **kwargs)\n</code></pre> <p>Logistic Regression is a statistical analysis technique that can measure  statistically the relative influence of several factors and explain objectively how values  depend on predictor variables. This method is applied to remotely sensed data. </p> <p>Parameters:</p> <ul> <li><code>**kwargs</code>:  These will be passed to MLN, please see full lists at: </li> <li><code>https</code>: //www.statsmodels.org/dev/generated/statsmodels.discrete.discrete_model.Logit.html </li> </ul> <p>Return: a dictionary with the summary of logistic regression and an array of probability with 2d. </p> <p>Note:</p> <p>Logistic regression allows obtaining a degradation risk map (for instance), in other words, it is a probability map.  References: - Tarazona, Y., Maria, Miyasiro-Lopez. (2020). Monitoring tropical forest degradation using remote sensing. Challenges and opportunities in the Madre de Dios region, Peru. Remote Sensing Applications: Society and Environment, 19, 100337.  - Chambers, J.M., 1992. Statistical Models in S. CRS Press. </p> <p>This file was automatically generated via lazydocs.</p>"},{"location":"mla/","title":"mla module","text":""},{"location":"mla/#module-mla","title":"module <code>mla</code>","text":""},{"location":"mla/#class-mla","title":"class <code>MLA</code>","text":"<p>Supervised classification in Remote Sensing </p> <p></p>"},{"location":"mla/#method-__init__","title":"method <code>__init__</code>","text":"<pre><code>__init__(image, endmembers, nodata=-99999)\n</code></pre> <p>Parameter: </p> <p>image: Optical images. It must be rasterio.io.DatasetReader with 3d.  </p> <p>endmembers: Endmembers must be a matrix (numpy.ndarray) and with more than one endmember.   Rows represent the endmembers and columns represent the spectral bands.  The number of bands must be equal to the number of endmembers.  E.g. an image with 6 bands, endmembers dimension should be $n*6$, where $n$   is rows with the number of endmembers and 6 is the number of bands   (should be equal).  In addition, Endmembers must have a field (type int or float) with the names   of classes to be predicted.  </p> <p>nodata: The NoData value to replace with -99999.  </p> <p></p>"},{"location":"mla/#method-dt","title":"method <code>DT</code>","text":"<pre><code>DT(training_split=0.8, random_state=None, **kwargs)\n</code></pre> <p>Decision Tree is also a supervised non-parametric statistical learning technique, where the input data is divided recursively  into branches depending on certain decision thresholds until the data are segmented into homogeneous subgroups.  This technique has substantial advantages for remote sensing classification problems due to its flexibility, intuitive simplicity,  and computational efficiency. </p> <p>DT support raster data read by rasterio (rasterio.io.DatasetReader) as input. </p> <p>Parameters:</p> <ul> <li> <p><code>training_split</code>:  For splitting samples into two subsets, i.e. training data and for testing  data.  </p> </li> <li> <p><code>**kwargs</code>:  These will be passed to DT, please see full lists at: </p> </li> <li><code>https</code>: //scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier </li> </ul> <p>Return: </p> <p>A dictionary containing labels of classification as numpy object, overall accuracy, kappa index, confusion matrix. </p> <p></p>"},{"location":"mla/#method-nb","title":"method <code>NB</code>","text":"<pre><code>NB(training_split=0.8, random_state=None, **kwargs)\n</code></pre> <p>Naive Bayes classifier is an effective and simple method for image classification based on probability theory. The NB  classifier assumes an underlying probabilistic model and captures the uncertainty about the model in a principled way,  that is, by calculating the occurrence probabilities of different attribute values for different classes in a training  set. </p> <p>NB support raster data read by rasterio (rasterio.io.DatasetReader) as input. </p> <p>Parameters:</p> <ul> <li> <p><code>training_split</code>:  For splitting samples into two subsets, i.e. training data and for testing  data.  </p> </li> <li> <p><code>**kwargs</code>:  These will be passed to SVM, please see full lists at: </p> </li> <li><code>https</code>: //scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html </li> </ul> <p>Return: </p> <p>A dictionary containing labels of classification as numpy object, overall accuracy, kappa index, confusion matrix. </p> <p></p>"},{"location":"mla/#method-nn","title":"method <code>NN</code>","text":"<pre><code>NN(training_split=0.8, max_iter=300, random_state=None, **kwargs)\n</code></pre> <p>This classification consists of a neural network that is organized into several layers, that is, an input layer of predictor  variables, one or more layers of hidden nodes, in which each node represents an activation function acting on a weighted input  of the previous layers\u2019 outputs, and an output layer. </p> <p>NN support raster data read by rasterio (rasterio.io.DatasetReader) as input. </p> <p>Parameters:</p> <ul> <li> <p><code>training_split</code>:  For splitting samples into two subsets, i.e. training data and for testing  data.  </p> </li> <li> <p><code>**kwargs</code>:  These will be passed to SVM, please see full lists at: </p> </li> <li><code>https</code>: //scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html </li> </ul> <p>Return: </p> <p>A dictionary containing labels of classification as numpy object, overall accuracy, kappa index, confusion matrix. </p> <p></p>"},{"location":"mla/#method-rf","title":"method <code>RF</code>","text":"<pre><code>RF(training_split=0.8, random_state=None, **kwargs)\n</code></pre> <p>Random Forest is a derivative of Decision Tree which provides an improvement over DT to overcome the weaknesses of a single DT.  The prediction model of the RF classifier only requires two parameters to be identified: the number of classification trees desired,  known as \u201cntree,\u201d and the number of prediction variables, known as \u201cmtry,\u201d used in each node to make the tree grow. </p> <p>RF support raster data read by rasterio (rasterio.io.DatasetReader) as input. </p> <p>Parameters:</p> <ul> <li> <p><code>training_split</code>:  For splitting samples into two subsets, i.e. training data and for testing  data.  </p> </li> <li> <p><code>**kwargs</code>:  These will be passed to RF, please see full lists at: </p> </li> <li><code>https</code>: //scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html </li> </ul> <p>Return: </p> <p>A dictionary containing labels of classification as numpy object, overall accuracy, kappa index, confusion matrix. </p> <p></p>"},{"location":"mla/#method-svm","title":"method <code>SVM</code>","text":"<pre><code>SVM(training_split=0.8, random_state=None, kernel='linear', **kwargs)\n</code></pre> <p>The Support Vector Machine (SVM) classifier is a supervised non-parametric statistical learning technique that  does not assume a preliminary distribution of input data. Its discrimination criterion is a  hyperplane that separates the classes in the multidimensional space in which the samples  that have established the same classes are located, generally some training areas. </p> <p>SVM support raster data read by rasterio (rasterio.io.DatasetReader) as input. </p> <p>Parameters:</p> <ul> <li> <p><code>training_split</code>:  For splitting samples into two subsets, i.e. training data and for testing  data. </p> </li> <li> <p><code>kernel</code>:  {'linear', 'poly', 'rbf', 'sigmoid', 'precomputed'}, default='rbf' Specifies   the kernel type to be used in the algorithm. It must be one of 'linear', 'poly',   'rbf', 'sigmoid', 'precomputed' or a callable. If None is given, 'rbf' will  </p> </li> <li> <p><code>be used. See https</code>: //scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC for more details. </p> </li> <li> <p><code>**kwargs</code>:  These will be passed to SVM, please see full lists at: </p> </li> <li><code>https</code>: //scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC </li> </ul> <p>Return: </p> <p>A dictionary containing labels of classification as numpy object, overall accuracy, kappa index, confusion matrix. </p> <p>This file was automatically generated via lazydocs.</p>"},{"location":"pca/","title":"pca module","text":""},{"location":"pca/#module-pca","title":"module <code>pca</code>","text":""},{"location":"pca/#function-pca","title":"function <code>PCA</code>","text":"<pre><code>PCA(image, stand_varb=True, nodata=-99999, **kwargs)\n</code></pre> <p>Runing Principal Component Analysis (PCA) with satellite images. </p> <p>This algorithm allows to obtain Principal Components from images either radar or optical coming from different spectral sensors. It is also possible to obtain the contribution (%)  of each variable. </p> <p>Parameters:</p> <ul> <li> <p><code>images</code>:  Optical or radar image, it must be rasterio.io.DatasetReader with 3d. </p> </li> <li> <p><code>stand_varb</code>:  Logical. If <code>stand.varb = True</code>, the PCA is calculated using the correlation   matrix (standardized variables) instead of the covariance matrix   (non-standardized variables).  </p> </li> <li> <p><code>nodata</code>:  The NoData value to replace with -99999. </p> </li> <li> <p><code>**kwargs</code>:  These will be passed to scikit-learn PCA, please see full lists at: </p> </li> <li><code>https</code>: //scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html </li> </ul> <p>Return: </p> <p>A dictionary. </p> <p>Note:</p> <p>The contributions of variables in accounting for the variability in a given principal component are expressed in percentage. Variables that are correlated with PC1 (i.e., Dim.1) and PC2 (i.e., Dim.2) are the most important in explaining the variability in the data set. Variables that do not correlated with any PC or correlated with the last dimensions are variables with low contribution and might be removed to simplify the overall analysis. The contribution is a scaled version of the squared correlation between variables and component axes (or the cosine, from a geometrical point of view) --- this is used to assess the quality of the representation of the variables of the principal component, and it is computed as (cos(variable,axis)^2/total cos2 of the component)\u00d7100. </p> <p>This file was automatically generated via lazydocs.</p>"},{"location":"plot/","title":"plot module","text":""},{"location":"plot/#module-plot","title":"module <code>plot</code>","text":""},{"location":"plot/#function-plothist","title":"function <code>plotHist</code>","text":"<pre><code>plotHist(\n    image,\n    bands=1,\n    bins=128,\n    alpha=0.8,\n    title=None,\n    xlabel=None,\n    ylabel=None,\n    label=None,\n    ax=None,\n    density=True,\n    **kwargs\n)\n</code></pre> <p>This function allows to plot satellite images histogram. </p> <p>Parameters:</p> <pre><code> - &lt;b&gt;`image`&lt;/b&gt;:  Optical images. It must be rasterio.io.DatasetReader with 3d or 2d.\n\n\n - &lt;b&gt;`bands`&lt;/b&gt;:  Must be specified as a number of a list.\n\n\n - &lt;b&gt;`bins`&lt;/b&gt;:  By default is 128.\n\n\n - &lt;b&gt;`alpha`&lt;/b&gt;:  Percentage (%) of transparency between 0 and 1. 0 indicates 0% and 1 indicates 100%. By default is 80%.\n\n\n - &lt;b&gt;`title`&lt;/b&gt;:  Assigned title.\n\n\n - &lt;b&gt;`xlabel`&lt;/b&gt;:  X axis title.\n\n\n - &lt;b&gt;`ylabel`&lt;/b&gt;:  Y axis title.\n\n\n - &lt;b&gt;`label`&lt;/b&gt;:  Labeling the histogram.\n\n\n - &lt;b&gt;`ax`&lt;/b&gt;:  current axes\n\n\n - &lt;b&gt;`**kwargs`&lt;/b&gt;:  These will be passed to the matplotlib imshow(), please see full lists at: \n - &lt;b&gt;`https`&lt;/b&gt;: //matplotlib.org/stable/api/_as_gen/matplotlib.axes.Axes.hist.html\n</code></pre> <p>Return: </p> <pre><code> - &lt;b&gt;`ax `&lt;/b&gt;:  A histogram of an image.\n</code></pre> <p></p>"},{"location":"plot/#function-plotrgb","title":"function <code>plotRGB</code>","text":"<pre><code>plotRGB(\n    image,\n    bands=[3, 2, 1],\n    stretch='std',\n    title=None,\n    xlabel=None,\n    ylabel=None,\n    ax=None,\n    **kwargs\n)\n</code></pre> <p>Plotting an image in RGB. </p> <p>This function allows to plot an satellite image in RGB channels.  </p> <p>Parameters:</p> <pre><code> - &lt;b&gt;`image`&lt;/b&gt;:  Optical images. It must be rasterio.io.DatasetReader with 3d.\n\n\n - &lt;b&gt;`bands`&lt;/b&gt;:  A list contain the order of bands to be used in order to plot in RGB. For example,  for six bands (blue, green, red, nir, swir1 and swir2), number four (4) indicates   the swir1 band, number three (3) indicates the nir band and the number two (2) indicates  the red band.\n\n\n - &lt;b&gt;`stretch`&lt;/b&gt;:  Contrast enhancement using the histogram. There are two options here: i) using  standard deviation ('std') and ii) using percentiles ('per'). For default is 'std', which means  standard deviation.\n\n\n - &lt;b&gt;`title`&lt;/b&gt;:  Assigned title.\n\n\n - &lt;b&gt;`xlabel`&lt;/b&gt;:  X axis title.\n\n\n - &lt;b&gt;`ylabel`&lt;/b&gt;:  Y axis title.\n\n\n - &lt;b&gt;`ax`&lt;/b&gt;:  current axes\n\n\n - &lt;b&gt;`**kwargs`&lt;/b&gt;:  These will be passed to the matplotlib imshow(), please see full lists at: \n - &lt;b&gt;`https`&lt;/b&gt;: //matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.imshow.html\n</code></pre> <p>Return: </p> <pre><code> - &lt;b&gt;`ax `&lt;/b&gt;:  Graphic of an image in RGB.\n</code></pre> <p>This file was automatically generated via lazydocs.</p>"},{"location":"process/","title":"process module","text":""},{"location":"process/#module-process","title":"module <code>process</code>","text":""},{"location":"process/#function-crop","title":"function <code>crop</code>","text":"<pre><code>crop(image, shp, filename=None, filepath=None)\n</code></pre> <p>This algorithm allows to clip a raster (.tif) including a satellite image using a shapefile. </p> <p>Parameters:</p> <ul> <li> <p><code>image</code>:  This parameter can be a string with the raster path (e.g., r'/home/image/b3.tif') or it can be a rasterio.io.DatasetReader type. </p> </li> <li> <p><code>shp</code>:  Vector file, tipically shapefile. </p> </li> <li> <p><code>filename</code>:  The image name to be saved. </p> </li> <li> <p><code>filepath</code>:  The path which the image will be stored. </p> </li> </ul> <p>Return: </p> <p>A raster in your filepath. </p> <p></p>"},{"location":"process/#function-extract","title":"function <code>extract</code>","text":"<pre><code>extract(image, shp)\n</code></pre> <p>This algorithm allows to extract raster values using a shapefile. </p> <p>Parameters:</p> <ul> <li> <p><code>image</code>:  Optical images. It must be rasterio.io.DatasetReader with 3d or 2d. </p> </li> <li> <p><code>shp</code>:  Vector file, tipically shapefile. </p> </li> </ul> <p>Return: </p> <p>A dataframe with raster values obtained. </p> <p>Note:</p> <p>This function is usually used to extract raster values to be used on machine learning algorithms. </p> <p></p>"},{"location":"process/#function-confintervalml","title":"function <code>confintervalML</code>","text":"<pre><code>confintervalML(matrix, image_pred, pixel_size=10, conf=1.96, nodata=None)\n</code></pre> <p>The error matrix is a simple cross-tabulation of the class labels allocated by the classification of the remotely  sensed data against the reference data for the sample sites. The error matrix organizes the acquired sample data  in a way that summarizes key results and aids the quantification of accuracy and area. The main diagonal of the error  matrix highlights correct classifications while the off-diagonal elements show omission and commission errors.  The cell entries and marginal values of the error matrix are fundamental to both accuracy assessment and area  estimation. The cell entries of the population error matrix and the parameters derived from it must be estimated  from a sample. This function shows how to obtain a confusion matrix by estimated proportions of area with a confidence interval at 95% (1.96). </p> <p>Parameters:</p> <ul> <li> <p><code>matrix</code>:  confusion matrix or error matrix in numpy.ndarray.  </p> </li> <li> <p><code>image_pred</code>:  Array with 2d (rows, cols). This array should be the image classified with predicted classes.  </p> </li> <li> <p><code>pixel_size</code>:  Pixel size of the image classified. By default is 10m of Sentinel-2.  </p> </li> <li> <p><code>conf</code>:  Confidence interval. By default is 95%. </p> </li> </ul> <p>Return: </p> <p>Information of confusion matrix by proportions of area, overall accuracy, user's accuracy with confidence interval  and estimated area with confidence interval as well. </p> <p>Note:</p> <p>Columns and rows in a confusion matrix indicate reference and prediction respectively.  Reference: - Olofsson, P., Foody, G.M., Herold, M., Stehman, S.V., Woodcock, C.E., and Wulder, M.A. 2014. \u201cGood practices for estimating area and assessing accuracy of land change.\u201d Remote Sensing of Environment, Vol. 148: 42\u201357. doi:https://doi.org/10.1016/j.rse.2014.02.015. </p> <p>This file was automatically generated via lazydocs.</p>"},{"location":"rkmeans/","title":"rkmeans module","text":""},{"location":"rkmeans/#module-rkmeans","title":"module <code>rkmeans</code>","text":""},{"location":"rkmeans/#function-rkmeans","title":"function <code>rkmeans</code>","text":"<pre><code>rkmeans(image, k, nodata=-99999, **kwargs)\n</code></pre> <p>This function allows to classify satellite images using k-means </p> <p>In principle, this function allows to classify satellite images specifying a <code>k</code> value (clusters), however it is recommended to find the optimal value of <code>k</code> using the <code>calkmeans</code> function embedded in this package.  </p> <p>Parameters:</p> <ul> <li> <p><code>image</code>:  Optical images. It must be rasterio.io.DatasetReader with 3d. </p> </li> <li> <p><code>k</code>:  The number of clusters to be detected. </p> </li> <li> <p><code>nodata</code>:  The NoData value to replace with -99999.  </p> </li> <li> <p><code>**kwargs</code>:  These will be passed to scikit-learn KMeans, please see full lists at: </p> </li> <li><code>https</code>: //scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html </li> </ul> <p>Return: </p> <p>Labels of classification as numpy object with 2d. </p> <p>This file was automatically generated via lazydocs.</p>"},{"location":"sma/","title":"sma module","text":""},{"location":"sma/#module-sma","title":"module <code>sma</code>","text":""},{"location":"sma/#function-sma","title":"function <code>sma</code>","text":"<pre><code>sma(image, endmembers, nodata=-99999)\n</code></pre> <p>The SMA assumes that the energy received within the field of vision of the remote sensor  can be considered as the sum of the energies received from each dominant endmember.  This function addresses a Linear Mixing Model. </p> <p>A regression analysis is used to obtain the fractions. In least squares inversion algorithms,  the common objective is to estimate abundances that minimize the squared error between the  actual spectrum and the estimated spectrum. The values of the fractions will be between 0 and 1. </p> <p>Parameters:</p> <ul> <li> <p><code>image</code>:  Optical images. It must be rasterio.io.DatasetReader with 3d. </p> </li> <li> <p><code>endmembers</code>:  Endmembers must be a matrix (numpy.ndarray) and with more than one endmember.   Rows represent the endmembers and columns represent the spectral bands.  The number of bands must be greater than the number of endmembers.  E.g. an image with 6 bands, endmembers dimension should be $n*6$, where $n$   is rows with the number of endmembers and 6 is the number of bands   (should be equal).  </p> </li> <li> <p><code>nodata</code>:  The NoData value to replace with -99999. </p> </li> </ul> <p>Return: </p> <p>numpy.ndarray with 2d. </p> <p>References Adams, J. B., Smith, M. O., &amp; Gillespie, A. R. (1993). Imaging spectroscopy: Interpretation based on spectral mixture analysis. In C. M. Pieters &amp; P. Englert (Eds.), Remote geochemical analysis: Elements and mineralogical composition. NY: Cambridge Univ. Press 145-166 pp. </p> <p>Shimabukuro, Y.E. and Smith, J., (1991). The least squares mixing models to generate fraction images derived from remote sensing multispectral data. IEEE Transactions on Geoscience and Remote Sensing, 29, pp. 16-21. </p> <p>Note:</p> <p>A regression analysis is used to obtain the fractions. In least squares inversion algorithms, the common objective is to estimate abundances that minimize the squared error between the actual spectrum and the estimated spectrum. The values of the fractions will be between 0 and 1. </p> <p>This file was automatically generated via lazydocs.</p>"},{"location":"tassCap/","title":"tassCap module","text":""},{"location":"tassCap/#module-tasscap","title":"module <code>tassCap</code>","text":""},{"location":"tassCap/#function-tasscap","title":"function <code>tassCap</code>","text":"<pre><code>tassCap(image, sat='Landsat8OLI', nodata=-99999, scale=None)\n</code></pre> <p>The Tasseled-Cap Transformation is a linear transformation method for various  remote sensing data. Not only can it perform volume data compression, but it can also provide parameters associated with the physical characteristics,  such as brightness, greenness and wetness indices. </p> <p>Parameters:</p> <ul> <li> <p><code>image</code>:  Optical images. It must be rasterio.io.DatasetReader with 3d. </p> </li> <li> <p><code>sat</code>:  Specify satellite and sensor type (Landsat5TM, Landsat7ETM or Landsat8OLI).  </p> </li> <li> <p><code>nodata</code>:  The NoData value to replace with -99999. </p> </li> <li> <p><code>scale</code>:  Conversion of coefficients values </p> </li> </ul> <p>Return: numpy.ndarray with 3d containing brightness, greenness and wetness indices. </p> <p>References: </p> <ul> <li>Crist, E.P., R. Laurin, and R.C. Cicone. 1986. Vegetation and soils information  contained in transformed Thematic Mapper data. Pages 1465-1470 Ref. ESA SP-254.  </li> <li> <p><code>European Space Agency, Paris, France. http</code>: //www.ciesin.org/docs/005-419/005-419.html. </p> </li> <li> <p>Baig, M.H.A., Shuai, T., Tong, Q., 2014. Derivation of a tasseled cap transformation  based on Landsat 8 at-satellite reflectance. Remote Sensing Letters, 5(5), 423-431.  </p> </li> <li> <p>Li, B., Ti, C., Zhao, Y., Yan, X., 2016. Estimating Soil Moisture with Landsat Data  and Its Application in Extracting the Spatial Distribution of Winter Flooded Paddies.  Remote Sensing, 8(1), 38. </p> </li> </ul> <p>Note:</p> <p>Currently implemented for satellites such as Landsat-4 TM, Landsat-5 TM, Landsat-7 ETM+, Landsat-8 OLI and Sentinel2. The input data must be in top of atmosphere reflectance (toa). Bands required as input must be ordered as:  Consider using the following satellite bands: ===============   ================================ Type of Sensor     Name of bands ===============   ================================ Landsat4TM         :blue, green, red, nir, swir1, swir2 Landsat5TM         :blue, green, red, nir, swir1, swir2 Landsat7ETM+       :blue, green, red, nir, swir1, swir2 Landsat8OLI        :blue, green, red, nir, swir1, swir2 Landsat8OLI-Li2016 :coastal, blue, green, red, nir, swir1, swir2 Sentinel2MSI       :coastal, blue, green, red, nir-1, mir-1, mir-2 </p> <p>This file was automatically generated via lazydocs.</p>"},{"location":"tutorials/","title":"Brief examples","text":""},{"location":"tutorials/#example-01-random-forest-rf-classifier","title":"Example 01: Random Forest (RF) classifier","text":"<p>In this example, in a small region of southern Brazil, optical imagery from Landsat-8 OLI (Operational Land Imager) will be used to classify land cover using the machine learning algorithm Random Forest (RF) Breiman2001. Four types of land cover will be mapped, i.e., agriculture, forest, bare soil and water. The input data needed is the satellite image and the spectral signatures collected. The output as a dictionary will provide: i) confusion matrix, ii) overall accuracy, iii) kappa index and iv) a classes map.</p> <pre><code># 01. Libraries to be used in these examples\nimport rasterio\nimport numpy as np\nfrom dbfread import DBF\nfrom scikeo.mla import MLA\nfrom scikeo.fusionrs import fusionrs\nfrom scikeo.calmla import calmla\nfrom scikeo.plot import plotRGB\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\n\n# 02. Image and endmembers\npath_raster = \"\\data\\ex_O1\\LC08_232066_20190727_SR.tif\"\nimg = rasterio.open(path_raster)\n\npath_endm = \"\\data\\ex_O1\\endmembers\\endmembers.dbf\"\nendm = DBF(path_endm)\n\n# 03. An instance of mla()\ninst = MLA(image = img, endmembers = endm)\n\n# 04. Applying RF with 70% of data to train\nrf_class = inst.RF(training_split = 0.7)\n</code></pre> <p>Classification results:</p> <p></p>"},{"location":"tutorials/#example-02-calibration-methods-for-supervised-classification","title":"Example 02: Calibration methods for supervised classification","text":"<p>Given a large number of machine learning algorithms, it is necessary to select the one with the best performance in the classification, i.e., the algorithm in which the training and testing data used converge the learning iteratively to a solution that appears to be satisfactory Tarazona2021. To deal with this, users can apply the calibration methods Leave One Out Cross-Validation (LOOCV), Cross-Validation (CV) and Monte Carlo Cross-Validation (MCCV) in order to calibrate a supervised classification with different algorithms. The input data needed are the spectral signatures collected as a .dbf or .csv. The output will provide a graph with the errors of each classifier obtained.</p> <pre><code># 01. Endmembers\npath_endm = \"\\data\\ex_O2\\\\endmembers\\endmembers.dbf\"\nendm = DBF(path_endm)\n\n# 02. An instance of calmla()\ninst = calmla(endmembers = endm)\n\n# 03. Applying the splitData() method\ndata = inst.splitData()\n</code></pre> <p>Calibrating with Monte Carlo Cross-Validation Calibration (MCCV)</p> <p>Parameters:</p> <ul> <li><code>split_data</code>: An instance obtaind with <code>splitData()</code>.</li> <li><code>models</code>: Support Vector Machine (svm), Decision Tree (dt), Random Forest (rf) and Naive Bayes (nb).</li> <li><code>n_iter</code>: Number of iterations.</li> </ul> <pre><code># 04. Running MCCV\nerror_mccv = inst.MCCV(split_data = data, models = ('svm', 'dt', 'rf', 'nb'), \n                       n_iter = 10)\n</code></pre> <p>Calibration results:</p> <p></p>"},{"location":"tutorials/#example-03-imagery-fusion","title":"Example 03: Imagery Fusion.","text":"<p>This is an area where scikit-eo provides a novel approach to merge different types of satellite imagery. We are in a case where, after combining different variables into a single output, we want to know the contributions of the different original variables in the data fusion. The fusion of radar and optical images, despite of its well-know use, to improve land cover mapping, currently has no tools that help researchers to integrate or combine those resources. In this third example, users can apply imagery fusion with different observation geometries and different ranges of the electromagnetic spectrum Tarazona2021. The input data needed are the optical satellite image and the radar satellite image, for instance.</p> <p>In <code>scikit-eo</code> we developed the function <code>fusionrs()</code> which provides us with a dictionary with the following image fusion interpretation features:</p> <ul> <li>Fused_images: The fusion of both images into a 3-dimensional array (rows, cols, bands).</li> <li>Variance: The variance obtained.</li> <li>Proportion_of_variance: The proportion of the obtained variance.</li> <li>Cumulative_variance: The cumulative variance.</li> <li>Correlation: Correlation of the original bands with the principal components.</li> <li>Contributions_in_%: The contributions of each optical and radar band in the fusion.</li> </ul> <pre><code># 01 Loagind dataset\npath_optical = \"data/ex_03/LC08_003069_20180906.tif\"\noptical = rasterio.open(path_optical)\n\npath_radar = \"data/ex_03/S1_2018_VV_VH.tif\"\nradar = rasterio.open(path_radar)\n\n# 02 Applying the fusionrs:\nfusion = fusionrs(optical = optical, radar = radar)\n\n# 03 Dictionary of results:\nfusion.keys()\n\n# 04 Proportion of variance:\nprop_var = fusion.get('Proportion_of_variance')\n\n# 05 Cumulative variance (%):\ncum_var = fusion.get('Cumulative_variance')*100\n\n# 06 Showing the proportion of variance and cumulative:\nx_labels = ['PC{}'.format(i+1) for i in range(len(prop_var))]\n\nfig, axes = plt.subplots(figsize = (6,5))\nln1 = axes.plot(x_labels, prop_var, marker ='o', markersize = 6,  \n                label = 'Proportion of variance')\n\naxes2 = axes.twinx()\nln2 = axes2.plot(x_labels, cum_var, marker = 'o', color = 'r', \n                 label = \"Cumulative variance\")\n\nln = ln1 + ln2\nlabs = [l.get_label() for l in ln]\n\naxes.legend(ln, labs, loc = 'center right')\naxes.set_xlabel(\"Principal Component\")\naxes.set_ylabel(\"Proportion of Variance\")\naxes2.set_ylabel(\"Cumulative (%)\")\naxes2.grid(False)\nplt.show()\n</code></pre> <p></p> <pre><code># 07 Contributions of each variable in %:\nfusion.get('Contributions_in_%')\n</code></pre> <p></p> <pre><code># 08 Preparing the image:\narr = fusion.get('Fused_images')\n\n## Let\u00b4s plot\nfig, axes = plt.subplots(figsize = (8, 8))\nplotRGB(arr, bands = [1,2,3], title = 'Fusion of optical and radar images')\nplt.show()\n</code></pre> <p></p>"},{"location":"tutorials/#example-04-accuracy-assessment","title":"Example 04: Accuracy assessment","text":"<p>In this final example, after obtaining the predicted class map, we are in a case where we want to know the uncertainties of each class. The assessing accuracy and area estimate will be obtained following guidance proposed by OLOFSSON201442. All that users need are the confusion matrix and a previously obtained predicted class map.</p> <p><code>confintervalML</code> requires the following parameters:</p> <ul> <li>matrix: confusion matrix or error matrix in numpy.ndarray.</li> <li>image_pred: a 2-dimensional array (rows, cols). This array should be the classified image with predicted classes.</li> <li>pixel_size: Pixel size of the classified image. Set by default as 10 meters. In this example is 30 meters (Landsat).</li> <li>conf: Confidence interval. By default is 95% (1.96).</li> <li>nodata: No data must be specified as 0, NaN or any other value. Keep in mind with this parameter.</li> </ul> <pre><code># 01 Load raster data\npath_raster = r\"\\data\\ex_O4\\ml\\predicted_map.tif\"\nimg = rasterio.open(path_optical).read(1)\n\n# 02 Load confusion matrix as .csv\npath_cm = r\"\\data\\ex_O4\\ml\\confusion_matrix.csv\"\nvalues = pd.read_csv(path_radar)\n\n# 03 Applying the confintervalML:\nconfintervalML(matrix = values, image_pred = img, pixel_size = 30, conf = 1.96, \n               nodata = -9999)\n</code></pre> <p>Results:</p> <p></p> <pre><code>\n</code></pre>"},{"location":"usage/","title":"Usage","text":"<p>To use scikit-eo in a project:</p> <pre><code>import scikit-eo\n</code></pre> <pre><code>\n</code></pre>"},{"location":"writeRaster/","title":"writeRaster module","text":""},{"location":"writeRaster/#module-writeraster","title":"module <code>writeRaster</code>","text":""},{"location":"writeRaster/#function-writeraster","title":"function <code>writeRaster</code>","text":"<pre><code>writeRaster(arr, image, filename=None, filepath=None, n=1)\n</code></pre> <p>This algorithm allows to save array images to raster format (.tif). </p> <p>Parameters:</p> <ul> <li> <p><code>arr</code>:  Array object with 2d (rows and cols) or 3d (rows, cols, bands). </p> </li> <li> <p><code>image</code>:  Optical images. It must be read by rasterio.open(). </p> </li> <li> <p><code>filename</code>:  The image name to be saved. </p> </li> <li> <p><code>filepath</code>:  The path which the image will be stored. </p> </li> <li> <p><code>n</code>:  Number of images to be saved. </p> </li> </ul> <p>Return: A raster in your filepath. </p> <p>This file was automatically generated via lazydocs.</p>"}]}
=======
{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to \u00b6 scikit-eo: A Python package for Remote Sensing Tools \u00b6 Links of interest: \u00b6 GitHub repo : https://github.com/ytarazona/scikit-eo Documentation : https://ytarazona.github.io/scikit-eo/ PyPI : https://pypi.org/project/scikeo/ Notebooks examples : https://github.com/ytarazona/scikit-eo/tree/main/examples Google Colab examples : https://github.com/ytarazona/scikit-eo/tree/main/examples Free software : Apache 2.0 Tutorials : Introduction \u00b6 Nowadays, remotely sensed data has increased dramatically. Microwaves and optical images with different spatial and temporal resolutions are available and are used to monitor a variety of environmental issues such as deforestation, land degradation, land use and land cover change, among others. Although there are efforts (i.e., Python packages, forums, communities, etc.) to make available line-of-code tools for pre-processing, processing and analysis of satellite imagery, there is still a gap that needs to be filled. In other words, too much time is still spent by many users developing Python lines of code. Algorithms for mapping land degradation through a linear trend of vegetation indices, fusion optical and radar images to classify vegetation cover, and calibration of machine learning algorithms, among others, are not available yet. Therefore, scikit-eo is a Python package that provides tools for remote sensing. This package was developed to fill the gaps in remotely sensed data processing tools. Most of the tools are based on scientific publications, and others are useful algorithms that will allow processing to be done in a few lines of code. With these tools, the user will be able to invest time in analyzing the results of their data and not spend time on elaborating lines of code, which can sometimes be stressful. Audience \u00b6 Scikit-eo is a versatile Python package designed to cover a wide range of users, including students, professionals of remote sensing, researchers of environmental analysis, and organizations looking for satellite image analysis. Its comprehensive features make it well-suited for various applications, such as university teaching, that include technical and practical sessions, and cutting-edge research using the most recent machine learning and deep learning techniques applied to the field of remote sensing. Whether the user are students seeking to get insights from a satellite image analysis or a experienced researcher looking for advanced tools, scikit-eo offers a valuable resource to support the most valuable methods for environmental studies. Tools for Remote Sensing \u00b6 Name of functions/classes Description mla Machine Learning (Random Forest, Support Vector Machine, Decition Tree, Naive Bayes, Neural Network, etc.) calmla Calibrating supervised classification in Remote Sensing (e.g., Monte Carlo Cross-Validation, Leave-One-Out Cross-Validation, etc.) confintervalML Information of confusion matrix by proportions of area, overall accuracy, user's accuracy with confidence interval and estimated area with confidence interval as well. rkmeans K-means classification calkmeans This function allows to calibrate the kmeans algorithm. It is possible to obtain the best k value and the best embedded algorithm in kmeans. pca Principal Components Analysis atmosCorr Atmospheric Correction of satellite imagery deepLearning Deep Learning algorithms linearTrend Linear trend is useful for mapping forest degradation or land degradation fusionrs This algorithm allows to fuse images coming from different spectral sensors (e.g., optical-optical, optical and SAR or SAR-SAR). Among many of the qualities of this function, it is possible to obtain the contribution (%) of each variable in the fused image sma Spectral Mixture Analysis - Classification sup-pixel tassCap The Tasseled-Cap Transformation You will find more algorithms!. Installation \u00b6 To use scikit-eo it is necessary to install it. There are two options: 1. From PyPI \u00b6 1 pip install scikeo 2. Installing from source \u00b6 It is also possible to install the latest development version directly from the GitHub repository with: 1 pip install git + https : // github . com / ytarazona / scikit - eo","title":"Home"},{"location":"#welcome-to","text":"","title":"Welcome to"},{"location":"#scikit-eo-a-python-package-for-remote-sensing-tools","text":"","title":"scikit-eo: A Python package for Remote Sensing Tools"},{"location":"#links-of-interest","text":"GitHub repo : https://github.com/ytarazona/scikit-eo Documentation : https://ytarazona.github.io/scikit-eo/ PyPI : https://pypi.org/project/scikeo/ Notebooks examples : https://github.com/ytarazona/scikit-eo/tree/main/examples Google Colab examples : https://github.com/ytarazona/scikit-eo/tree/main/examples Free software : Apache 2.0 Tutorials :","title":"Links of interest:"},{"location":"#introduction","text":"Nowadays, remotely sensed data has increased dramatically. Microwaves and optical images with different spatial and temporal resolutions are available and are used to monitor a variety of environmental issues such as deforestation, land degradation, land use and land cover change, among others. Although there are efforts (i.e., Python packages, forums, communities, etc.) to make available line-of-code tools for pre-processing, processing and analysis of satellite imagery, there is still a gap that needs to be filled. In other words, too much time is still spent by many users developing Python lines of code. Algorithms for mapping land degradation through a linear trend of vegetation indices, fusion optical and radar images to classify vegetation cover, and calibration of machine learning algorithms, among others, are not available yet. Therefore, scikit-eo is a Python package that provides tools for remote sensing. This package was developed to fill the gaps in remotely sensed data processing tools. Most of the tools are based on scientific publications, and others are useful algorithms that will allow processing to be done in a few lines of code. With these tools, the user will be able to invest time in analyzing the results of their data and not spend time on elaborating lines of code, which can sometimes be stressful.","title":"Introduction"},{"location":"#audience","text":"Scikit-eo is a versatile Python package designed to cover a wide range of users, including students, professionals of remote sensing, researchers of environmental analysis, and organizations looking for satellite image analysis. Its comprehensive features make it well-suited for various applications, such as university teaching, that include technical and practical sessions, and cutting-edge research using the most recent machine learning and deep learning techniques applied to the field of remote sensing. Whether the user are students seeking to get insights from a satellite image analysis or a experienced researcher looking for advanced tools, scikit-eo offers a valuable resource to support the most valuable methods for environmental studies.","title":"Audience"},{"location":"#tools-for-remote-sensing","text":"Name of functions/classes Description mla Machine Learning (Random Forest, Support Vector Machine, Decition Tree, Naive Bayes, Neural Network, etc.) calmla Calibrating supervised classification in Remote Sensing (e.g., Monte Carlo Cross-Validation, Leave-One-Out Cross-Validation, etc.) confintervalML Information of confusion matrix by proportions of area, overall accuracy, user's accuracy with confidence interval and estimated area with confidence interval as well. rkmeans K-means classification calkmeans This function allows to calibrate the kmeans algorithm. It is possible to obtain the best k value and the best embedded algorithm in kmeans. pca Principal Components Analysis atmosCorr Atmospheric Correction of satellite imagery deepLearning Deep Learning algorithms linearTrend Linear trend is useful for mapping forest degradation or land degradation fusionrs This algorithm allows to fuse images coming from different spectral sensors (e.g., optical-optical, optical and SAR or SAR-SAR). Among many of the qualities of this function, it is possible to obtain the contribution (%) of each variable in the fused image sma Spectral Mixture Analysis - Classification sup-pixel tassCap The Tasseled-Cap Transformation You will find more algorithms!.","title":"Tools for Remote Sensing"},{"location":"#installation","text":"To use scikit-eo it is necessary to install it. There are two options:","title":"Installation"},{"location":"#1-from-pypi","text":"1 pip install scikeo","title":"1. From PyPI"},{"location":"#2-installing-from-source","text":"It is also possible to install the latest development version directly from the GitHub repository with: 1 pip install git + https : // github . com / ytarazona / scikit - eo","title":"2. Installing from source"},{"location":"atmosCorr/","text":"module atmosCorr \u00b6 class atmosCorr \u00b6 Atmospheric Correction in Optical domain method __init__ \u00b6 1 __init__ ( path , nodata =- 99999 ) Parameter: path: String. The folder in which the satellite bands are located. This images could be Landsat Collection 2 Level-1. For example: path = r'/folder/image/raster'. nodata: The NoData value to replace with -99999. method DOS \u00b6 1 DOS ( sat = 'LC08' , mindn = None ) The Dark Object Subtraction Method was proposed by Chavez (1988). This image-based atmospheric correction method considers absolutely critical and valid the existence of a dark object in the scene, which is used in the selection of a minimum value in the haze correction. The most valid dark objects in this kind of correction are areas totally shaded or otherwise areas representing dark water bodies. Parameters: sat : Type of Satellite. It could be Landsat-5 TM, Landsat-8 OLI or Landsat-9 OLI-2. mindn : Min of digital number for each band in a list. Return: An array with Surface Reflectance values with 3d, i.e. (rows, cols, bands). References: - Chavez, P.S. (1988). An Improved Dark-Object Subtraction Technique for Atmospheric Scattering Correction of Multispectral Data. Remote Sensing of Envrironment, 24(3), 459-479. method RAD \u00b6 1 RAD ( sat = 'LC08' ) Conversion to TOA Radiance. Landsat Level-1 data can be converted to TOA spectral radiance using the radiance rescaling factors in the MTL file: L\u03bb = MLQcal + AL where: L\u03bb = TOA spectral radiance (Watts/(m2 srad \u03bcm)) ML = Band-specific multiplicative rescaling factor from the metadata (RADIANCE_MULT_BAND_x, where x is the band number) AL = Band-specific additive rescaling factor from the metadata (RADIANCE_ADD_BAND_x, where x is the band number) Qcal = Quantized and calibrated standard product pixel values (DN) Parameters: sat : Type of Satellite. It could be Landsat-5 TM, Landsat-8 OLI or Landsat-9 OLI-2. Return: An array with radiance values with 3d, i.e. (rows, cols, bands). method TOA \u00b6 1 TOA ( sat = 'LC08' ) A reduction in scene-to-scene variability can be achieved by converting the at-sensor spectral radiance to exoatmospheric TOA reflectance, also known as in-band planetary albedo. Equation to obtain TOA reflectance: \u03c1\u03bb\u2032 = M\u03c1*DN + A\u03c1 \u03c1\u03bb = \u03c1\u03bb\u2032/sin(theta) Parameters: sat : Type of Satellite. It could be Landsat-5 TM, Landsat-8 OLI or Landsat-9 OLI-2. Return: An array with TOA values with 3d, i.e. (rows, cols, bands). This file was automatically generated via lazydocs .","title":"atmosCorr module"},{"location":"atmosCorr/#module-atmoscorr","text":"","title":"module atmosCorr"},{"location":"atmosCorr/#class-atmoscorr","text":"Atmospheric Correction in Optical domain","title":"class atmosCorr"},{"location":"atmosCorr/#method-__init__","text":"1 __init__ ( path , nodata =- 99999 ) Parameter: path: String. The folder in which the satellite bands are located. This images could be Landsat Collection 2 Level-1. For example: path = r'/folder/image/raster'. nodata: The NoData value to replace with -99999.","title":"method __init__"},{"location":"atmosCorr/#method-dos","text":"1 DOS ( sat = 'LC08' , mindn = None ) The Dark Object Subtraction Method was proposed by Chavez (1988). This image-based atmospheric correction method considers absolutely critical and valid the existence of a dark object in the scene, which is used in the selection of a minimum value in the haze correction. The most valid dark objects in this kind of correction are areas totally shaded or otherwise areas representing dark water bodies. Parameters: sat : Type of Satellite. It could be Landsat-5 TM, Landsat-8 OLI or Landsat-9 OLI-2. mindn : Min of digital number for each band in a list. Return: An array with Surface Reflectance values with 3d, i.e. (rows, cols, bands). References: - Chavez, P.S. (1988). An Improved Dark-Object Subtraction Technique for Atmospheric Scattering Correction of Multispectral Data. Remote Sensing of Envrironment, 24(3), 459-479.","title":"method DOS"},{"location":"atmosCorr/#method-rad","text":"1 RAD ( sat = 'LC08' ) Conversion to TOA Radiance. Landsat Level-1 data can be converted to TOA spectral radiance using the radiance rescaling factors in the MTL file: L\u03bb = MLQcal + AL where: L\u03bb = TOA spectral radiance (Watts/(m2 srad \u03bcm)) ML = Band-specific multiplicative rescaling factor from the metadata (RADIANCE_MULT_BAND_x, where x is the band number) AL = Band-specific additive rescaling factor from the metadata (RADIANCE_ADD_BAND_x, where x is the band number) Qcal = Quantized and calibrated standard product pixel values (DN) Parameters: sat : Type of Satellite. It could be Landsat-5 TM, Landsat-8 OLI or Landsat-9 OLI-2. Return: An array with radiance values with 3d, i.e. (rows, cols, bands).","title":"method RAD"},{"location":"atmosCorr/#method-toa","text":"1 TOA ( sat = 'LC08' ) A reduction in scene-to-scene variability can be achieved by converting the at-sensor spectral radiance to exoatmospheric TOA reflectance, also known as in-band planetary albedo. Equation to obtain TOA reflectance: \u03c1\u03bb\u2032 = M\u03c1*DN + A\u03c1 \u03c1\u03bb = \u03c1\u03bb\u2032/sin(theta) Parameters: sat : Type of Satellite. It could be Landsat-5 TM, Landsat-8 OLI or Landsat-9 OLI-2. Return: An array with TOA values with 3d, i.e. (rows, cols, bands). This file was automatically generated via lazydocs .","title":"method TOA"},{"location":"calkmeans/","text":"module calkmeans \u00b6 function calkmeans \u00b6 1 2 3 4 5 6 7 8 9 calkmeans ( image , k = None , algo = ( 'auto' , 'elkan' ), max_iter = 300 , n_iter = 10 , nodata =- 99999 , ** kwargs ) Calibrating kmeans This function allows to calibrate the kmeans algorithm. It is possible to obtain the best 'k' value and the best embedded algorithm in KMmeans. Parameters: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 - <b>`image`</b>: Optical images. It must be rasterio.io.DatasetReader with 3d. - <b>`k`</b>: k This argument is None when the objective is to obtain the best 'k' value. If the objective is to select the best algorithm embedded in kmeans, please specify a 'k' value. - <b>`max_iter`</b>: The maximum number of iterations allowed. Strictly related to KMeans. Please see - <b>`https`</b>: //scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html - <b>`algo`</b>: It can be \"auto\" and 'elkan'. \"auto\" and \"full\" are deprecated and they will be removed in Scikit-Learn 1.3. They are both aliases for \"lloyd\". - <b>`Changed in version 1.1`</b>: Renamed \u201cfull\u201d to \u201clloyd\u201d, and deprecated \u201cauto\u201d and \u201cfull\u201d. Changed \u201cauto\u201d to use \u201clloyd\u201d instead of \u201celkan\u201d. - <b>`n_iter`</b>: Iterations number to obtain the best 'k' value. 'n_iter' must be greater than the number of classes expected to be obtained in the classification. Default is 10. - <b>`nodata`</b>: The NoData value to replace with -99999. - <b>`**kwargs`</b>: These will be passed to scikit-learn KMeans, please see full lists at: - <b>`https`</b>: //scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html. Return: Labels of classification as numpy object with 2d. Note: If the idea is to find the optimal value of 'k' (clusters or classes), k = None as an argument of the function must be put, because the function find 'k' for which the intra-class inertia is stabilized. If the 'k' value is known and the idea is to find the best algorithm embedded in kmeans (that maximizes inter-class distances), k = n, which 'n' is a specific class number, must be put. It can be greater than or equal to 0. This file was automatically generated via lazydocs .","title":"calkmeans module"},{"location":"calkmeans/#module-calkmeans","text":"","title":"module calkmeans"},{"location":"calkmeans/#function-calkmeans","text":"1 2 3 4 5 6 7 8 9 calkmeans ( image , k = None , algo = ( 'auto' , 'elkan' ), max_iter = 300 , n_iter = 10 , nodata =- 99999 , ** kwargs ) Calibrating kmeans This function allows to calibrate the kmeans algorithm. It is possible to obtain the best 'k' value and the best embedded algorithm in KMmeans. Parameters: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 - <b>`image`</b>: Optical images. It must be rasterio.io.DatasetReader with 3d. - <b>`k`</b>: k This argument is None when the objective is to obtain the best 'k' value. If the objective is to select the best algorithm embedded in kmeans, please specify a 'k' value. - <b>`max_iter`</b>: The maximum number of iterations allowed. Strictly related to KMeans. Please see - <b>`https`</b>: //scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html - <b>`algo`</b>: It can be \"auto\" and 'elkan'. \"auto\" and \"full\" are deprecated and they will be removed in Scikit-Learn 1.3. They are both aliases for \"lloyd\". - <b>`Changed in version 1.1`</b>: Renamed \u201cfull\u201d to \u201clloyd\u201d, and deprecated \u201cauto\u201d and \u201cfull\u201d. Changed \u201cauto\u201d to use \u201clloyd\u201d instead of \u201celkan\u201d. - <b>`n_iter`</b>: Iterations number to obtain the best 'k' value. 'n_iter' must be greater than the number of classes expected to be obtained in the classification. Default is 10. - <b>`nodata`</b>: The NoData value to replace with -99999. - <b>`**kwargs`</b>: These will be passed to scikit-learn KMeans, please see full lists at: - <b>`https`</b>: //scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html. Return: Labels of classification as numpy object with 2d. Note: If the idea is to find the optimal value of 'k' (clusters or classes), k = None as an argument of the function must be put, because the function find 'k' for which the intra-class inertia is stabilized. If the 'k' value is known and the idea is to find the best algorithm embedded in kmeans (that maximizes inter-class distances), k = n, which 'n' is a specific class number, must be put. It can be greater than or equal to 0. This file was automatically generated via lazydocs .","title":"function calkmeans"},{"location":"calmla/","text":"module calmla \u00b6 class calmla \u00b6 Calibrating supervised classification in Remote Sensing This module allows to calibrate supervised classification in satellite images through various algorithms and using approaches such as Set-Approach, Leave-One-Out Cross-Validation (LOOCV), Cross-Validation (k-fold) and Monte Carlo Cross-Validation (MCCV) method __init__ \u00b6 1 __init__ ( endmembers ) Parameter: endmembers: Endmembers must be a matrix (numpy.ndarray) and with more than one endmember. Rows represent the endmembers and columns represent the spectral bands. The number of bands must be equal to the number of endmembers. E.g. an image with 6 bands, endmembers dimension should be $n*6$, where $n$ is rows with the number of endmembers and 6 is the number of bands (should be equal). In addition, Endmembers must have a field (type int or float) with the names of classes to be predicted. method CV \u00b6 1 2 3 4 5 6 7 8 CV ( split_data , models = ( 'svm' , 'dt' ), k = 5 , n_iter = 10 , random_state = None , ** kwargs ) This module allows to calibrate supervised classification in satellite images through various algorithms and using Cross-Validation. Parameters: split_data : A dictionary obtained from the splitData method of this package. models : Models to be used such as Support Vector Machine ('svm'), Decision Tree ('dt'), Random Forest ('rf'), Naive Bayes ('nb') and Neural Networks ('nn'). This parameter can be passed like models = ('svm', 'dt', 'rf', 'nb', 'nn'). cv : For splitting samples into two subsets, i.e. training data and for testing data. Following Leave One Out Cross-Validation. n_iter : Number of iterations, i.e number of times the analysis is executed. **kwargs : These will be passed to SVM, DT, RF, NB and NN, please see full lists at: https : //scikit-learn.org/stable/supervised_learning.html#supervised-learning Return: method LOOCV \u00b6 1 LOOCV ( split_data , models = ( 'svm' , 'dt' ), cv = LeaveOneOut (), n_iter = 10 , ** kwargs ) This module allows to calibrate supervised classification in satellite images through various algorithms and using Leave One Out Cross-Validation. Parameters: split_data : A dictionary obtained from the splitData method of this package. models : Models to be used such as Support Vector Machine ('svm'), Decision Tree ('dt'), Random Forest ('rf'), Naive Bayes ('nb') and Neural Networks ('nn'). This parameter can be passed like models = ('svm', 'dt', 'rf', 'nb', 'nn'). cv : For splitting samples into two subsets, i.e. training data and for testing data. Following Leave One Out Cross-Validation. n_iter : Number of iterations, i.e number of times the analysis is executed. **kwargs : These will be passed to SVM, DT, RF, NB and NN, please see full lists at: https : //scikit-learn.org/stable/supervised_learning.html#supervised-learning Return: method MCCV \u00b6 1 2 3 4 5 6 7 8 9 MCCV ( split_data , models = 'svm' , train_size = 0.5 , n_splits = 5 , n_iter = 10 , random_state = None , ** kwargs ) This module allows to calibrate supervised classification in satellite images through various algorithms and using Cross-Validation. Parameters: split_data : A dictionary obtained from the splitData method of this package. models : Models to be used such as Support Vector Machine ('svm'), Decision Tree ('dt'), Random Forest ('rf'), Naive Bayes ('nb') and Neural Networks ('nn'). This parameter can be passed like models = ('svm', 'dt', 'rf', 'nb', 'nn'). cv : For splitting samples into two subsets, i.e. training data and for testing data. Following Leave One Out Cross-Validation. n_iter : Number of iterations, i.e number of times the analysis is executed. **kwargs : These will be passed to SVM, DT, RF, NB and NN, please see full lists at: https : //scikit-learn.org/stable/supervised_learning.html#supervised-learning Return: method SA \u00b6 1 SA ( split_data , models = ( 'svm' , 'dt' , 'rf' ), train_size = 0.5 , n_iter = 10 , ** kwargs ) This module allows to calibrate supervised classification in satellite images through various algorithms and using approaches such as Set-Approach. Parameters: split_data : A dictionary obtained from the splitData method of this package. models : Models to be used such as Support Vector Machine ('svm'), Decision Tree ('dt'), Random Forest ('rf'), Naive Bayes ('nb') and Neural Networks ('nn'). This parameter can be passed like models = ('svm', 'dt', 'rf', 'nb', 'nn'). train_size : For splitting samples into two subsets, i.e. training data and for testing data. n_iter : Number of iterations, i.e number of times the analysis is executed. **kwargs : These will be passed to SVM, DT, RF, NB and NN, please see full lists at: https : //scikit-learn.org/stable/supervised_learning.html#supervised-learning Return: method splitData \u00b6 1 splitData ( random_state = None ) This method is to separate the dataset in predictor variables and the variable to be predicted Parameter: self: Attributes of class calmla. Return: A dictionary with X and y. This file was automatically generated via lazydocs .","title":"calmla module"},{"location":"calmla/#module-calmla","text":"","title":"module calmla"},{"location":"calmla/#class-calmla","text":"Calibrating supervised classification in Remote Sensing This module allows to calibrate supervised classification in satellite images through various algorithms and using approaches such as Set-Approach, Leave-One-Out Cross-Validation (LOOCV), Cross-Validation (k-fold) and Monte Carlo Cross-Validation (MCCV)","title":"class calmla"},{"location":"calmla/#method-__init__","text":"1 __init__ ( endmembers ) Parameter: endmembers: Endmembers must be a matrix (numpy.ndarray) and with more than one endmember. Rows represent the endmembers and columns represent the spectral bands. The number of bands must be equal to the number of endmembers. E.g. an image with 6 bands, endmembers dimension should be $n*6$, where $n$ is rows with the number of endmembers and 6 is the number of bands (should be equal). In addition, Endmembers must have a field (type int or float) with the names of classes to be predicted.","title":"method __init__"},{"location":"calmla/#method-cv","text":"1 2 3 4 5 6 7 8 CV ( split_data , models = ( 'svm' , 'dt' ), k = 5 , n_iter = 10 , random_state = None , ** kwargs ) This module allows to calibrate supervised classification in satellite images through various algorithms and using Cross-Validation. Parameters: split_data : A dictionary obtained from the splitData method of this package. models : Models to be used such as Support Vector Machine ('svm'), Decision Tree ('dt'), Random Forest ('rf'), Naive Bayes ('nb') and Neural Networks ('nn'). This parameter can be passed like models = ('svm', 'dt', 'rf', 'nb', 'nn'). cv : For splitting samples into two subsets, i.e. training data and for testing data. Following Leave One Out Cross-Validation. n_iter : Number of iterations, i.e number of times the analysis is executed. **kwargs : These will be passed to SVM, DT, RF, NB and NN, please see full lists at: https : //scikit-learn.org/stable/supervised_learning.html#supervised-learning Return:","title":"method CV"},{"location":"calmla/#method-loocv","text":"1 LOOCV ( split_data , models = ( 'svm' , 'dt' ), cv = LeaveOneOut (), n_iter = 10 , ** kwargs ) This module allows to calibrate supervised classification in satellite images through various algorithms and using Leave One Out Cross-Validation. Parameters: split_data : A dictionary obtained from the splitData method of this package. models : Models to be used such as Support Vector Machine ('svm'), Decision Tree ('dt'), Random Forest ('rf'), Naive Bayes ('nb') and Neural Networks ('nn'). This parameter can be passed like models = ('svm', 'dt', 'rf', 'nb', 'nn'). cv : For splitting samples into two subsets, i.e. training data and for testing data. Following Leave One Out Cross-Validation. n_iter : Number of iterations, i.e number of times the analysis is executed. **kwargs : These will be passed to SVM, DT, RF, NB and NN, please see full lists at: https : //scikit-learn.org/stable/supervised_learning.html#supervised-learning Return:","title":"method LOOCV"},{"location":"calmla/#method-mccv","text":"1 2 3 4 5 6 7 8 9 MCCV ( split_data , models = 'svm' , train_size = 0.5 , n_splits = 5 , n_iter = 10 , random_state = None , ** kwargs ) This module allows to calibrate supervised classification in satellite images through various algorithms and using Cross-Validation. Parameters: split_data : A dictionary obtained from the splitData method of this package. models : Models to be used such as Support Vector Machine ('svm'), Decision Tree ('dt'), Random Forest ('rf'), Naive Bayes ('nb') and Neural Networks ('nn'). This parameter can be passed like models = ('svm', 'dt', 'rf', 'nb', 'nn'). cv : For splitting samples into two subsets, i.e. training data and for testing data. Following Leave One Out Cross-Validation. n_iter : Number of iterations, i.e number of times the analysis is executed. **kwargs : These will be passed to SVM, DT, RF, NB and NN, please see full lists at: https : //scikit-learn.org/stable/supervised_learning.html#supervised-learning Return:","title":"method MCCV"},{"location":"calmla/#method-sa","text":"1 SA ( split_data , models = ( 'svm' , 'dt' , 'rf' ), train_size = 0.5 , n_iter = 10 , ** kwargs ) This module allows to calibrate supervised classification in satellite images through various algorithms and using approaches such as Set-Approach. Parameters: split_data : A dictionary obtained from the splitData method of this package. models : Models to be used such as Support Vector Machine ('svm'), Decision Tree ('dt'), Random Forest ('rf'), Naive Bayes ('nb') and Neural Networks ('nn'). This parameter can be passed like models = ('svm', 'dt', 'rf', 'nb', 'nn'). train_size : For splitting samples into two subsets, i.e. training data and for testing data. n_iter : Number of iterations, i.e number of times the analysis is executed. **kwargs : These will be passed to SVM, DT, RF, NB and NN, please see full lists at: https : //scikit-learn.org/stable/supervised_learning.html#supervised-learning Return:","title":"method SA"},{"location":"calmla/#method-splitdata","text":"1 splitData ( random_state = None ) This method is to separate the dataset in predictor variables and the variable to be predicted Parameter: self: Attributes of class calmla. Return: A dictionary with X and y. This file was automatically generated via lazydocs .","title":"method splitData"},{"location":"changelog/","text":"Changelog \u00b6 v0.0.1 - Date \u00b6 Improvement : TBD New Features : TBD","title":"Changelog"},{"location":"changelog/#changelog","text":"","title":"Changelog"},{"location":"changelog/#v001-date","text":"Improvement : TBD New Features : TBD","title":"v0.0.1 - Date"},{"location":"contributing/","text":"Contributing \u00b6 Contributions are welcome, and they are greatly appreciated! Every little bit helps, and credit will always be given. You can contribute in many ways: Types of Contributions \u00b6 Report Bugs \u00b6 Report bugs at https://github.com/ytarazona/scikit-eo/issues . If you are reporting a bug, please include: Your operating system name and version. Any details about your local setup that might be helpful in troubleshooting. Detailed steps to reproduce the bug. Fix Bugs \u00b6 Look through the GitHub issues for bugs. Anything tagged with bug and help wanted is open to whoever wants to implement it. Implement Features \u00b6 Look through the GitHub issues for features. Anything tagged with enhancement and help wanted is open to whoever wants to implement it. Write Documentation \u00b6 scikit-eo could always use more documentation, whether as part of the official scikit-eo docs, in docstrings, or even on the web in blog posts, articles, and such. Submit Feedback \u00b6 The best way to send feedback is to file an issue at https://github.com/ytarazona/scikit-eo/issues . If you are proposing a feature: Explain in detail how it would work. Keep the scope as narrow as possible, to make it easier to implement. Remember that this is a volunteer-driven project, and that contributions are welcome :) Get Started! \u00b6 Ready to contribute? Here's how to set up scikit-eo for local development. Fork the scikit-eo repo on GitHub. Clone your fork locally: 1 $ git clone git@github.com:your_name_here/scikit-eo.git Install your local copy into a virtualenv. Assuming you have virtualenvwrapper installed, this is how you set up your fork for local development: 1 2 3 $ mkvirtualenv scikit-eo $ cd scikit-eo/ $ python setup.py develop Create a branch for local development: 1 $ git checkout -b name-of-your-bugfix-or-feature Now you can make your changes locally. When you're done making changes, check that your changes pass flake8 and the tests, including testing other Python versions with tox: 1 2 3 $ flake8 scikit-eo tests $ python setup.py test or pytest $ tox To get flake8 and tox, just pip install them into your virtualenv. Commit your changes and push your branch to GitHub: 1 2 3 $ git add . $ git commit -m \"Your detailed description of your changes.\" $ git push origin name-of-your-bugfix-or-feature Submit a pull request through the GitHub website. Pull Request Guidelines \u00b6 Before you submit a pull request, check that it meets these guidelines: The pull request should include tests. If the pull request adds functionality, the docs should be updated. Put your new functionality into a function with a docstring, and add the feature to the list in README.rst. The pull request should work for Python 3.5, 3.6, 3.7 and 3.8, and for PyPy. Check https://github.com/ytarazona/scikit-eo/pull_requests and make sure that the tests pass for all supported Python versions. 1","title":"Contributing"},{"location":"contributing/#contributing","text":"Contributions are welcome, and they are greatly appreciated! Every little bit helps, and credit will always be given. You can contribute in many ways:","title":"Contributing"},{"location":"contributing/#types-of-contributions","text":"","title":"Types of Contributions"},{"location":"contributing/#report-bugs","text":"Report bugs at https://github.com/ytarazona/scikit-eo/issues . If you are reporting a bug, please include: Your operating system name and version. Any details about your local setup that might be helpful in troubleshooting. Detailed steps to reproduce the bug.","title":"Report Bugs"},{"location":"contributing/#fix-bugs","text":"Look through the GitHub issues for bugs. Anything tagged with bug and help wanted is open to whoever wants to implement it.","title":"Fix Bugs"},{"location":"contributing/#implement-features","text":"Look through the GitHub issues for features. Anything tagged with enhancement and help wanted is open to whoever wants to implement it.","title":"Implement Features"},{"location":"contributing/#write-documentation","text":"scikit-eo could always use more documentation, whether as part of the official scikit-eo docs, in docstrings, or even on the web in blog posts, articles, and such.","title":"Write Documentation"},{"location":"contributing/#submit-feedback","text":"The best way to send feedback is to file an issue at https://github.com/ytarazona/scikit-eo/issues . If you are proposing a feature: Explain in detail how it would work. Keep the scope as narrow as possible, to make it easier to implement. Remember that this is a volunteer-driven project, and that contributions are welcome :)","title":"Submit Feedback"},{"location":"contributing/#get-started","text":"Ready to contribute? Here's how to set up scikit-eo for local development. Fork the scikit-eo repo on GitHub. Clone your fork locally: 1 $ git clone git@github.com:your_name_here/scikit-eo.git Install your local copy into a virtualenv. Assuming you have virtualenvwrapper installed, this is how you set up your fork for local development: 1 2 3 $ mkvirtualenv scikit-eo $ cd scikit-eo/ $ python setup.py develop Create a branch for local development: 1 $ git checkout -b name-of-your-bugfix-or-feature Now you can make your changes locally. When you're done making changes, check that your changes pass flake8 and the tests, including testing other Python versions with tox: 1 2 3 $ flake8 scikit-eo tests $ python setup.py test or pytest $ tox To get flake8 and tox, just pip install them into your virtualenv. Commit your changes and push your branch to GitHub: 1 2 3 $ git add . $ git commit -m \"Your detailed description of your changes.\" $ git push origin name-of-your-bugfix-or-feature Submit a pull request through the GitHub website.","title":"Get Started!"},{"location":"contributing/#pull-request-guidelines","text":"Before you submit a pull request, check that it meets these guidelines: The pull request should include tests. If the pull request adds functionality, the docs should be updated. Put your new functionality into a function with a docstring, and add the feature to the list in README.rst. The pull request should work for Python 3.5, 3.6, 3.7 and 3.8, and for PyPy. Check https://github.com/ytarazona/scikit-eo/pull_requests and make sure that the tests pass for all supported Python versions. 1","title":"Pull Request Guidelines"},{"location":"deeplearning/","text":"module deeplearning \u00b6 class DL \u00b6 Deep Learning classification in Remote Sensing method __init__ \u00b6 1 __init__ ( image , endmembers , nodata =- 99999 ) Parameter: image: Optical images. It must be rasterio.io.DatasetReader with 3d. endmembers: Endmembers must be a matrix (numpy.ndarray) and with more than one endmember. Rows represent the endmembers and columns represent the spectral bands. The number of bands must be equal to the number of endmembers. E.g. an image with 6 bands, endmembers dimension should be $n*6$, where $n$ is rows with the number of endmembers and 6 is the number of bands (should be equal). In addition, Endmembers must have a field (type int or float) with the names of classes to be predicted. nodata: The NoData value to replace with -99999. method CNN \u00b6 1 CNN () method FullyConnected \u00b6 1 2 3 4 5 6 7 8 9 10 FullyConnected ( hidden_layers = 3 , hidden_units = [ 64 , 32 , 16 ], output_units = 10 , input_shape = ( 6 ,), epochs = 300 , batch_size = 32 , training_split = 0.8 , random_state = None ) This algorithm consiste of a network with a sequence of Dense layers, which area densely connnected (also called fully connected ) neural layers. This is the simplest of deep learning. Parameters: hidden_layers : Number of hidden layers to be used. 3 is for default. hidden_units : Number of units to be used. This is related to 'neurons' in each hidden layers. output_units : Number of clases to be obtained. input_shape : The input shape is generally the shape of the input data provided to the Keras model while training. The model cannot know the shape of the training data. The shape of other tensors(layers) is computed automatically. epochs : Number of iteration, the network will compute the gradients of the weights with regard to the loss on the batch, and update the weights accordingly. batch_size : This break the data into small batches. In deep learning, models do not process antire dataset at once. training_split : For splitting samples into two subsets, i.e. training data and for testing data. random_state : Random state ensures that the splits that you generate are reproducible. Please, see for more details https : //scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html Return: A dictionary with Labels of classification as numpy object, overall accuracy, among others results. This file was automatically generated via lazydocs .","title":"deeplearning module"},{"location":"deeplearning/#module-deeplearning","text":"","title":"module deeplearning"},{"location":"deeplearning/#class-dl","text":"Deep Learning classification in Remote Sensing","title":"class DL"},{"location":"deeplearning/#method-__init__","text":"1 __init__ ( image , endmembers , nodata =- 99999 ) Parameter: image: Optical images. It must be rasterio.io.DatasetReader with 3d. endmembers: Endmembers must be a matrix (numpy.ndarray) and with more than one endmember. Rows represent the endmembers and columns represent the spectral bands. The number of bands must be equal to the number of endmembers. E.g. an image with 6 bands, endmembers dimension should be $n*6$, where $n$ is rows with the number of endmembers and 6 is the number of bands (should be equal). In addition, Endmembers must have a field (type int or float) with the names of classes to be predicted. nodata: The NoData value to replace with -99999.","title":"method __init__"},{"location":"deeplearning/#method-cnn","text":"1 CNN ()","title":"method CNN"},{"location":"deeplearning/#method-fullyconnected","text":"1 2 3 4 5 6 7 8 9 10 FullyConnected ( hidden_layers = 3 , hidden_units = [ 64 , 32 , 16 ], output_units = 10 , input_shape = ( 6 ,), epochs = 300 , batch_size = 32 , training_split = 0.8 , random_state = None ) This algorithm consiste of a network with a sequence of Dense layers, which area densely connnected (also called fully connected ) neural layers. This is the simplest of deep learning. Parameters: hidden_layers : Number of hidden layers to be used. 3 is for default. hidden_units : Number of units to be used. This is related to 'neurons' in each hidden layers. output_units : Number of clases to be obtained. input_shape : The input shape is generally the shape of the input data provided to the Keras model while training. The model cannot know the shape of the training data. The shape of other tensors(layers) is computed automatically. epochs : Number of iteration, the network will compute the gradients of the weights with regard to the loss on the batch, and update the weights accordingly. batch_size : This break the data into small batches. In deep learning, models do not process antire dataset at once. training_split : For splitting samples into two subsets, i.e. training data and for testing data. random_state : Random state ensures that the splits that you generate are reproducible. Please, see for more details https : //scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html Return: A dictionary with Labels of classification as numpy object, overall accuracy, among others results. This file was automatically generated via lazydocs .","title":"method FullyConnected"},{"location":"faq/","text":"FAQ \u00b6","title":"FAQ"},{"location":"faq/#faq","text":"","title":"FAQ"},{"location":"fusionrs/","text":"module fusionrs \u00b6 function fusionrs \u00b6 1 fusionrs ( optical , radar , stand_varb = True , nodata =- 99999 , ** kwargs ) Fusion of images with different observation geometries through Principal Component Analysis (PCA). This algorithm allows to fusion images coming from different spectral sensors (e.g., optical-optical, optical and SAR, or SAR-SAR). It is also possible to obtain the contribution (%) of each variable in the fused image. Parameters: optical : Optical image. It must be rasterio.io.DatasetReader with 3d. radar : Radar image. It must be rasterio.io.DatasetReader with 3d. stand_varb : Logical. If stand.varb = True , the PCA is calculated using the correlation matrix (standardized variables) instead of the covariance matrix (non-standardized variables). nodata : The NoData value to replace with -99999. **kwargs : These will be passed to scikit-learn PCA, please see full lists at: https : //scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html Return: A dictionary. Note: Before executing the function, it is recommended that images coming from different sensors or from the same sensor have a co-registration. The contributions of variables in accounting for the variability in a given principal component are expressed in percentage. Variables that are correlated with PC1 (i.e., Dim.1) and PC2 (i.e., Dim.2) are the most important in explaining the variability in the data set. Variables that do not correlated with any PC or correlated with the last dimensions are variables with low contribution and might be removed to simplify the overall analysis. The contribution is a scaled version of the squared correlation between variables and component axes (or the cosine, from a geometrical point of view) --- this is used to assess the quality of the representation of the variables of the principal component, and it is computed as (cos(variable,axis)^2/total cos2 of the component)\u00d7100. References: - Tarazona, Y., Zabala, A., Pons, X., Broquetas, A., Nowosad, J., and Zurqani, H.A. Fusing Landsat and SAR data for mapping tropical deforestation through machine learning classification and the PVts-\u03b2 non-seasonal detection approach, Canadian Journal of Remote Sensing., vol. 47, no. 5, pp. 677\u2013696, Sep. 2021. This file was automatically generated via lazydocs .","title":"fusionrs module"},{"location":"fusionrs/#module-fusionrs","text":"","title":"module fusionrs"},{"location":"fusionrs/#function-fusionrs","text":"1 fusionrs ( optical , radar , stand_varb = True , nodata =- 99999 , ** kwargs ) Fusion of images with different observation geometries through Principal Component Analysis (PCA). This algorithm allows to fusion images coming from different spectral sensors (e.g., optical-optical, optical and SAR, or SAR-SAR). It is also possible to obtain the contribution (%) of each variable in the fused image. Parameters: optical : Optical image. It must be rasterio.io.DatasetReader with 3d. radar : Radar image. It must be rasterio.io.DatasetReader with 3d. stand_varb : Logical. If stand.varb = True , the PCA is calculated using the correlation matrix (standardized variables) instead of the covariance matrix (non-standardized variables). nodata : The NoData value to replace with -99999. **kwargs : These will be passed to scikit-learn PCA, please see full lists at: https : //scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html Return: A dictionary. Note: Before executing the function, it is recommended that images coming from different sensors or from the same sensor have a co-registration. The contributions of variables in accounting for the variability in a given principal component are expressed in percentage. Variables that are correlated with PC1 (i.e., Dim.1) and PC2 (i.e., Dim.2) are the most important in explaining the variability in the data set. Variables that do not correlated with any PC or correlated with the last dimensions are variables with low contribution and might be removed to simplify the overall analysis. The contribution is a scaled version of the squared correlation between variables and component axes (or the cosine, from a geometrical point of view) --- this is used to assess the quality of the representation of the variables of the principal component, and it is computed as (cos(variable,axis)^2/total cos2 of the component)\u00d7100. References: - Tarazona, Y., Zabala, A., Pons, X., Broquetas, A., Nowosad, J., and Zurqani, H.A. Fusing Landsat and SAR data for mapping tropical deforestation through machine learning classification and the PVts-\u03b2 non-seasonal detection approach, Canadian Journal of Remote Sensing., vol. 47, no. 5, pp. 677\u2013696, Sep. 2021. This file was automatically generated via lazydocs .","title":"function fusionrs"},{"location":"get-started/","text":"Get Started \u00b6 This Get Started is intended as a guide to apply several remote sensing tools in order to analyze and process satellite imagery such as Landsat, Sentinel-2, etc. Various methods including ML/DL, Spectral Mixture Analysis, Calibrations methods, Principal Component Analysis, among others are available in this python package. Content \u00b6 Example 01: Random Forest (RF) classifier Example 02: Calibration methods for supervised classification Example 03: Imagery Fusion - optical/radar Example 04: Confusion matrix by estimated proportions of area with a confidence interval at 95% (1.96) Brief examples \u00b6 Example 01: Random Forest (RF) classifier \u00b6 In this example, in a small region of southern Brazil, optical imagery from Landsat-8 OLI (Operational Land Imager) will be used to classify land cover using the machine learning algorithm Random Forest (RF) (Breiman, 2001) . Four types of land cover will be mapped, i.e., agriculture, forest, bare soil and water. The input data needed is the satellite image and the spectral signatures collected. The output as a dictionary will provide: i) confusion matrix, ii) overall accuracy, iii) kappa index and iv) a classes map. 01. Optical image to be used \u00b6 Landsat-8 OLI (Operational Land Imager) will be used to obtain in order to classify using Random Forest (RF). This image, which is in surface reflectance with bands: Blue -> B2 Green -> B3 Red -> B4 Nir -> B5 Swir1 -> B6 Swir2 -> B7 The image and signatures to be used can be downloaded here : 02. Libraries to be used in this example \u00b6 1 2 3 4 5 6 7 import rasterio import numpy as np from scikeo.mla import MLA import matplotlib.pyplot as plt from dbfread import DBF import matplotlib as mpl import pandas as pd 03. Image and endmembers \u00b6 We will upload a satellite image as a .tif and endmembers as a .dbf . 1 2 3 4 5 6 7 8 9 path_raster = r \"C:\\data\\ml\\LC08_232066_20190727_SR.tif\" img = rasterio . open ( path_raster ) path_endm = r \"C:\\data\\ml\\endmembers.dbf\" endm = DBF ( path_endm ) # endmembers df = pd . DataFrame ( iter ( endm )) df . head () 04. Classifying with Random Forest \u00b6 An instance of mla() : 1 inst = MLA ( image = img , endmembers = endm ) Applying with 70% of data to train: 1 rf_class = inst . SVM ( training_split = 0.7 ) 5.0 Results \u00b6 Dictionary of results 1 rf_class . keys () Overall accuracy 1 rf_class . get ( 'Overall_Accuracy' ) Kappa index 1 rf_class . get ( 'Kappa_Index' ) Confusion matrix or error matrix 1 rf_class . get ( 'Confusion_Matrix' ) 06. Preparing the image before plotting \u00b6 1 2 # Let's define the color palette palette = mpl . colors . ListedColormap ([ \"#2232F9\" , \"#F922AE\" , \"#229954\" , \"#7CED5E\" ]) Applying the plotRGB() algorithm is easy: 1 2 3 4 5 6 7 8 9 10 # Let\u00b4s plot fig , axes = plt . subplots ( nrows = 1 , ncols = 2 , figsize = ( 15 , 9 )) # satellite image plotRGB ( img , title = 'Image in Surface Reflectance' , ax = axes [ 0 ]) # class results axes [ 1 ] . imshow ( svm_class . get ( 'Classification_Map' ), cmap = palette ) axes [ 1 ] . set_title ( \"Classification map\" ) axes [ 1 ] . grid ( False ) Example 02: Calibration methods for supervised classification \u00b6 Given a large number of machine learning algorithms, it is necessary to select the one with the best performance in the classification, i.e., the algorithm in which the training and testing data used converge the learning iteratively to a solution that appears to be satisfactory (Tarazona et al., 2021) . To deal with this, users can apply the calibration methods Leave One Out Cross-Validation (LOOCV), Cross-Validation (CV) and Monte Carlo Cross-Validation (MCCV) in order to calibrate a supervised classification with different algorithms. The input data needed are the spectral signatures collected as a .dbf or .csv . The output will provide a graph with the errors of each classifier obtained. 01. Endmembers as a .dbf \u00b6 1 2 path_endm = \"\\data\\ex_O2 \\\\ endmembers\\endmembers.dbf\" endm = DBF ( path_endm ) 02. An instance of calmla() \u00b6 1 inst = calmla ( endmembers = endm ) 03. Applying the splitData() method \u00b6 1 data = inst . splitData () Calibrating with Monte Carlo Cross-Validation Calibration (MCCV) Parameters : split_data : An instance obtaind with splitData() . models : Support Vector Machine (svm), Decision Tree (dt), Random Forest (rf) and Naive Bayes (nb). n_iter : Number of iterations. 04. Running MCCV \u00b6 1 2 error_mccv = inst . MCCV ( split_data = data , models = ( 'svm' , 'dt' , 'rf' , 'nb' ), n_iter = 10 ) Calibration results: With this result it can be observed that SVM and RF obtained a higher overall accuracy (less error). Therefore, you can use these algorithms to classify a satellite image. Example 03: Imagery Fusion - optical/radar \u00b6 This is an area where scikit-eo provides a novel approach to merge different types of satellite imagery. We are in a case where, after combining different variables into a single output, we want to know the contributions of the different original variables in the data fusion. The fusion of radar and optical images, despite of its well-know use, to improve land cover mapping, currently has no tools that help researchers to integrate or combine those resources. In this third example, users can apply imagery fusion with different observation geometries and different ranges of the electromagnetic spectrum (Tarazona et al., 2021) . The input data needed are the optical satellite image and the radar satellite image, for instance. In scikit-eo we developed the function fusionrs() which provides us with a dictionary with the following image fusion interpretation features: Fused_images : The fusion of both images into a 3-dimensional array (rows, cols, bands). Variance : The variance obtained. Proportion_of_variance : The proportion of the obtained variance. Cumulative_variance : The cumulative variance. Correlation : Correlation of the original bands with the principal components. Contributions_in_% : The contributions of each optical and radar band in the fusion. 01. Loagind dataset \u00b6 Loading a radar and optical imagery with a total of 9 bands. Optical imagery has 6 bands Blue, Green, Red, NIR, SWIR1 and SWIR2, while radar imagery has 3 bandas VV, VH and VV/VH. 1 2 3 4 5 path_optical = \"data/ex_03/LC08_003069_20180906.tif\" optical = rasterio . open ( path_optical ) path_radar = \"data/ex_03/S1_2018_VV_VH.tif\" radar = rasterio . open ( path_radar ) 02. Applying the fusionrs: \u00b6 1 fusion = fusionrs ( optical = optical , radar = radar ) 03. Dictionary of results: \u00b6 1 fusion . keys () 04. Proportion of variance: \u00b6 1 prop_var = fusion . get ( 'Proportion_of_variance' ) 05. Cumulative variance (%): \u00b6 1 cum_var = fusion . get ( 'Cumulative_variance' ) * 100 06. Showing the proportion of variance and cumulative: \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 x_labels = [ 'PC {} ' . format ( i + 1 ) for i in range ( len ( prop_var ))] fig , axes = plt . subplots ( figsize = ( 6 , 5 )) ln1 = axes . plot ( x_labels , prop_var , marker = 'o' , markersize = 6 , label = 'Proportion of variance' ) axes2 = axes . twinx () ln2 = axes2 . plot ( x_labels , cum_var , marker = 'o' , color = 'r' , label = \"Cumulative variance\" ) ln = ln1 + ln2 labs = [ l . get_label () for l in ln ] axes . legend ( ln , labs , loc = 'center right' ) axes . set_xlabel ( \"Principal Component\" ) axes . set_ylabel ( \"Proportion of Variance\" ) axes2 . set_ylabel ( \"Cumulative (%)\" ) axes2 . grid ( False ) plt . show () 07. Contributions of each variable in %: \u00b6 1 fusion . get ( 'Contributions_in_%' ) Here, var1 , var2 , ... var12 refer to Blue , Green , ... VV/VH bands respectively. It can be observed that var2 (Green) has a higher contribution percentage 16.9% than other variables. With respect to radar polarizaciones, we can note that var8 (VH polarization) has a higher contribution 11.8% than other radar bands. 08. Preparing the image: \u00b6 1 2 3 4 5 6 arr = fusion . get ( 'Fused_images' ) ## Let\u00b4s plot fig , axes = plt . subplots ( figsize = ( 8 , 8 )) plotRGB ( arr , bands = [ 1 , 2 , 3 ], title = 'Fusion of optical and radar images' ) plt . show () Example 04: Confusion matrix by estimated proportions of area with a confidence interval at 95% (1.96) \u00b6 In this final example, after obtaining the predicted class map, we are in a case where we want to know the uncertainties of each class. The assessing accuracy and area estimate will be obtained following guidance proposed by (Olofsson et al., 2014) . All that users need are the confusion matrix and a previously obtained predicted class map. confintervalML requires the following parameters: matrix : confusion matrix or error matrix in numpy.ndarray. image_pred : a 2-dimensional array (rows, cols). This array should be the classified image with predicted classes. pixel_size : Pixel size of the classified image. Set by default as 10 meters. In this example is 30 meters (Landsat). conf : Confidence interval. By default is 95% (1.96). nodata : No data must be specified as 0, NaN or any other value. Keep in mind with this parameter. 1 2 3 4 5 6 7 8 9 10 11 #### 01. Load raster data path_raster = r \"\\data\\ex_O4\\ml\\predicted_map.tif\" img = rasterio . open ( path_optical ) . read ( 1 ) #### 02. Load confusion matrix as .csv path_cm = r \"\\data\\ex_O4\\ml\\confusion_matrix.csv\" values = pd . read_csv ( path_radar ) #### 03. Applying the confintervalML: confintervalML ( matrix = values , image_pred = img , pixel_size = 30 , conf = 1.96 , nodata = - 9999 ) Results:","title":"Get Started"},{"location":"get-started/#get-started","text":"This Get Started is intended as a guide to apply several remote sensing tools in order to analyze and process satellite imagery such as Landsat, Sentinel-2, etc. Various methods including ML/DL, Spectral Mixture Analysis, Calibrations methods, Principal Component Analysis, among others are available in this python package.","title":"Get Started"},{"location":"get-started/#content","text":"Example 01: Random Forest (RF) classifier Example 02: Calibration methods for supervised classification Example 03: Imagery Fusion - optical/radar Example 04: Confusion matrix by estimated proportions of area with a confidence interval at 95% (1.96)","title":"Content"},{"location":"get-started/#brief-examples","text":"","title":"Brief examples"},{"location":"get-started/#example-01-random-forest-rf-classifier","text":"In this example, in a small region of southern Brazil, optical imagery from Landsat-8 OLI (Operational Land Imager) will be used to classify land cover using the machine learning algorithm Random Forest (RF) (Breiman, 2001) . Four types of land cover will be mapped, i.e., agriculture, forest, bare soil and water. The input data needed is the satellite image and the spectral signatures collected. The output as a dictionary will provide: i) confusion matrix, ii) overall accuracy, iii) kappa index and iv) a classes map.","title":"Example 01: Random Forest (RF) classifier"},{"location":"get-started/#01-optical-image-to-be-used","text":"Landsat-8 OLI (Operational Land Imager) will be used to obtain in order to classify using Random Forest (RF). This image, which is in surface reflectance with bands: Blue -> B2 Green -> B3 Red -> B4 Nir -> B5 Swir1 -> B6 Swir2 -> B7 The image and signatures to be used can be downloaded here :","title":"01. Optical image to be used"},{"location":"get-started/#02-libraries-to-be-used-in-this-example","text":"1 2 3 4 5 6 7 import rasterio import numpy as np from scikeo.mla import MLA import matplotlib.pyplot as plt from dbfread import DBF import matplotlib as mpl import pandas as pd","title":"02. Libraries to be used in this example"},{"location":"get-started/#03-image-and-endmembers","text":"We will upload a satellite image as a .tif and endmembers as a .dbf . 1 2 3 4 5 6 7 8 9 path_raster = r \"C:\\data\\ml\\LC08_232066_20190727_SR.tif\" img = rasterio . open ( path_raster ) path_endm = r \"C:\\data\\ml\\endmembers.dbf\" endm = DBF ( path_endm ) # endmembers df = pd . DataFrame ( iter ( endm )) df . head ()","title":"03. Image and endmembers"},{"location":"get-started/#04-classifying-with-random-forest","text":"An instance of mla() : 1 inst = MLA ( image = img , endmembers = endm ) Applying with 70% of data to train: 1 rf_class = inst . SVM ( training_split = 0.7 )","title":"04. Classifying with Random Forest"},{"location":"get-started/#50-results","text":"Dictionary of results 1 rf_class . keys () Overall accuracy 1 rf_class . get ( 'Overall_Accuracy' ) Kappa index 1 rf_class . get ( 'Kappa_Index' ) Confusion matrix or error matrix 1 rf_class . get ( 'Confusion_Matrix' )","title":"5.0 Results"},{"location":"get-started/#06-preparing-the-image-before-plotting","text":"1 2 # Let's define the color palette palette = mpl . colors . ListedColormap ([ \"#2232F9\" , \"#F922AE\" , \"#229954\" , \"#7CED5E\" ]) Applying the plotRGB() algorithm is easy: 1 2 3 4 5 6 7 8 9 10 # Let\u00b4s plot fig , axes = plt . subplots ( nrows = 1 , ncols = 2 , figsize = ( 15 , 9 )) # satellite image plotRGB ( img , title = 'Image in Surface Reflectance' , ax = axes [ 0 ]) # class results axes [ 1 ] . imshow ( svm_class . get ( 'Classification_Map' ), cmap = palette ) axes [ 1 ] . set_title ( \"Classification map\" ) axes [ 1 ] . grid ( False )","title":"06. Preparing the image before plotting"},{"location":"get-started/#example-02-calibration-methods-for-supervised-classification","text":"Given a large number of machine learning algorithms, it is necessary to select the one with the best performance in the classification, i.e., the algorithm in which the training and testing data used converge the learning iteratively to a solution that appears to be satisfactory (Tarazona et al., 2021) . To deal with this, users can apply the calibration methods Leave One Out Cross-Validation (LOOCV), Cross-Validation (CV) and Monte Carlo Cross-Validation (MCCV) in order to calibrate a supervised classification with different algorithms. The input data needed are the spectral signatures collected as a .dbf or .csv . The output will provide a graph with the errors of each classifier obtained.","title":"Example 02: Calibration methods for supervised classification"},{"location":"get-started/#01-endmembers-as-a-dbf","text":"1 2 path_endm = \"\\data\\ex_O2 \\\\ endmembers\\endmembers.dbf\" endm = DBF ( path_endm )","title":"01. Endmembers as a .dbf"},{"location":"get-started/#02-an-instance-of-calmla","text":"1 inst = calmla ( endmembers = endm )","title":"02. An instance of calmla()"},{"location":"get-started/#03-applying-the-splitdata-method","text":"1 data = inst . splitData () Calibrating with Monte Carlo Cross-Validation Calibration (MCCV) Parameters : split_data : An instance obtaind with splitData() . models : Support Vector Machine (svm), Decision Tree (dt), Random Forest (rf) and Naive Bayes (nb). n_iter : Number of iterations.","title":"03. Applying the splitData() method"},{"location":"get-started/#04-running-mccv","text":"1 2 error_mccv = inst . MCCV ( split_data = data , models = ( 'svm' , 'dt' , 'rf' , 'nb' ), n_iter = 10 ) Calibration results: With this result it can be observed that SVM and RF obtained a higher overall accuracy (less error). Therefore, you can use these algorithms to classify a satellite image.","title":"04. Running MCCV"},{"location":"get-started/#example-03-imagery-fusion-opticalradar","text":"This is an area where scikit-eo provides a novel approach to merge different types of satellite imagery. We are in a case where, after combining different variables into a single output, we want to know the contributions of the different original variables in the data fusion. The fusion of radar and optical images, despite of its well-know use, to improve land cover mapping, currently has no tools that help researchers to integrate or combine those resources. In this third example, users can apply imagery fusion with different observation geometries and different ranges of the electromagnetic spectrum (Tarazona et al., 2021) . The input data needed are the optical satellite image and the radar satellite image, for instance. In scikit-eo we developed the function fusionrs() which provides us with a dictionary with the following image fusion interpretation features: Fused_images : The fusion of both images into a 3-dimensional array (rows, cols, bands). Variance : The variance obtained. Proportion_of_variance : The proportion of the obtained variance. Cumulative_variance : The cumulative variance. Correlation : Correlation of the original bands with the principal components. Contributions_in_% : The contributions of each optical and radar band in the fusion.","title":"Example 03: Imagery Fusion - optical/radar"},{"location":"get-started/#01-loagind-dataset","text":"Loading a radar and optical imagery with a total of 9 bands. Optical imagery has 6 bands Blue, Green, Red, NIR, SWIR1 and SWIR2, while radar imagery has 3 bandas VV, VH and VV/VH. 1 2 3 4 5 path_optical = \"data/ex_03/LC08_003069_20180906.tif\" optical = rasterio . open ( path_optical ) path_radar = \"data/ex_03/S1_2018_VV_VH.tif\" radar = rasterio . open ( path_radar )","title":"01. Loagind dataset"},{"location":"get-started/#02-applying-the-fusionrs","text":"1 fusion = fusionrs ( optical = optical , radar = radar )","title":"02. Applying the fusionrs:"},{"location":"get-started/#03-dictionary-of-results","text":"1 fusion . keys ()","title":"03. Dictionary of results:"},{"location":"get-started/#04-proportion-of-variance","text":"1 prop_var = fusion . get ( 'Proportion_of_variance' )","title":"04. Proportion of variance:"},{"location":"get-started/#05-cumulative-variance","text":"1 cum_var = fusion . get ( 'Cumulative_variance' ) * 100","title":"05. Cumulative variance (%):"},{"location":"get-started/#06-showing-the-proportion-of-variance-and-cumulative","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 x_labels = [ 'PC {} ' . format ( i + 1 ) for i in range ( len ( prop_var ))] fig , axes = plt . subplots ( figsize = ( 6 , 5 )) ln1 = axes . plot ( x_labels , prop_var , marker = 'o' , markersize = 6 , label = 'Proportion of variance' ) axes2 = axes . twinx () ln2 = axes2 . plot ( x_labels , cum_var , marker = 'o' , color = 'r' , label = \"Cumulative variance\" ) ln = ln1 + ln2 labs = [ l . get_label () for l in ln ] axes . legend ( ln , labs , loc = 'center right' ) axes . set_xlabel ( \"Principal Component\" ) axes . set_ylabel ( \"Proportion of Variance\" ) axes2 . set_ylabel ( \"Cumulative (%)\" ) axes2 . grid ( False ) plt . show ()","title":"06. Showing the proportion of variance and cumulative:"},{"location":"get-started/#07-contributions-of-each-variable-in","text":"1 fusion . get ( 'Contributions_in_%' ) Here, var1 , var2 , ... var12 refer to Blue , Green , ... VV/VH bands respectively. It can be observed that var2 (Green) has a higher contribution percentage 16.9% than other variables. With respect to radar polarizaciones, we can note that var8 (VH polarization) has a higher contribution 11.8% than other radar bands.","title":"07. Contributions of each variable in %:"},{"location":"get-started/#08-preparing-the-image","text":"1 2 3 4 5 6 arr = fusion . get ( 'Fused_images' ) ## Let\u00b4s plot fig , axes = plt . subplots ( figsize = ( 8 , 8 )) plotRGB ( arr , bands = [ 1 , 2 , 3 ], title = 'Fusion of optical and radar images' ) plt . show ()","title":"08. Preparing the image:"},{"location":"get-started/#example-04-confusion-matrix-by-estimated-proportions-of-area-with-a-confidence-interval-at-95-196","text":"In this final example, after obtaining the predicted class map, we are in a case where we want to know the uncertainties of each class. The assessing accuracy and area estimate will be obtained following guidance proposed by (Olofsson et al., 2014) . All that users need are the confusion matrix and a previously obtained predicted class map. confintervalML requires the following parameters: matrix : confusion matrix or error matrix in numpy.ndarray. image_pred : a 2-dimensional array (rows, cols). This array should be the classified image with predicted classes. pixel_size : Pixel size of the classified image. Set by default as 10 meters. In this example is 30 meters (Landsat). conf : Confidence interval. By default is 95% (1.96). nodata : No data must be specified as 0, NaN or any other value. Keep in mind with this parameter. 1 2 3 4 5 6 7 8 9 10 11 #### 01. Load raster data path_raster = r \"\\data\\ex_O4\\ml\\predicted_map.tif\" img = rasterio . open ( path_optical ) . read ( 1 ) #### 02. Load confusion matrix as .csv path_cm = r \"\\data\\ex_O4\\ml\\confusion_matrix.csv\" values = pd . read_csv ( path_radar ) #### 03. Applying the confintervalML: confintervalML ( matrix = values , image_pred = img , pixel_size = 30 , conf = 1.96 , nodata = - 9999 ) Results:","title":"Example 04: Confusion matrix by estimated proportions of area with a confidence interval at 95% (1.96)"},{"location":"installation/","text":"Installation \u00b6 To use scikit-eo it is necessary to install it in your terminal. There are two options to use its functions/classes: 1. From PyPI \u00b6 scikit-eo is available on PyPI , so to install it, run this command in your terminal: 1 pip install scikeo If you don't have pip installed, this Python installation guide can guide you through the process. 2. Installing from source \u00b6 It is also possible to install the latest development version directly from the GitHub repository with: 1 pip install git + https : // github . com / ytarazona / scikit - eo This is the preferred method to install scikit-eo, as it will always install the most recent stable release.","title":"Installation"},{"location":"installation/#installation","text":"To use scikit-eo it is necessary to install it in your terminal. There are two options to use its functions/classes:","title":"Installation"},{"location":"installation/#1-from-pypi","text":"scikit-eo is available on PyPI , so to install it, run this command in your terminal: 1 pip install scikeo If you don't have pip installed, this Python installation guide can guide you through the process.","title":"1. From PyPI"},{"location":"installation/#2-installing-from-source","text":"It is also possible to install the latest development version directly from the GitHub repository with: 1 pip install git + https : // github . com / ytarazona / scikit - eo This is the preferred method to install scikit-eo, as it will always install the most recent stable release.","title":"2. Installing from source"},{"location":"linearTrend/","text":"module linearTrend \u00b6 class linearTrend \u00b6 Linear Trend in Remote Sensing method __init__ \u00b6 1 __init__ ( image , nodata =- 99999 ) Parameters: image : Optical images. It must be rasterio.io.DatasetReader with 3d. nodata : The NoData value to replace with -99999. method LN \u00b6 1 LN ( ** kwargs ) Linear trend is useful for mapping forest degradation, land degradation, etc. This algorithm is capable of obtaining the slope of an ordinary least-squares linear regression and its reliability (p-value). Parameters: **kwargs : These will be passed to LN, please see full lists at: https : //docs.scipy.org/doc/scipy/reference/generated/scipy.stats.linregress.html Return: a dictionary with slope, intercept and p-value obtained. All of them in numpy.ndarray with 2d. Note: Linear regression is widely used to analyze forest degradation or land degradation. Specifically, the slope and its reliability are used as main parameters and they can be obtained with this function. On the other hand, logistic regression allows obtaining a degradation risk map, in other words, it is a probability map. References: - Tarazona, Y., Maria, Miyasiro-Lopez. (2020). Monitoring tropical forest degradation using remote sensing. Challenges and opportunities in the Madre de Dios region, Peru. Remote Sensing Applications: Society and Environment, 19, 100337. - Wilkinson, G.N., Rogers, C.E., 1973. Symbolic descriptions of factorial models for analysis of variance. Appl. Stat. 22, 392-399. - Chambers, J.M., 1992. Statistical Models in S. CRS Press. Note: Linear regression is widely used to analyze forest degradation or land degradation. Specifically, the slope and its reliability are used as main parameters and they can be obtained with this function. On the other hand, logistic regression allows obtaining a degradation risk map, in other words, it is a probability map. method LR \u00b6 1 LR ( col_pos = 0 , ** kwargs ) Logistic Regression is a statistical analysis technique that can measure statistically the relative influence of several factors and explain objectively how values depend on predictor variables. This method is applied to remotely sensed data. Parameters: **kwargs : These will be passed to MLN, please see full lists at: https : //www.statsmodels.org/dev/generated/statsmodels.discrete.discrete_model.Logit.html Return: a dictionary with the summary of logistic regression and an array of probability with 2d. Note: Logistic regression allows obtaining a degradation risk map (for instance), in other words, it is a probability map. References: - Tarazona, Y., Maria, Miyasiro-Lopez. (2020). Monitoring tropical forest degradation using remote sensing. Challenges and opportunities in the Madre de Dios region, Peru. Remote Sensing Applications: Society and Environment, 19, 100337. - Chambers, J.M., 1992. Statistical Models in S. CRS Press. This file was automatically generated via lazydocs .","title":"linearTrend module"},{"location":"linearTrend/#module-lineartrend","text":"","title":"module linearTrend"},{"location":"linearTrend/#class-lineartrend","text":"Linear Trend in Remote Sensing","title":"class linearTrend"},{"location":"linearTrend/#method-__init__","text":"1 __init__ ( image , nodata =- 99999 ) Parameters: image : Optical images. It must be rasterio.io.DatasetReader with 3d. nodata : The NoData value to replace with -99999.","title":"method __init__"},{"location":"linearTrend/#method-ln","text":"1 LN ( ** kwargs ) Linear trend is useful for mapping forest degradation, land degradation, etc. This algorithm is capable of obtaining the slope of an ordinary least-squares linear regression and its reliability (p-value). Parameters: **kwargs : These will be passed to LN, please see full lists at: https : //docs.scipy.org/doc/scipy/reference/generated/scipy.stats.linregress.html Return: a dictionary with slope, intercept and p-value obtained. All of them in numpy.ndarray with 2d. Note: Linear regression is widely used to analyze forest degradation or land degradation. Specifically, the slope and its reliability are used as main parameters and they can be obtained with this function. On the other hand, logistic regression allows obtaining a degradation risk map, in other words, it is a probability map. References: - Tarazona, Y., Maria, Miyasiro-Lopez. (2020). Monitoring tropical forest degradation using remote sensing. Challenges and opportunities in the Madre de Dios region, Peru. Remote Sensing Applications: Society and Environment, 19, 100337. - Wilkinson, G.N., Rogers, C.E., 1973. Symbolic descriptions of factorial models for analysis of variance. Appl. Stat. 22, 392-399. - Chambers, J.M., 1992. Statistical Models in S. CRS Press. Note: Linear regression is widely used to analyze forest degradation or land degradation. Specifically, the slope and its reliability are used as main parameters and they can be obtained with this function. On the other hand, logistic regression allows obtaining a degradation risk map, in other words, it is a probability map.","title":"method LN"},{"location":"linearTrend/#method-lr","text":"1 LR ( col_pos = 0 , ** kwargs ) Logistic Regression is a statistical analysis technique that can measure statistically the relative influence of several factors and explain objectively how values depend on predictor variables. This method is applied to remotely sensed data. Parameters: **kwargs : These will be passed to MLN, please see full lists at: https : //www.statsmodels.org/dev/generated/statsmodels.discrete.discrete_model.Logit.html Return: a dictionary with the summary of logistic regression and an array of probability with 2d. Note: Logistic regression allows obtaining a degradation risk map (for instance), in other words, it is a probability map. References: - Tarazona, Y., Maria, Miyasiro-Lopez. (2020). Monitoring tropical forest degradation using remote sensing. Challenges and opportunities in the Madre de Dios region, Peru. Remote Sensing Applications: Society and Environment, 19, 100337. - Chambers, J.M., 1992. Statistical Models in S. CRS Press. This file was automatically generated via lazydocs .","title":"method LR"},{"location":"mla/","text":"module mla \u00b6 class MLA \u00b6 Supervised classification in Remote Sensing method __init__ \u00b6 1 __init__ ( image , endmembers , nodata =- 99999 ) Parameter: image: Optical images. It must be rasterio.io.DatasetReader with 3d. endmembers: Endmembers must be a matrix (numpy.ndarray) and with more than one endmember. Rows represent the endmembers and columns represent the spectral bands. The number of bands must be equal to the number of endmembers. E.g. an image with 6 bands, endmembers dimension should be $n*6$, where $n$ is rows with the number of endmembers and 6 is the number of bands (should be equal). In addition, Endmembers must have a field (type int or float) with the names of classes to be predicted. nodata: The NoData value to replace with -99999. method DT \u00b6 1 DT ( training_split = 0.8 , random_state = None , ** kwargs ) Decision Tree is also a supervised non-parametric statistical learning technique, where the input data is divided recursively into branches depending on certain decision thresholds until the data are segmented into homogeneous subgroups. This technique has substantial advantages for remote sensing classification problems due to its flexibility, intuitive simplicity, and computational efficiency. DT support raster data read by rasterio (rasterio.io.DatasetReader) as input. Parameters: training_split : For splitting samples into two subsets, i.e. training data and for testing data. **kwargs : These will be passed to DT, please see full lists at: https : //scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier Return: A dictionary containing labels of classification as numpy object, overall accuracy, kappa index, confusion matrix. method NB \u00b6 1 NB ( training_split = 0.8 , random_state = None , ** kwargs ) Naive Bayes classifier is an effective and simple method for image classification based on probability theory. The NB classifier assumes an underlying probabilistic model and captures the uncertainty about the model in a principled way, that is, by calculating the occurrence probabilities of different attribute values for different classes in a training set. NB support raster data read by rasterio (rasterio.io.DatasetReader) as input. Parameters: training_split : For splitting samples into two subsets, i.e. training data and for testing data. **kwargs : These will be passed to SVM, please see full lists at: https : //scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html Return: A dictionary containing labels of classification as numpy object, overall accuracy, kappa index, confusion matrix. method NN \u00b6 1 NN ( training_split = 0.8 , max_iter = 300 , random_state = None , ** kwargs ) This classification consists of a neural network that is organized into several layers, that is, an input layer of predictor variables, one or more layers of hidden nodes, in which each node represents an activation function acting on a weighted input of the previous layers\u2019 outputs, and an output layer. NN support raster data read by rasterio (rasterio.io.DatasetReader) as input. Parameters: training_split : For splitting samples into two subsets, i.e. training data and for testing data. **kwargs : These will be passed to SVM, please see full lists at: https : //scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html Return: A dictionary containing labels of classification as numpy object, overall accuracy, kappa index, confusion matrix. method RF \u00b6 1 RF ( training_split = 0.8 , random_state = None , ** kwargs ) Random Forest is a derivative of Decision Tree which provides an improvement over DT to overcome the weaknesses of a single DT. The prediction model of the RF classifier only requires two parameters to be identified: the number of classification trees desired, known as \u201cntree,\u201d and the number of prediction variables, known as \u201cmtry,\u201d used in each node to make the tree grow. RF support raster data read by rasterio (rasterio.io.DatasetReader) as input. Parameters: training_split : For splitting samples into two subsets, i.e. training data and for testing data. **kwargs : These will be passed to RF, please see full lists at: https : //scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html Return: A dictionary containing labels of classification as numpy object, overall accuracy, kappa index, confusion matrix. method SVM \u00b6 1 SVM ( training_split = 0.8 , random_state = None , kernel = 'linear' , ** kwargs ) The Support Vector Machine (SVM) classifier is a supervised non-parametric statistical learning technique that does not assume a preliminary distribution of input data. Its discrimination criterion is a hyperplane that separates the classes in the multidimensional space in which the samples that have established the same classes are located, generally some training areas. SVM support raster data read by rasterio (rasterio.io.DatasetReader) as input. Parameters: training_split : For splitting samples into two subsets, i.e. training data and for testing data. kernel : {'linear', 'poly', 'rbf', 'sigmoid', 'precomputed'}, default='rbf' Specifies the kernel type to be used in the algorithm. It must be one of 'linear', 'poly', 'rbf', 'sigmoid', 'precomputed' or a callable. If None is given, 'rbf' will be used. See https : //scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC for more details. **kwargs : These will be passed to SVM, please see full lists at: https : //scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC Return: A dictionary containing labels of classification as numpy object, overall accuracy, kappa index, confusion matrix. This file was automatically generated via lazydocs .","title":"mla module"},{"location":"mla/#module-mla","text":"","title":"module mla"},{"location":"mla/#class-mla","text":"Supervised classification in Remote Sensing","title":"class MLA"},{"location":"mla/#method-__init__","text":"1 __init__ ( image , endmembers , nodata =- 99999 ) Parameter: image: Optical images. It must be rasterio.io.DatasetReader with 3d. endmembers: Endmembers must be a matrix (numpy.ndarray) and with more than one endmember. Rows represent the endmembers and columns represent the spectral bands. The number of bands must be equal to the number of endmembers. E.g. an image with 6 bands, endmembers dimension should be $n*6$, where $n$ is rows with the number of endmembers and 6 is the number of bands (should be equal). In addition, Endmembers must have a field (type int or float) with the names of classes to be predicted. nodata: The NoData value to replace with -99999.","title":"method __init__"},{"location":"mla/#method-dt","text":"1 DT ( training_split = 0.8 , random_state = None , ** kwargs ) Decision Tree is also a supervised non-parametric statistical learning technique, where the input data is divided recursively into branches depending on certain decision thresholds until the data are segmented into homogeneous subgroups. This technique has substantial advantages for remote sensing classification problems due to its flexibility, intuitive simplicity, and computational efficiency. DT support raster data read by rasterio (rasterio.io.DatasetReader) as input. Parameters: training_split : For splitting samples into two subsets, i.e. training data and for testing data. **kwargs : These will be passed to DT, please see full lists at: https : //scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier Return: A dictionary containing labels of classification as numpy object, overall accuracy, kappa index, confusion matrix.","title":"method DT"},{"location":"mla/#method-nb","text":"1 NB ( training_split = 0.8 , random_state = None , ** kwargs ) Naive Bayes classifier is an effective and simple method for image classification based on probability theory. The NB classifier assumes an underlying probabilistic model and captures the uncertainty about the model in a principled way, that is, by calculating the occurrence probabilities of different attribute values for different classes in a training set. NB support raster data read by rasterio (rasterio.io.DatasetReader) as input. Parameters: training_split : For splitting samples into two subsets, i.e. training data and for testing data. **kwargs : These will be passed to SVM, please see full lists at: https : //scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html Return: A dictionary containing labels of classification as numpy object, overall accuracy, kappa index, confusion matrix.","title":"method NB"},{"location":"mla/#method-nn","text":"1 NN ( training_split = 0.8 , max_iter = 300 , random_state = None , ** kwargs ) This classification consists of a neural network that is organized into several layers, that is, an input layer of predictor variables, one or more layers of hidden nodes, in which each node represents an activation function acting on a weighted input of the previous layers\u2019 outputs, and an output layer. NN support raster data read by rasterio (rasterio.io.DatasetReader) as input. Parameters: training_split : For splitting samples into two subsets, i.e. training data and for testing data. **kwargs : These will be passed to SVM, please see full lists at: https : //scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html Return: A dictionary containing labels of classification as numpy object, overall accuracy, kappa index, confusion matrix.","title":"method NN"},{"location":"mla/#method-rf","text":"1 RF ( training_split = 0.8 , random_state = None , ** kwargs ) Random Forest is a derivative of Decision Tree which provides an improvement over DT to overcome the weaknesses of a single DT. The prediction model of the RF classifier only requires two parameters to be identified: the number of classification trees desired, known as \u201cntree,\u201d and the number of prediction variables, known as \u201cmtry,\u201d used in each node to make the tree grow. RF support raster data read by rasterio (rasterio.io.DatasetReader) as input. Parameters: training_split : For splitting samples into two subsets, i.e. training data and for testing data. **kwargs : These will be passed to RF, please see full lists at: https : //scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html Return: A dictionary containing labels of classification as numpy object, overall accuracy, kappa index, confusion matrix.","title":"method RF"},{"location":"mla/#method-svm","text":"1 SVM ( training_split = 0.8 , random_state = None , kernel = 'linear' , ** kwargs ) The Support Vector Machine (SVM) classifier is a supervised non-parametric statistical learning technique that does not assume a preliminary distribution of input data. Its discrimination criterion is a hyperplane that separates the classes in the multidimensional space in which the samples that have established the same classes are located, generally some training areas. SVM support raster data read by rasterio (rasterio.io.DatasetReader) as input. Parameters: training_split : For splitting samples into two subsets, i.e. training data and for testing data. kernel : {'linear', 'poly', 'rbf', 'sigmoid', 'precomputed'}, default='rbf' Specifies the kernel type to be used in the algorithm. It must be one of 'linear', 'poly', 'rbf', 'sigmoid', 'precomputed' or a callable. If None is given, 'rbf' will be used. See https : //scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC for more details. **kwargs : These will be passed to SVM, please see full lists at: https : //scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC Return: A dictionary containing labels of classification as numpy object, overall accuracy, kappa index, confusion matrix. This file was automatically generated via lazydocs .","title":"method SVM"},{"location":"pca/","text":"module pca \u00b6 function PCA \u00b6 1 PCA ( image , stand_varb = True , nodata =- 99999 , ** kwargs ) Runing Principal Component Analysis (PCA) with satellite images. This algorithm allows to obtain Principal Components from images either radar or optical coming from different spectral sensors. It is also possible to obtain the contribution (%) of each variable. Parameters: images : Optical or radar image, it must be rasterio.io.DatasetReader with 3d. stand_varb : Logical. If stand.varb = True , the PCA is calculated using the correlation matrix (standardized variables) instead of the covariance matrix (non-standardized variables). nodata : The NoData value to replace with -99999. **kwargs : These will be passed to scikit-learn PCA, please see full lists at: https : //scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html Return: A dictionary. Note: The contributions of variables in accounting for the variability in a given principal component are expressed in percentage. Variables that are correlated with PC1 (i.e., Dim.1) and PC2 (i.e., Dim.2) are the most important in explaining the variability in the data set. Variables that do not correlated with any PC or correlated with the last dimensions are variables with low contribution and might be removed to simplify the overall analysis. The contribution is a scaled version of the squared correlation between variables and component axes (or the cosine, from a geometrical point of view) --- this is used to assess the quality of the representation of the variables of the principal component, and it is computed as (cos(variable,axis)^2/total cos2 of the component)\u00d7100. This file was automatically generated via lazydocs .","title":"pca module"},{"location":"pca/#module-pca","text":"","title":"module pca"},{"location":"pca/#function-pca","text":"1 PCA ( image , stand_varb = True , nodata =- 99999 , ** kwargs ) Runing Principal Component Analysis (PCA) with satellite images. This algorithm allows to obtain Principal Components from images either radar or optical coming from different spectral sensors. It is also possible to obtain the contribution (%) of each variable. Parameters: images : Optical or radar image, it must be rasterio.io.DatasetReader with 3d. stand_varb : Logical. If stand.varb = True , the PCA is calculated using the correlation matrix (standardized variables) instead of the covariance matrix (non-standardized variables). nodata : The NoData value to replace with -99999. **kwargs : These will be passed to scikit-learn PCA, please see full lists at: https : //scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html Return: A dictionary. Note: The contributions of variables in accounting for the variability in a given principal component are expressed in percentage. Variables that are correlated with PC1 (i.e., Dim.1) and PC2 (i.e., Dim.2) are the most important in explaining the variability in the data set. Variables that do not correlated with any PC or correlated with the last dimensions are variables with low contribution and might be removed to simplify the overall analysis. The contribution is a scaled version of the squared correlation between variables and component axes (or the cosine, from a geometrical point of view) --- this is used to assess the quality of the representation of the variables of the principal component, and it is computed as (cos(variable,axis)^2/total cos2 of the component)\u00d7100. This file was automatically generated via lazydocs .","title":"function PCA"},{"location":"plot/","text":"module plot \u00b6 function plotHist \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 plotHist ( image , bands = 1 , bins = 128 , alpha = 0.8 , title = None , xlabel = None , ylabel = None , label = None , ax = None , density = True , ** kwargs ) This function allows to plot satellite images histogram. Parameters: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 - <b>`image`</b>: Optical images. It must be rasterio.io.DatasetReader with 3d or 2d. - <b>`bands`</b>: Must be specified as a number of a list. - <b>`bins`</b>: By default is 128. - <b>`alpha`</b>: Percentage (%) of transparency between 0 and 1. 0 indicates 0% and 1 indicates 100%. By default is 80%. - <b>`title`</b>: Assigned title. - <b>`xlabel`</b>: X axis title. - <b>`ylabel`</b>: Y axis title. - <b>`label`</b>: Labeling the histogram. - <b>`ax`</b>: current axes - <b>`**kwargs`</b>: These will be passed to the matplotlib imshow(), please see full lists at: - <b>`https`</b>: //matplotlib.org/stable/api/_as_gen/matplotlib.axes.Axes.hist.html Return: 1 - <b>`ax `</b>: A histogram of an image. function plotRGB \u00b6 1 2 3 4 5 6 7 8 9 10 plotRGB ( image , bands = [ 3 , 2 , 1 ], stretch = 'std' , title = None , xlabel = None , ylabel = None , ax = None , ** kwargs ) Plotting an image in RGB. This function allows to plot an satellite image in RGB channels. Parameters: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 - <b>`image`</b>: Optical images. It must be rasterio.io.DatasetReader with 3d. - <b>`bands`</b>: A list contain the order of bands to be used in order to plot in RGB. For example, for six bands (blue, green, red, nir, swir1 and swir2), number four (4) indicates the swir1 band, number three (3) indicates the nir band and the number two (2) indicates the red band. - <b>`stretch`</b>: Contrast enhancement using the histogram. There are two options here: i) using standard deviation ('std') and ii) using percentiles ('per'). For default is 'std', which means standard deviation. - <b>`title`</b>: Assigned title. - <b>`xlabel`</b>: X axis title. - <b>`ylabel`</b>: Y axis title. - <b>`ax`</b>: current axes - <b>`**kwargs`</b>: These will be passed to the matplotlib imshow(), please see full lists at: - <b>`https`</b>: //matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.imshow.html Return: 1 - <b>`ax `</b>: Graphic of an image in RGB. This file was automatically generated via lazydocs .","title":"plot module"},{"location":"plot/#module-plot","text":"","title":"module plot"},{"location":"plot/#function-plothist","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 plotHist ( image , bands = 1 , bins = 128 , alpha = 0.8 , title = None , xlabel = None , ylabel = None , label = None , ax = None , density = True , ** kwargs ) This function allows to plot satellite images histogram. Parameters: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 - <b>`image`</b>: Optical images. It must be rasterio.io.DatasetReader with 3d or 2d. - <b>`bands`</b>: Must be specified as a number of a list. - <b>`bins`</b>: By default is 128. - <b>`alpha`</b>: Percentage (%) of transparency between 0 and 1. 0 indicates 0% and 1 indicates 100%. By default is 80%. - <b>`title`</b>: Assigned title. - <b>`xlabel`</b>: X axis title. - <b>`ylabel`</b>: Y axis title. - <b>`label`</b>: Labeling the histogram. - <b>`ax`</b>: current axes - <b>`**kwargs`</b>: These will be passed to the matplotlib imshow(), please see full lists at: - <b>`https`</b>: //matplotlib.org/stable/api/_as_gen/matplotlib.axes.Axes.hist.html Return: 1 - <b>`ax `</b>: A histogram of an image.","title":"function plotHist"},{"location":"plot/#function-plotrgb","text":"1 2 3 4 5 6 7 8 9 10 plotRGB ( image , bands = [ 3 , 2 , 1 ], stretch = 'std' , title = None , xlabel = None , ylabel = None , ax = None , ** kwargs ) Plotting an image in RGB. This function allows to plot an satellite image in RGB channels. Parameters: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 - <b>`image`</b>: Optical images. It must be rasterio.io.DatasetReader with 3d. - <b>`bands`</b>: A list contain the order of bands to be used in order to plot in RGB. For example, for six bands (blue, green, red, nir, swir1 and swir2), number four (4) indicates the swir1 band, number three (3) indicates the nir band and the number two (2) indicates the red band. - <b>`stretch`</b>: Contrast enhancement using the histogram. There are two options here: i) using standard deviation ('std') and ii) using percentiles ('per'). For default is 'std', which means standard deviation. - <b>`title`</b>: Assigned title. - <b>`xlabel`</b>: X axis title. - <b>`ylabel`</b>: Y axis title. - <b>`ax`</b>: current axes - <b>`**kwargs`</b>: These will be passed to the matplotlib imshow(), please see full lists at: - <b>`https`</b>: //matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.imshow.html Return: 1 - <b>`ax `</b>: Graphic of an image in RGB. This file was automatically generated via lazydocs .","title":"function plotRGB"},{"location":"process/","text":"module process \u00b6 function crop \u00b6 1 crop ( image , shp , filename = None , filepath = None ) This algorithm allows to clip a raster (.tif) including a satellite image using a shapefile. Parameters: image : This parameter can be a string with the raster path (e.g., r'/home/image/b3.tif') or it can be a rasterio.io.DatasetReader type. shp : Vector file, tipically shapefile. filename : The image name to be saved. filepath : The path which the image will be stored. Return: A raster in your filepath. function extract \u00b6 1 extract ( image , shp ) This algorithm allows to extract raster values using a shapefile. Parameters: image : Optical images. It must be rasterio.io.DatasetReader with 3d or 2d. shp : Vector file, tipically shapefile. Return: A dataframe with raster values obtained. Note: This function is usually used to extract raster values to be used on machine learning algorithms. function confintervalML \u00b6 1 confintervalML ( matrix , image_pred , pixel_size = 10 , conf = 1.96 , nodata = None ) The error matrix is a simple cross-tabulation of the class labels allocated by the classification of the remotely sensed data against the reference data for the sample sites. The error matrix organizes the acquired sample data in a way that summarizes key results and aids the quantification of accuracy and area. The main diagonal of the error matrix highlights correct classifications while the off-diagonal elements show omission and commission errors. The cell entries and marginal values of the error matrix are fundamental to both accuracy assessment and area estimation. The cell entries of the population error matrix and the parameters derived from it must be estimated from a sample. This function shows how to obtain a confusion matrix by estimated proportions of area with a confidence interval at 95% (1.96). Parameters: matrix : confusion matrix or error matrix in numpy.ndarray. image_pred : Array with 2d (rows, cols). This array should be the image classified with predicted classes. pixel_size : Pixel size of the image classified. By default is 10m of Sentinel-2. conf : Confidence interval. By default is 95%. Return: Information of confusion matrix by proportions of area, overall accuracy, user's accuracy with confidence interval and estimated area with confidence interval as well. Note: Columns and rows in a confusion matrix indicate reference and prediction respectively. Reference: - Olofsson, P., Foody, G.M., Herold, M., Stehman, S.V., Woodcock, C.E., and Wulder, M.A. 2014. \u201cGood practices for estimating area and assessing accuracy of land change.\u201d Remote Sensing of Environment, Vol. 148: 42\u201357. doi:https://doi.org/10.1016/j.rse.2014.02.015. This file was automatically generated via lazydocs .","title":"process module"},{"location":"process/#module-process","text":"","title":"module process"},{"location":"process/#function-crop","text":"1 crop ( image , shp , filename = None , filepath = None ) This algorithm allows to clip a raster (.tif) including a satellite image using a shapefile. Parameters: image : This parameter can be a string with the raster path (e.g., r'/home/image/b3.tif') or it can be a rasterio.io.DatasetReader type. shp : Vector file, tipically shapefile. filename : The image name to be saved. filepath : The path which the image will be stored. Return: A raster in your filepath.","title":"function crop"},{"location":"process/#function-extract","text":"1 extract ( image , shp ) This algorithm allows to extract raster values using a shapefile. Parameters: image : Optical images. It must be rasterio.io.DatasetReader with 3d or 2d. shp : Vector file, tipically shapefile. Return: A dataframe with raster values obtained. Note: This function is usually used to extract raster values to be used on machine learning algorithms.","title":"function extract"},{"location":"process/#function-confintervalml","text":"1 confintervalML ( matrix , image_pred , pixel_size = 10 , conf = 1.96 , nodata = None ) The error matrix is a simple cross-tabulation of the class labels allocated by the classification of the remotely sensed data against the reference data for the sample sites. The error matrix organizes the acquired sample data in a way that summarizes key results and aids the quantification of accuracy and area. The main diagonal of the error matrix highlights correct classifications while the off-diagonal elements show omission and commission errors. The cell entries and marginal values of the error matrix are fundamental to both accuracy assessment and area estimation. The cell entries of the population error matrix and the parameters derived from it must be estimated from a sample. This function shows how to obtain a confusion matrix by estimated proportions of area with a confidence interval at 95% (1.96). Parameters: matrix : confusion matrix or error matrix in numpy.ndarray. image_pred : Array with 2d (rows, cols). This array should be the image classified with predicted classes. pixel_size : Pixel size of the image classified. By default is 10m of Sentinel-2. conf : Confidence interval. By default is 95%. Return: Information of confusion matrix by proportions of area, overall accuracy, user's accuracy with confidence interval and estimated area with confidence interval as well. Note: Columns and rows in a confusion matrix indicate reference and prediction respectively. Reference: - Olofsson, P., Foody, G.M., Herold, M., Stehman, S.V., Woodcock, C.E., and Wulder, M.A. 2014. \u201cGood practices for estimating area and assessing accuracy of land change.\u201d Remote Sensing of Environment, Vol. 148: 42\u201357. doi:https://doi.org/10.1016/j.rse.2014.02.015. This file was automatically generated via lazydocs .","title":"function confintervalML"},{"location":"rkmeans/","text":"module rkmeans \u00b6 function rkmeans \u00b6 1 rkmeans ( image , k , nodata =- 99999 , ** kwargs ) This function allows to classify satellite images using k-means In principle, this function allows to classify satellite images specifying a k value (clusters), however it is recommended to find the optimal value of k using the calkmeans function embedded in this package. Parameters: image : Optical images. It must be rasterio.io.DatasetReader with 3d. k : The number of clusters to be detected. nodata : The NoData value to replace with -99999. **kwargs : These will be passed to scikit-learn KMeans, please see full lists at: https : //scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html Return: Labels of classification as numpy object with 2d. This file was automatically generated via lazydocs .","title":"rkmeans module"},{"location":"rkmeans/#module-rkmeans","text":"","title":"module rkmeans"},{"location":"rkmeans/#function-rkmeans","text":"1 rkmeans ( image , k , nodata =- 99999 , ** kwargs ) This function allows to classify satellite images using k-means In principle, this function allows to classify satellite images specifying a k value (clusters), however it is recommended to find the optimal value of k using the calkmeans function embedded in this package. Parameters: image : Optical images. It must be rasterio.io.DatasetReader with 3d. k : The number of clusters to be detected. nodata : The NoData value to replace with -99999. **kwargs : These will be passed to scikit-learn KMeans, please see full lists at: https : //scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html Return: Labels of classification as numpy object with 2d. This file was automatically generated via lazydocs .","title":"function rkmeans"},{"location":"sma/","text":"module sma \u00b6 function sma \u00b6 1 sma ( image , endmembers , nodata =- 99999 ) The SMA assumes that the energy received within the field of vision of the remote sensor can be considered as the sum of the energies received from each dominant endmember. This function addresses a Linear Mixing Model. A regression analysis is used to obtain the fractions. In least squares inversion algorithms, the common objective is to estimate abundances that minimize the squared error between the actual spectrum and the estimated spectrum. The values of the fractions will be between 0 and 1. Parameters: image : Optical images. It must be rasterio.io.DatasetReader with 3d. endmembers : Endmembers must be a matrix (numpy.ndarray) and with more than one endmember. Rows represent the endmembers and columns represent the spectral bands. The number of bands must be greater than the number of endmembers. E.g. an image with 6 bands, endmembers dimension should be $n*6$, where $n$ is rows with the number of endmembers and 6 is the number of bands (should be equal). nodata : The NoData value to replace with -99999. Return: numpy.ndarray with 2d. References Adams, J. B., Smith, M. O., & Gillespie, A. R. (1993). Imaging spectroscopy: Interpretation based on spectral mixture analysis. In C. M. Pieters & P. Englert (Eds.), Remote geochemical analysis: Elements and mineralogical composition. NY: Cambridge Univ. Press 145-166 pp. Shimabukuro, Y.E. and Smith, J., (1991). The least squares mixing models to generate fraction images derived from remote sensing multispectral data. IEEE Transactions on Geoscience and Remote Sensing, 29, pp. 16-21. Note: A regression analysis is used to obtain the fractions. In least squares inversion algorithms, the common objective is to estimate abundances that minimize the squared error between the actual spectrum and the estimated spectrum. The values of the fractions will be between 0 and 1. This file was automatically generated via lazydocs .","title":"sma module"},{"location":"sma/#module-sma","text":"","title":"module sma"},{"location":"sma/#function-sma","text":"1 sma ( image , endmembers , nodata =- 99999 ) The SMA assumes that the energy received within the field of vision of the remote sensor can be considered as the sum of the energies received from each dominant endmember. This function addresses a Linear Mixing Model. A regression analysis is used to obtain the fractions. In least squares inversion algorithms, the common objective is to estimate abundances that minimize the squared error between the actual spectrum and the estimated spectrum. The values of the fractions will be between 0 and 1. Parameters: image : Optical images. It must be rasterio.io.DatasetReader with 3d. endmembers : Endmembers must be a matrix (numpy.ndarray) and with more than one endmember. Rows represent the endmembers and columns represent the spectral bands. The number of bands must be greater than the number of endmembers. E.g. an image with 6 bands, endmembers dimension should be $n*6$, where $n$ is rows with the number of endmembers and 6 is the number of bands (should be equal). nodata : The NoData value to replace with -99999. Return: numpy.ndarray with 2d. References Adams, J. B., Smith, M. O., & Gillespie, A. R. (1993). Imaging spectroscopy: Interpretation based on spectral mixture analysis. In C. M. Pieters & P. Englert (Eds.), Remote geochemical analysis: Elements and mineralogical composition. NY: Cambridge Univ. Press 145-166 pp. Shimabukuro, Y.E. and Smith, J., (1991). The least squares mixing models to generate fraction images derived from remote sensing multispectral data. IEEE Transactions on Geoscience and Remote Sensing, 29, pp. 16-21. Note: A regression analysis is used to obtain the fractions. In least squares inversion algorithms, the common objective is to estimate abundances that minimize the squared error between the actual spectrum and the estimated spectrum. The values of the fractions will be between 0 and 1. This file was automatically generated via lazydocs .","title":"function sma"},{"location":"tassCap/","text":"module tassCap \u00b6 function tassCap \u00b6 1 tassCap ( image , sat = 'Landsat8OLI' , nodata =- 99999 , scale = None ) The Tasseled-Cap Transformation is a linear transformation method for various remote sensing data. Not only can it perform volume data compression, but it can also provide parameters associated with the physical characteristics, such as brightness, greenness and wetness indices. Parameters: image : Optical images. It must be rasterio.io.DatasetReader with 3d. sat : Specify satellite and sensor type (Landsat5TM, Landsat7ETM or Landsat8OLI). nodata : The NoData value to replace with -99999. scale : Conversion of coefficients values Return: numpy.ndarray with 3d containing brightness, greenness and wetness indices. References: Crist, E.P., R. Laurin, and R.C. Cicone. 1986. Vegetation and soils information contained in transformed Thematic Mapper data. Pages 1465-1470 Ref. ESA SP-254. European Space Agency, Paris, France. http : //www.ciesin.org/docs/005-419/005-419.html. Baig, M.H.A., Shuai, T., Tong, Q., 2014. Derivation of a tasseled cap transformation based on Landsat 8 at-satellite reflectance. Remote Sensing Letters, 5(5), 423-431. Li, B., Ti, C., Zhao, Y., Yan, X., 2016. Estimating Soil Moisture with Landsat Data and Its Application in Extracting the Spatial Distribution of Winter Flooded Paddies. Remote Sensing, 8(1), 38. Note: Currently implemented for satellites such as Landsat-4 TM, Landsat-5 TM, Landsat-7 ETM+, Landsat-8 OLI and Sentinel2. The input data must be in top of atmosphere reflectance (toa). Bands required as input must be ordered as: Consider using the following satellite bands: =============== ================================ Type of Sensor Name of bands =============== ================================ Landsat4TM :blue, green, red, nir, swir1, swir2 Landsat5TM :blue, green, red, nir, swir1, swir2 Landsat7ETM+ :blue, green, red, nir, swir1, swir2 Landsat8OLI :blue, green, red, nir, swir1, swir2 Landsat8OLI-Li2016 :coastal, blue, green, red, nir, swir1, swir2 Sentinel2MSI :coastal, blue, green, red, nir-1, mir-1, mir-2 This file was automatically generated via lazydocs .","title":"tassCap module"},{"location":"tassCap/#module-tasscap","text":"","title":"module tassCap"},{"location":"tassCap/#function-tasscap","text":"1 tassCap ( image , sat = 'Landsat8OLI' , nodata =- 99999 , scale = None ) The Tasseled-Cap Transformation is a linear transformation method for various remote sensing data. Not only can it perform volume data compression, but it can also provide parameters associated with the physical characteristics, such as brightness, greenness and wetness indices. Parameters: image : Optical images. It must be rasterio.io.DatasetReader with 3d. sat : Specify satellite and sensor type (Landsat5TM, Landsat7ETM or Landsat8OLI). nodata : The NoData value to replace with -99999. scale : Conversion of coefficients values Return: numpy.ndarray with 3d containing brightness, greenness and wetness indices. References: Crist, E.P., R. Laurin, and R.C. Cicone. 1986. Vegetation and soils information contained in transformed Thematic Mapper data. Pages 1465-1470 Ref. ESA SP-254. European Space Agency, Paris, France. http : //www.ciesin.org/docs/005-419/005-419.html. Baig, M.H.A., Shuai, T., Tong, Q., 2014. Derivation of a tasseled cap transformation based on Landsat 8 at-satellite reflectance. Remote Sensing Letters, 5(5), 423-431. Li, B., Ti, C., Zhao, Y., Yan, X., 2016. Estimating Soil Moisture with Landsat Data and Its Application in Extracting the Spatial Distribution of Winter Flooded Paddies. Remote Sensing, 8(1), 38. Note: Currently implemented for satellites such as Landsat-4 TM, Landsat-5 TM, Landsat-7 ETM+, Landsat-8 OLI and Sentinel2. The input data must be in top of atmosphere reflectance (toa). Bands required as input must be ordered as: Consider using the following satellite bands: =============== ================================ Type of Sensor Name of bands =============== ================================ Landsat4TM :blue, green, red, nir, swir1, swir2 Landsat5TM :blue, green, red, nir, swir1, swir2 Landsat7ETM+ :blue, green, red, nir, swir1, swir2 Landsat8OLI :blue, green, red, nir, swir1, swir2 Landsat8OLI-Li2016 :coastal, blue, green, red, nir, swir1, swir2 Sentinel2MSI :coastal, blue, green, red, nir-1, mir-1, mir-2 This file was automatically generated via lazydocs .","title":"function tassCap"},{"location":"tutorials/","text":"Tutorials \u00b6 Scikit-eo provides a rich suite of algorithms specifically designed for environmental studies. These include statistical analysis, machine learning, deep learning, data fusion and spatial analysis. Researchers can leverage these tools to explore patterns, relationships, and trends within their datasets, to uncover complex land or forest degradation or mapping and classify the land cover, and generate insightful visualizations, among others tools. Scikit-eo tutorials notebooks \u00b6 Machine_Learning.ipynb Estimated area and uncertainty in Machine Learning.ipynb Calibrating_supervised_classification_in_Remote_Sensing.ipynb K_means_classification.ipynb Fusion_of_radar_and_optical_images.ipynb Spectral_Mixture_Analysis.ipynb Principal_Components_Analysis.ipynb Tasseled_Cap_Transformation.ipynb Linear_trend_analysis.ipynb Logistic_regression_in_remote_sensing.ipynb Atmospheric_Correction.ipynb Plot_an_satellite_image_in_RGB.ipynb Plot_a_satellite_image_histogram.ipynb Deep_Learning_Classification_FullyConnected.ipynb Clipping_an_image.ipynb 1","title":"Tutorials"},{"location":"tutorials/#tutorials","text":"Scikit-eo provides a rich suite of algorithms specifically designed for environmental studies. These include statistical analysis, machine learning, deep learning, data fusion and spatial analysis. Researchers can leverage these tools to explore patterns, relationships, and trends within their datasets, to uncover complex land or forest degradation or mapping and classify the land cover, and generate insightful visualizations, among others tools.","title":"Tutorials"},{"location":"tutorials/#scikit-eo-tutorials-notebooks","text":"Machine_Learning.ipynb Estimated area and uncertainty in Machine Learning.ipynb Calibrating_supervised_classification_in_Remote_Sensing.ipynb K_means_classification.ipynb Fusion_of_radar_and_optical_images.ipynb Spectral_Mixture_Analysis.ipynb Principal_Components_Analysis.ipynb Tasseled_Cap_Transformation.ipynb Linear_trend_analysis.ipynb Logistic_regression_in_remote_sensing.ipynb Atmospheric_Correction.ipynb Plot_an_satellite_image_in_RGB.ipynb Plot_a_satellite_image_histogram.ipynb Deep_Learning_Classification_FullyConnected.ipynb Clipping_an_image.ipynb 1","title":"Scikit-eo tutorials notebooks"},{"location":"writeRaster/","text":"module writeRaster \u00b6 function writeRaster \u00b6 1 writeRaster ( arr , image , filename = None , filepath = None , n = 1 ) This algorithm allows to save array images to raster format (.tif). Parameters: arr : Array object with 2d (rows and cols) or 3d (rows, cols, bands). image : Optical images. It must be read by rasterio.open(). filename : The image name to be saved. filepath : The path which the image will be stored. n : Number of images to be saved. Return: A raster in your filepath. This file was automatically generated via lazydocs .","title":"writeRaster module"},{"location":"writeRaster/#module-writeraster","text":"","title":"module writeRaster"},{"location":"writeRaster/#function-writeraster","text":"1 writeRaster ( arr , image , filename = None , filepath = None , n = 1 ) This algorithm allows to save array images to raster format (.tif). Parameters: arr : Array object with 2d (rows and cols) or 3d (rows, cols, bands). image : Optical images. It must be read by rasterio.open(). filename : The image name to be saved. filepath : The path which the image will be stored. n : Number of images to be saved. Return: A raster in your filepath. This file was automatically generated via lazydocs .","title":"function writeRaster"}]}
>>>>>>> Stashed changes
