<<<<<<< Updated upstream
{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to \u00b6 scikit-eo: A Python package for Remote Sensing Tools \u00b6 Links of interest: \u00b6 GitHub repo: https://github.com/ytarazona/scikit-eo Documentation: https://ytarazona.github.io/scikit-eo/ PyPI: https://pypi.org/project/scikeo/ Notebooks examples: https://github.com/ytarazona/scikit-eo/tree/main/examples Google Colab examples: https://github.com/ytarazona/scikit-eo/tree/main/examples Free software: Apache 2.0 Introduction \u00b6 Now a day, remotely sensed data has increased dramatically. Microwaves and optical images with different spatial and temporal resolutions are available and are using to monitor a variaty of environmental issues such as deforestation, land degradation, crop classifications, among other. Although there are efforts (i.e., Python packages, forums, communities, etc.) to make available line-of-code tools for pre-processing, processing and analysis of satellite imagery, there is still a gap that needs to be filled. In other words, too much time is still spent by many users in developing Python lines of code. Algorithms for mapping land degradation through linear trend of vegetation indices (Tarazona and Miyasiro, 2021), fusion optical and radar images to classify vegetation cover, calibration of machine learning lagorithms, among others, are not available yet. Therefore, scikit-eo is a Python package that provides tools for remote sensing. This package was developed to fill the gaps in remotely sensed data processing tools. Most of the tools are based on scientific publications, and others are useful algorithms that will allow processing to be done in a few lines of code. With these tools, the user will be able to invest time in analyzing the results of their data and not spend time on elaborating lines of code, which can sometimes be stressful. Tools for Remote Sensing \u00b6 Name of functions Description mla Machine Learning calmla Calibrating supervised classification in Remote Sensing rkmeans K-means classification calkmeans This function allows to calibrate the kmeans algorithm. It is possible to obtain the best k value and the best embedded algorithm in kmeans. pca Principal Components Analysis atmosCorr Atmospheric Correction of satellite imagery deepLearning Deep Learning algorithms linearTrend Linear trend is useful for mapping forest degradation or land degradation fusionrs This algorithm allows to fusion images coming from different spectral sensors (e.g., optical-optical, optical and SAR or SAR-SAR). Among many of the qualities of this function, it is possible to obtain the contribution (%) of each variable in the fused image sma Spectral Mixture Analysis - Classification sup-pixel tassCap The Tasseled-Cap Transformation You will find more algorithms!. Installation \u00b6 To use scikit-eo it is necessary to install it. There are two options: 1. From PyPI \u00b6 1 pip install scikeo 2. Installing from source \u00b6 It is also possible to install the latest development version directly from the GitHub repository with: 1 pip install git + https : // github . com / ytarazona / scikit - eo","title":"Home"},{"location":"#welcome-to","text":"","title":"Welcome to"},{"location":"#scikit-eo-a-python-package-for-remote-sensing-tools","text":"","title":"scikit-eo: A Python package for Remote Sensing Tools"},{"location":"#links-of-interest","text":"GitHub repo: https://github.com/ytarazona/scikit-eo Documentation: https://ytarazona.github.io/scikit-eo/ PyPI: https://pypi.org/project/scikeo/ Notebooks examples: https://github.com/ytarazona/scikit-eo/tree/main/examples Google Colab examples: https://github.com/ytarazona/scikit-eo/tree/main/examples Free software: Apache 2.0","title":"Links of interest:"},{"location":"#introduction","text":"Now a day, remotely sensed data has increased dramatically. Microwaves and optical images with different spatial and temporal resolutions are available and are using to monitor a variaty of environmental issues such as deforestation, land degradation, crop classifications, among other. Although there are efforts (i.e., Python packages, forums, communities, etc.) to make available line-of-code tools for pre-processing, processing and analysis of satellite imagery, there is still a gap that needs to be filled. In other words, too much time is still spent by many users in developing Python lines of code. Algorithms for mapping land degradation through linear trend of vegetation indices (Tarazona and Miyasiro, 2021), fusion optical and radar images to classify vegetation cover, calibration of machine learning lagorithms, among others, are not available yet. Therefore, scikit-eo is a Python package that provides tools for remote sensing. This package was developed to fill the gaps in remotely sensed data processing tools. Most of the tools are based on scientific publications, and others are useful algorithms that will allow processing to be done in a few lines of code. With these tools, the user will be able to invest time in analyzing the results of their data and not spend time on elaborating lines of code, which can sometimes be stressful.","title":"Introduction"},{"location":"#tools-for-remote-sensing","text":"Name of functions Description mla Machine Learning calmla Calibrating supervised classification in Remote Sensing rkmeans K-means classification calkmeans This function allows to calibrate the kmeans algorithm. It is possible to obtain the best k value and the best embedded algorithm in kmeans. pca Principal Components Analysis atmosCorr Atmospheric Correction of satellite imagery deepLearning Deep Learning algorithms linearTrend Linear trend is useful for mapping forest degradation or land degradation fusionrs This algorithm allows to fusion images coming from different spectral sensors (e.g., optical-optical, optical and SAR or SAR-SAR). Among many of the qualities of this function, it is possible to obtain the contribution (%) of each variable in the fused image sma Spectral Mixture Analysis - Classification sup-pixel tassCap The Tasseled-Cap Transformation You will find more algorithms!.","title":"Tools for Remote Sensing"},{"location":"#installation","text":"To use scikit-eo it is necessary to install it. There are two options:","title":"Installation"},{"location":"#1-from-pypi","text":"1 pip install scikeo","title":"1. From PyPI"},{"location":"#2-installing-from-source","text":"It is also possible to install the latest development version directly from the GitHub repository with: 1 pip install git + https : // github . com / ytarazona / scikit - eo","title":"2. Installing from source"},{"location":"atmosCorr/","text":"module atmosCorr \u00b6 class atmosCorr \u00b6 Atmospheric Correction in Optical domain method __init__ \u00b6 1 __init__ ( path , nodata =- 99999 ) Parameter: path: String. The folder in which the satellite bands are located. This images could be Landsat Collection 2 Level-1. For example: path = r'/folder/image/raster'. nodata: The NoData value to replace with -99999. method DOS \u00b6 1 DOS ( sat = 'LC08' , mindn = None ) The Dark Object Subtraction Method was proposed by Chavez (1988). This image-based atmospheric correction method considers absolutely critical and valid the existence of a dark object in the scene, which is used in the selection of a minimum value in the haze correction. The most valid dark objects in this kind of correction are areas totally shaded or otherwise areas representing dark water bodies. Parameters: sat : Type of Satellite. It could be Landsat-5 TM, Landsat-8 OLI or Landsat-9 OLI-2. mindn : Min of digital number for each band in a list. Return: An array with Surface Reflectance values with 3d, i.e. (rows, cols, bands). References: - Chavez, P.S. (1988). An Improved Dark-Object Subtraction Technique for Atmospheric Scattering Correction of Multispectral Data. Remote Sensing of Envrironment, 24(3), 459-479. method RAD \u00b6 1 RAD ( sat = 'LC08' ) Conversion to TOA Radiance. Landsat Level-1 data can be converted to TOA spectral radiance using the radiance rescaling factors in the MTL file: L\u03bb = MLQcal + AL where: L\u03bb = TOA spectral radiance (Watts/(m2 srad \u03bcm)) ML = Band-specific multiplicative rescaling factor from the metadata (RADIANCE_MULT_BAND_x, where x is the band number) AL = Band-specific additive rescaling factor from the metadata (RADIANCE_ADD_BAND_x, where x is the band number) Qcal = Quantized and calibrated standard product pixel values (DN) Parameters: sat : Type of Satellite. It could be Landsat-5 TM, Landsat-8 OLI or Landsat-9 OLI-2. Return: An array with radiance values with 3d, i.e. (rows, cols, bands). method TOA \u00b6 1 TOA ( sat = 'LC08' ) A reduction in scene-to-scene variability can be achieved by converting the at-sensor spectral radiance to exoatmospheric TOA reflectance, also known as in-band planetary albedo. Equation to obtain TOA reflectance: \u03c1\u03bb\u2032 = M\u03c1*DN + A\u03c1 \u03c1\u03bb = \u03c1\u03bb\u2032/sin(theta) Parameters: sat : Type of Satellite. It could be Landsat-5 TM, Landsat-8 OLI or Landsat-9 OLI-2. Return: An array with TOA values with 3d, i.e. (rows, cols, bands). This file was automatically generated via lazydocs .","title":"atmosCorr module"},{"location":"atmosCorr/#module-atmoscorr","text":"","title":"module atmosCorr"},{"location":"atmosCorr/#class-atmoscorr","text":"Atmospheric Correction in Optical domain","title":"class atmosCorr"},{"location":"atmosCorr/#method-__init__","text":"1 __init__ ( path , nodata =- 99999 ) Parameter: path: String. The folder in which the satellite bands are located. This images could be Landsat Collection 2 Level-1. For example: path = r'/folder/image/raster'. nodata: The NoData value to replace with -99999.","title":"method __init__"},{"location":"atmosCorr/#method-dos","text":"1 DOS ( sat = 'LC08' , mindn = None ) The Dark Object Subtraction Method was proposed by Chavez (1988). This image-based atmospheric correction method considers absolutely critical and valid the existence of a dark object in the scene, which is used in the selection of a minimum value in the haze correction. The most valid dark objects in this kind of correction are areas totally shaded or otherwise areas representing dark water bodies. Parameters: sat : Type of Satellite. It could be Landsat-5 TM, Landsat-8 OLI or Landsat-9 OLI-2. mindn : Min of digital number for each band in a list. Return: An array with Surface Reflectance values with 3d, i.e. (rows, cols, bands). References: - Chavez, P.S. (1988). An Improved Dark-Object Subtraction Technique for Atmospheric Scattering Correction of Multispectral Data. Remote Sensing of Envrironment, 24(3), 459-479.","title":"method DOS"},{"location":"atmosCorr/#method-rad","text":"1 RAD ( sat = 'LC08' ) Conversion to TOA Radiance. Landsat Level-1 data can be converted to TOA spectral radiance using the radiance rescaling factors in the MTL file: L\u03bb = MLQcal + AL where: L\u03bb = TOA spectral radiance (Watts/(m2 srad \u03bcm)) ML = Band-specific multiplicative rescaling factor from the metadata (RADIANCE_MULT_BAND_x, where x is the band number) AL = Band-specific additive rescaling factor from the metadata (RADIANCE_ADD_BAND_x, where x is the band number) Qcal = Quantized and calibrated standard product pixel values (DN) Parameters: sat : Type of Satellite. It could be Landsat-5 TM, Landsat-8 OLI or Landsat-9 OLI-2. Return: An array with radiance values with 3d, i.e. (rows, cols, bands).","title":"method RAD"},{"location":"atmosCorr/#method-toa","text":"1 TOA ( sat = 'LC08' ) A reduction in scene-to-scene variability can be achieved by converting the at-sensor spectral radiance to exoatmospheric TOA reflectance, also known as in-band planetary albedo. Equation to obtain TOA reflectance: \u03c1\u03bb\u2032 = M\u03c1*DN + A\u03c1 \u03c1\u03bb = \u03c1\u03bb\u2032/sin(theta) Parameters: sat : Type of Satellite. It could be Landsat-5 TM, Landsat-8 OLI or Landsat-9 OLI-2. Return: An array with TOA values with 3d, i.e. (rows, cols, bands). This file was automatically generated via lazydocs .","title":"method TOA"},{"location":"calkmeans/","text":"module calkmeans \u00b6 function calkmeans \u00b6 1 2 3 4 5 6 7 8 9 calkmeans ( image , k = None , algo = ( 'auto' , 'elkan' ), max_iter = 300 , n_iter = 10 , nodata =- 99999 , ** kwargs ) Calibrating kmeans This function allows to calibrate the kmeans algorithm. It is possible to obtain the best 'k' value and the best embedded algorithm in KMmeans. Parameters: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 - <b>`image`</b>: Optical images. It must be rasterio.io.DatasetReader with 3d. - <b>`k`</b>: k This argument is None when the objective is to obtain the best 'k' value. If the objective is to select the best algorithm embedded in kmeans, please specify a 'k' value. - <b>`max_iter`</b>: The maximum number of iterations allowed. Strictly related to KMeans. Please see - <b>`https`</b>: //scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html - <b>`algo`</b>: It can be \"auto\" and 'elkan'. \"auto\" and \"full\" are deprecated and they will be removed in Scikit-Learn 1.3. They are both aliases for \"lloyd\". - <b>`Changed in version 1.1`</b>: Renamed \u201cfull\u201d to \u201clloyd\u201d, and deprecated \u201cauto\u201d and \u201cfull\u201d. Changed \u201cauto\u201d to use \u201clloyd\u201d instead of \u201celkan\u201d. - <b>`n_iter`</b>: Iterations number to obtain the best 'k' value. 'n_iter' must be greater than the number of classes expected to be obtained in the classification. Default is 10. - <b>`nodata`</b>: The NoData value to replace with -99999. - <b>`**kwargs`</b>: These will be passed to scikit-learn KMeans, please see full lists at: - <b>`https`</b>: //scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html. Return: Labels of classification as numpy object with 2d. Note: If the idea is to find the optimal value of 'k' (clusters or classes), k = None as an argument of the function must be put, because the function find 'k' for which the intra-class inertia is stabilized. If the 'k' value is known and the idea is to find the best algorithm embedded in kmeans (that maximizes inter-class distances), k = n, which 'n' is a specific class number, must be put. It can be greater than or equal to 0. This file was automatically generated via lazydocs .","title":"calkmeans module"},{"location":"calkmeans/#module-calkmeans","text":"","title":"module calkmeans"},{"location":"calkmeans/#function-calkmeans","text":"1 2 3 4 5 6 7 8 9 calkmeans ( image , k = None , algo = ( 'auto' , 'elkan' ), max_iter = 300 , n_iter = 10 , nodata =- 99999 , ** kwargs ) Calibrating kmeans This function allows to calibrate the kmeans algorithm. It is possible to obtain the best 'k' value and the best embedded algorithm in KMmeans. Parameters: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 - <b>`image`</b>: Optical images. It must be rasterio.io.DatasetReader with 3d. - <b>`k`</b>: k This argument is None when the objective is to obtain the best 'k' value. If the objective is to select the best algorithm embedded in kmeans, please specify a 'k' value. - <b>`max_iter`</b>: The maximum number of iterations allowed. Strictly related to KMeans. Please see - <b>`https`</b>: //scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html - <b>`algo`</b>: It can be \"auto\" and 'elkan'. \"auto\" and \"full\" are deprecated and they will be removed in Scikit-Learn 1.3. They are both aliases for \"lloyd\". - <b>`Changed in version 1.1`</b>: Renamed \u201cfull\u201d to \u201clloyd\u201d, and deprecated \u201cauto\u201d and \u201cfull\u201d. Changed \u201cauto\u201d to use \u201clloyd\u201d instead of \u201celkan\u201d. - <b>`n_iter`</b>: Iterations number to obtain the best 'k' value. 'n_iter' must be greater than the number of classes expected to be obtained in the classification. Default is 10. - <b>`nodata`</b>: The NoData value to replace with -99999. - <b>`**kwargs`</b>: These will be passed to scikit-learn KMeans, please see full lists at: - <b>`https`</b>: //scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html. Return: Labels of classification as numpy object with 2d. Note: If the idea is to find the optimal value of 'k' (clusters or classes), k = None as an argument of the function must be put, because the function find 'k' for which the intra-class inertia is stabilized. If the 'k' value is known and the idea is to find the best algorithm embedded in kmeans (that maximizes inter-class distances), k = n, which 'n' is a specific class number, must be put. It can be greater than or equal to 0. This file was automatically generated via lazydocs .","title":"function calkmeans"},{"location":"calmla/","text":"module calmla \u00b6 class calmla \u00b6 Calibrating supervised classification in Remote Sensing This module allows to calibrate supervised classification in satellite images through various algorithms and using approaches such as Set-Approach, Leave-One-Out Cross-Validation (LOOCV), Cross-Validation (k-fold) and Monte Carlo Cross-Validation (MCCV) method __init__ \u00b6 1 __init__ ( endmembers ) Parameter: endmembers: Endmembers must be a matrix (numpy.ndarray) and with more than one endmember. Rows represent the endmembers and columns represent the spectral bands. The number of bands must be equal to the number of endmembers. E.g. an image with 6 bands, endmembers dimension should be $n*6$, where $n$ is rows with the number of endmembers and 6 is the number of bands (should be equal). In addition, Endmembers must have a field (type int or float) with the names of classes to be predicted. method CV \u00b6 1 2 3 4 5 6 7 8 CV ( split_data , models = ( 'svm' , 'dt' ), k = 5 , n_iter = 10 , random_state = None , ** kwargs ) This module allows to calibrate supervised classification in satellite images through various algorithms and using Cross-Validation. Parameters: split_data : A dictionary obtained from the splitData method of this package. models : Models to be used such as Support Vector Machine ('svm'), Decision Tree ('dt'), Random Forest ('rf'), Naive Bayes ('nb') and Neural Networks ('nn'). This parameter can be passed like models = ('svm', 'dt', 'rf', 'nb', 'nn'). cv : For splitting samples into two subsets, i.e. training data and for testing data. Following Leave One Out Cross-Validation. n_iter : Number of iterations, i.e number of times the analysis is executed. **kwargs : These will be passed to SVM, DT, RF, NB and NN, please see full lists at: https : //scikit-learn.org/stable/supervised_learning.html#supervised-learning Return: method LOOCV \u00b6 1 LOOCV ( split_data , models = ( 'svm' , 'dt' ), cv = LeaveOneOut (), n_iter = 10 , ** kwargs ) This module allows to calibrate supervised classification in satellite images through various algorithms and using Leave One Out Cross-Validation. Parameters: split_data : A dictionary obtained from the splitData method of this package. models : Models to be used such as Support Vector Machine ('svm'), Decision Tree ('dt'), Random Forest ('rf'), Naive Bayes ('nb') and Neural Networks ('nn'). This parameter can be passed like models = ('svm', 'dt', 'rf', 'nb', 'nn'). cv : For splitting samples into two subsets, i.e. training data and for testing data. Following Leave One Out Cross-Validation. n_iter : Number of iterations, i.e number of times the analysis is executed. **kwargs : These will be passed to SVM, DT, RF, NB and NN, please see full lists at: https : //scikit-learn.org/stable/supervised_learning.html#supervised-learning Return: method MCCV \u00b6 1 2 3 4 5 6 7 8 9 MCCV ( split_data , models = 'svm' , train_size = 0.5 , n_splits = 5 , n_iter = 10 , random_state = None , ** kwargs ) This module allows to calibrate supervised classification in satellite images through various algorithms and using Cross-Validation. Parameters: split_data : A dictionary obtained from the splitData method of this package. models : Models to be used such as Support Vector Machine ('svm'), Decision Tree ('dt'), Random Forest ('rf'), Naive Bayes ('nb') and Neural Networks ('nn'). This parameter can be passed like models = ('svm', 'dt', 'rf', 'nb', 'nn'). cv : For splitting samples into two subsets, i.e. training data and for testing data. Following Leave One Out Cross-Validation. n_iter : Number of iterations, i.e number of times the analysis is executed. **kwargs : These will be passed to SVM, DT, RF, NB and NN, please see full lists at: https : //scikit-learn.org/stable/supervised_learning.html#supervised-learning Return: method SA \u00b6 1 SA ( split_data , models = ( 'svm' , 'dt' , 'rf' ), train_size = 0.5 , n_iter = 10 , ** kwargs ) This module allows to calibrate supervised classification in satellite images through various algorithms and using approaches such as Set-Approach. Parameters: split_data : A dictionary obtained from the splitData method of this package. models : Models to be used such as Support Vector Machine ('svm'), Decision Tree ('dt'), Random Forest ('rf'), Naive Bayes ('nb') and Neural Networks ('nn'). This parameter can be passed like models = ('svm', 'dt', 'rf', 'nb', 'nn'). train_size : For splitting samples into two subsets, i.e. training data and for testing data. n_iter : Number of iterations, i.e number of times the analysis is executed. **kwargs : These will be passed to SVM, DT, RF, NB and NN, please see full lists at: https : //scikit-learn.org/stable/supervised_learning.html#supervised-learning Return: method splitData \u00b6 1 splitData ( random_state = None ) This method is to separate the dataset in predictor variables and the variable to be predicted Parameter: self: Attributes of class calmla. Return: A dictionary with X and y. This file was automatically generated via lazydocs .","title":"calmla module"},{"location":"calmla/#module-calmla","text":"","title":"module calmla"},{"location":"calmla/#class-calmla","text":"Calibrating supervised classification in Remote Sensing This module allows to calibrate supervised classification in satellite images through various algorithms and using approaches such as Set-Approach, Leave-One-Out Cross-Validation (LOOCV), Cross-Validation (k-fold) and Monte Carlo Cross-Validation (MCCV)","title":"class calmla"},{"location":"calmla/#method-__init__","text":"1 __init__ ( endmembers ) Parameter: endmembers: Endmembers must be a matrix (numpy.ndarray) and with more than one endmember. Rows represent the endmembers and columns represent the spectral bands. The number of bands must be equal to the number of endmembers. E.g. an image with 6 bands, endmembers dimension should be $n*6$, where $n$ is rows with the number of endmembers and 6 is the number of bands (should be equal). In addition, Endmembers must have a field (type int or float) with the names of classes to be predicted.","title":"method __init__"},{"location":"calmla/#method-cv","text":"1 2 3 4 5 6 7 8 CV ( split_data , models = ( 'svm' , 'dt' ), k = 5 , n_iter = 10 , random_state = None , ** kwargs ) This module allows to calibrate supervised classification in satellite images through various algorithms and using Cross-Validation. Parameters: split_data : A dictionary obtained from the splitData method of this package. models : Models to be used such as Support Vector Machine ('svm'), Decision Tree ('dt'), Random Forest ('rf'), Naive Bayes ('nb') and Neural Networks ('nn'). This parameter can be passed like models = ('svm', 'dt', 'rf', 'nb', 'nn'). cv : For splitting samples into two subsets, i.e. training data and for testing data. Following Leave One Out Cross-Validation. n_iter : Number of iterations, i.e number of times the analysis is executed. **kwargs : These will be passed to SVM, DT, RF, NB and NN, please see full lists at: https : //scikit-learn.org/stable/supervised_learning.html#supervised-learning Return:","title":"method CV"},{"location":"calmla/#method-loocv","text":"1 LOOCV ( split_data , models = ( 'svm' , 'dt' ), cv = LeaveOneOut (), n_iter = 10 , ** kwargs ) This module allows to calibrate supervised classification in satellite images through various algorithms and using Leave One Out Cross-Validation. Parameters: split_data : A dictionary obtained from the splitData method of this package. models : Models to be used such as Support Vector Machine ('svm'), Decision Tree ('dt'), Random Forest ('rf'), Naive Bayes ('nb') and Neural Networks ('nn'). This parameter can be passed like models = ('svm', 'dt', 'rf', 'nb', 'nn'). cv : For splitting samples into two subsets, i.e. training data and for testing data. Following Leave One Out Cross-Validation. n_iter : Number of iterations, i.e number of times the analysis is executed. **kwargs : These will be passed to SVM, DT, RF, NB and NN, please see full lists at: https : //scikit-learn.org/stable/supervised_learning.html#supervised-learning Return:","title":"method LOOCV"},{"location":"calmla/#method-mccv","text":"1 2 3 4 5 6 7 8 9 MCCV ( split_data , models = 'svm' , train_size = 0.5 , n_splits = 5 , n_iter = 10 , random_state = None , ** kwargs ) This module allows to calibrate supervised classification in satellite images through various algorithms and using Cross-Validation. Parameters: split_data : A dictionary obtained from the splitData method of this package. models : Models to be used such as Support Vector Machine ('svm'), Decision Tree ('dt'), Random Forest ('rf'), Naive Bayes ('nb') and Neural Networks ('nn'). This parameter can be passed like models = ('svm', 'dt', 'rf', 'nb', 'nn'). cv : For splitting samples into two subsets, i.e. training data and for testing data. Following Leave One Out Cross-Validation. n_iter : Number of iterations, i.e number of times the analysis is executed. **kwargs : These will be passed to SVM, DT, RF, NB and NN, please see full lists at: https : //scikit-learn.org/stable/supervised_learning.html#supervised-learning Return:","title":"method MCCV"},{"location":"calmla/#method-sa","text":"1 SA ( split_data , models = ( 'svm' , 'dt' , 'rf' ), train_size = 0.5 , n_iter = 10 , ** kwargs ) This module allows to calibrate supervised classification in satellite images through various algorithms and using approaches such as Set-Approach. Parameters: split_data : A dictionary obtained from the splitData method of this package. models : Models to be used such as Support Vector Machine ('svm'), Decision Tree ('dt'), Random Forest ('rf'), Naive Bayes ('nb') and Neural Networks ('nn'). This parameter can be passed like models = ('svm', 'dt', 'rf', 'nb', 'nn'). train_size : For splitting samples into two subsets, i.e. training data and for testing data. n_iter : Number of iterations, i.e number of times the analysis is executed. **kwargs : These will be passed to SVM, DT, RF, NB and NN, please see full lists at: https : //scikit-learn.org/stable/supervised_learning.html#supervised-learning Return:","title":"method SA"},{"location":"calmla/#method-splitdata","text":"1 splitData ( random_state = None ) This method is to separate the dataset in predictor variables and the variable to be predicted Parameter: self: Attributes of class calmla. Return: A dictionary with X and y. This file was automatically generated via lazydocs .","title":"method splitData"},{"location":"changelog/","text":"Changelog \u00b6 v0.0.1 - Date \u00b6 Improvement : TBD New Features : TBD","title":"Changelog"},{"location":"changelog/#changelog","text":"","title":"Changelog"},{"location":"changelog/#v001-date","text":"Improvement : TBD New Features : TBD","title":"v0.0.1 - Date"},{"location":"contributing/","text":"Contributing \u00b6 Contributions are welcome, and they are greatly appreciated! Every little bit helps, and credit will always be given. You can contribute in many ways: Types of Contributions \u00b6 Report Bugs \u00b6 Report bugs at https://github.com/ytarazona/scikit-eo/issues . If you are reporting a bug, please include: Your operating system name and version. Any details about your local setup that might be helpful in troubleshooting. Detailed steps to reproduce the bug. Fix Bugs \u00b6 Look through the GitHub issues for bugs. Anything tagged with bug and help wanted is open to whoever wants to implement it. Implement Features \u00b6 Look through the GitHub issues for features. Anything tagged with enhancement and help wanted is open to whoever wants to implement it. Write Documentation \u00b6 scikit-eo could always use more documentation, whether as part of the official scikit-eo docs, in docstrings, or even on the web in blog posts, articles, and such. Submit Feedback \u00b6 The best way to send feedback is to file an issue at https://github.com/ytarazona/scikit-eo/issues . If you are proposing a feature: Explain in detail how it would work. Keep the scope as narrow as possible, to make it easier to implement. Remember that this is a volunteer-driven project, and that contributions are welcome :) Get Started! \u00b6 Ready to contribute? Here's how to set up scikit-eo for local development. Fork the scikit-eo repo on GitHub. Clone your fork locally: 1 $ git clone git@github.com:your_name_here/scikit-eo.git Install your local copy into a virtualenv. Assuming you have virtualenvwrapper installed, this is how you set up your fork for local development: 1 2 3 $ mkvirtualenv scikit-eo $ cd scikit-eo/ $ python setup.py develop Create a branch for local development: 1 $ git checkout -b name-of-your-bugfix-or-feature Now you can make your changes locally. When you're done making changes, check that your changes pass flake8 and the tests, including testing other Python versions with tox: 1 2 3 $ flake8 scikit-eo tests $ python setup.py test or pytest $ tox To get flake8 and tox, just pip install them into your virtualenv. Commit your changes and push your branch to GitHub: 1 2 3 $ git add . $ git commit -m \"Your detailed description of your changes.\" $ git push origin name-of-your-bugfix-or-feature Submit a pull request through the GitHub website. Pull Request Guidelines \u00b6 Before you submit a pull request, check that it meets these guidelines: The pull request should include tests. If the pull request adds functionality, the docs should be updated. Put your new functionality into a function with a docstring, and add the feature to the list in README.rst. The pull request should work for Python 3.5, 3.6, 3.7 and 3.8, and for PyPy. Check https://github.com/ytarazona/scikit-eo/pull_requests and make sure that the tests pass for all supported Python versions. 1","title":"Contributing"},{"location":"contributing/#contributing","text":"Contributions are welcome, and they are greatly appreciated! Every little bit helps, and credit will always be given. You can contribute in many ways:","title":"Contributing"},{"location":"contributing/#types-of-contributions","text":"","title":"Types of Contributions"},{"location":"contributing/#report-bugs","text":"Report bugs at https://github.com/ytarazona/scikit-eo/issues . If you are reporting a bug, please include: Your operating system name and version. Any details about your local setup that might be helpful in troubleshooting. Detailed steps to reproduce the bug.","title":"Report Bugs"},{"location":"contributing/#fix-bugs","text":"Look through the GitHub issues for bugs. Anything tagged with bug and help wanted is open to whoever wants to implement it.","title":"Fix Bugs"},{"location":"contributing/#implement-features","text":"Look through the GitHub issues for features. Anything tagged with enhancement and help wanted is open to whoever wants to implement it.","title":"Implement Features"},{"location":"contributing/#write-documentation","text":"scikit-eo could always use more documentation, whether as part of the official scikit-eo docs, in docstrings, or even on the web in blog posts, articles, and such.","title":"Write Documentation"},{"location":"contributing/#submit-feedback","text":"The best way to send feedback is to file an issue at https://github.com/ytarazona/scikit-eo/issues . If you are proposing a feature: Explain in detail how it would work. Keep the scope as narrow as possible, to make it easier to implement. Remember that this is a volunteer-driven project, and that contributions are welcome :)","title":"Submit Feedback"},{"location":"contributing/#get-started","text":"Ready to contribute? Here's how to set up scikit-eo for local development. Fork the scikit-eo repo on GitHub. Clone your fork locally: 1 $ git clone git@github.com:your_name_here/scikit-eo.git Install your local copy into a virtualenv. Assuming you have virtualenvwrapper installed, this is how you set up your fork for local development: 1 2 3 $ mkvirtualenv scikit-eo $ cd scikit-eo/ $ python setup.py develop Create a branch for local development: 1 $ git checkout -b name-of-your-bugfix-or-feature Now you can make your changes locally. When you're done making changes, check that your changes pass flake8 and the tests, including testing other Python versions with tox: 1 2 3 $ flake8 scikit-eo tests $ python setup.py test or pytest $ tox To get flake8 and tox, just pip install them into your virtualenv. Commit your changes and push your branch to GitHub: 1 2 3 $ git add . $ git commit -m \"Your detailed description of your changes.\" $ git push origin name-of-your-bugfix-or-feature Submit a pull request through the GitHub website.","title":"Get Started!"},{"location":"contributing/#pull-request-guidelines","text":"Before you submit a pull request, check that it meets these guidelines: The pull request should include tests. If the pull request adds functionality, the docs should be updated. Put your new functionality into a function with a docstring, and add the feature to the list in README.rst. The pull request should work for Python 3.5, 3.6, 3.7 and 3.8, and for PyPy. Check https://github.com/ytarazona/scikit-eo/pull_requests and make sure that the tests pass for all supported Python versions. 1","title":"Pull Request Guidelines"},{"location":"crop/","text":"module crop \u00b6 function crop \u00b6 1 crop ( image , shp , filename = None , filepath = None ) This algorithm allows to clip a raster (.tif) including a satellite image using a shapefile. Parameters: image : This parameter can be a string with the raster path (e.g., r'/home/image/b3.tif') or it can be a rasterio.io.DatasetReader type. shp : vector file, tipically shapefile. filename : The image name to be saved. filepath : The path which the image will be stored. Return: A raster in your filepath. This file was automatically generated via lazydocs .","title":"crop module"},{"location":"crop/#module-crop","text":"","title":"module crop"},{"location":"crop/#function-crop","text":"1 crop ( image , shp , filename = None , filepath = None ) This algorithm allows to clip a raster (.tif) including a satellite image using a shapefile. Parameters: image : This parameter can be a string with the raster path (e.g., r'/home/image/b3.tif') or it can be a rasterio.io.DatasetReader type. shp : vector file, tipically shapefile. filename : The image name to be saved. filepath : The path which the image will be stored. Return: A raster in your filepath. This file was automatically generated via lazydocs .","title":"function crop"},{"location":"deeplearning/","text":"module deeplearning \u00b6 class DL \u00b6 Deep Learning classification in Remote Sensing method __init__ \u00b6 1 __init__ ( image , endmembers , nodata =- 99999 ) Parameter: image: Optical images. It must be rasterio.io.DatasetReader with 3d. endmembers: Endmembers must be a matrix (numpy.ndarray) and with more than one endmember. Rows represent the endmembers and columns represent the spectral bands. The number of bands must be equal to the number of endmembers. E.g. an image with 6 bands, endmembers dimension should be $n*6$, where $n$ is rows with the number of endmembers and 6 is the number of bands (should be equal). In addition, Endmembers must have a field (type int or float) with the names of classes to be predicted. nodata: The NoData value to replace with -99999. method CNN \u00b6 1 CNN () method FullyConnected \u00b6 1 2 3 4 5 6 7 8 9 10 FullyConnected ( hidden_layers = 3 , hidden_units = [ 64 , 32 , 16 ], output_units = 10 , input_shape = ( 6 ,), epochs = 300 , batch_size = 32 , training_split = 0.8 , random_state = None ) This algorithm consiste of a network with a sequence of Dense layers, which area densely connnected (also called fully connected ) neural layers. This is the simplest of deep learning. Parameters: hidden_layers : Number of hidden layers to be used. 3 is for default. hidden_units : Number of units to be used. This is related to 'neurons' in each hidden layers. output_units : Number of clases to be obtained. input_shape : The input shape is generally the shape of the input data provided to the Keras model while training. The model cannot know the shape of the training data. The shape of other tensors(layers) is computed automatically. epochs : Number of iteration, the network will compute the gradients of the weights with regard to the loss on the batch, and update the weights accordingly. batch_size : This break the data into small batches. In deep learning, models do not process antire dataset at once. training_split : For splitting samples into two subsets, i.e. training data and for testing data. random_state : Random state ensures that the splits that you generate are reproducible. Please, see for more details https : //scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html Return: A dictionary with Labels of classification as numpy object, overall accuracy, among others results. This file was automatically generated via lazydocs .","title":"deeplearning module"},{"location":"deeplearning/#module-deeplearning","text":"","title":"module deeplearning"},{"location":"deeplearning/#class-dl","text":"Deep Learning classification in Remote Sensing","title":"class DL"},{"location":"deeplearning/#method-__init__","text":"1 __init__ ( image , endmembers , nodata =- 99999 ) Parameter: image: Optical images. It must be rasterio.io.DatasetReader with 3d. endmembers: Endmembers must be a matrix (numpy.ndarray) and with more than one endmember. Rows represent the endmembers and columns represent the spectral bands. The number of bands must be equal to the number of endmembers. E.g. an image with 6 bands, endmembers dimension should be $n*6$, where $n$ is rows with the number of endmembers and 6 is the number of bands (should be equal). In addition, Endmembers must have a field (type int or float) with the names of classes to be predicted. nodata: The NoData value to replace with -99999.","title":"method __init__"},{"location":"deeplearning/#method-cnn","text":"1 CNN ()","title":"method CNN"},{"location":"deeplearning/#method-fullyconnected","text":"1 2 3 4 5 6 7 8 9 10 FullyConnected ( hidden_layers = 3 , hidden_units = [ 64 , 32 , 16 ], output_units = 10 , input_shape = ( 6 ,), epochs = 300 , batch_size = 32 , training_split = 0.8 , random_state = None ) This algorithm consiste of a network with a sequence of Dense layers, which area densely connnected (also called fully connected ) neural layers. This is the simplest of deep learning. Parameters: hidden_layers : Number of hidden layers to be used. 3 is for default. hidden_units : Number of units to be used. This is related to 'neurons' in each hidden layers. output_units : Number of clases to be obtained. input_shape : The input shape is generally the shape of the input data provided to the Keras model while training. The model cannot know the shape of the training data. The shape of other tensors(layers) is computed automatically. epochs : Number of iteration, the network will compute the gradients of the weights with regard to the loss on the batch, and update the weights accordingly. batch_size : This break the data into small batches. In deep learning, models do not process antire dataset at once. training_split : For splitting samples into two subsets, i.e. training data and for testing data. random_state : Random state ensures that the splits that you generate are reproducible. Please, see for more details https : //scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html Return: A dictionary with Labels of classification as numpy object, overall accuracy, among others results. This file was automatically generated via lazydocs .","title":"method FullyConnected"},{"location":"faq/","text":"FAQ \u00b6","title":"FAQ"},{"location":"faq/#faq","text":"","title":"FAQ"},{"location":"fusionrs/","text":"module fusionrs \u00b6 function fusionrs \u00b6 1 fusionrs ( optical , radar , stand_varb = True , nodata =- 99999 , ** kwargs ) Fusion of images with different observation geometries through Principal Component Analysis (PCA). This algorithm allows to fusion images coming from different spectral sensors (e.g., optical-optical, optical and SAR, or SAR-SAR). It is also possible to obtain the contribution (%) of each variable in the fused image. Parameters: optical : Optical image. It must be rasterio.io.DatasetReader with 3d. radar : Radar image. It must be rasterio.io.DatasetReader with 3d. stand_varb : Logical. If stand.varb = True , the PCA is calculated using the correlation matrix (standardized variables) instead of the covariance matrix (non-standardized variables). nodata : The NoData value to replace with -99999. **kwargs : These will be passed to scikit-learn PCA, please see full lists at: https : //scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html Return: A dictionary. Note: Before executing the function, it is recommended that images coming from different sensors or from the same sensor have a co-registration. The contributions of variables in accounting for the variability in a given principal component are expressed in percentage. Variables that are correlated with PC1 (i.e., Dim.1) and PC2 (i.e., Dim.2) are the most important in explaining the variability in the data set. Variables that do not correlated with any PC or correlated with the last dimensions are variables with low contribution and might be removed to simplify the overall analysis. The contribution is a scaled version of the squared correlation between variables and component axes (or the cosine, from a geometrical point of view) --- this is used to assess the quality of the representation of the variables of the principal component, and it is computed as (cos(variable,axis)^2/total cos2 of the component)\u00d7100. References: - Tarazona, Y., Zabala, A., Pons, X., Broquetas, A., Nowosad, J., and Zurqani, H.A. Fusing Landsat and SAR data for mapping tropical deforestation through machine learning classification and the PVts-\u03b2 non-seasonal detection approach, Canadian Journal of Remote Sensing., vol. 47, no. 5, pp. 677\u2013696, Sep. 2021. This file was automatically generated via lazydocs .","title":"fusionrs module"},{"location":"fusionrs/#module-fusionrs","text":"","title":"module fusionrs"},{"location":"fusionrs/#function-fusionrs","text":"1 fusionrs ( optical , radar , stand_varb = True , nodata =- 99999 , ** kwargs ) Fusion of images with different observation geometries through Principal Component Analysis (PCA). This algorithm allows to fusion images coming from different spectral sensors (e.g., optical-optical, optical and SAR, or SAR-SAR). It is also possible to obtain the contribution (%) of each variable in the fused image. Parameters: optical : Optical image. It must be rasterio.io.DatasetReader with 3d. radar : Radar image. It must be rasterio.io.DatasetReader with 3d. stand_varb : Logical. If stand.varb = True , the PCA is calculated using the correlation matrix (standardized variables) instead of the covariance matrix (non-standardized variables). nodata : The NoData value to replace with -99999. **kwargs : These will be passed to scikit-learn PCA, please see full lists at: https : //scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html Return: A dictionary. Note: Before executing the function, it is recommended that images coming from different sensors or from the same sensor have a co-registration. The contributions of variables in accounting for the variability in a given principal component are expressed in percentage. Variables that are correlated with PC1 (i.e., Dim.1) and PC2 (i.e., Dim.2) are the most important in explaining the variability in the data set. Variables that do not correlated with any PC or correlated with the last dimensions are variables with low contribution and might be removed to simplify the overall analysis. The contribution is a scaled version of the squared correlation between variables and component axes (or the cosine, from a geometrical point of view) --- this is used to assess the quality of the representation of the variables of the principal component, and it is computed as (cos(variable,axis)^2/total cos2 of the component)\u00d7100. References: - Tarazona, Y., Zabala, A., Pons, X., Broquetas, A., Nowosad, J., and Zurqani, H.A. Fusing Landsat and SAR data for mapping tropical deforestation through machine learning classification and the PVts-\u03b2 non-seasonal detection approach, Canadian Journal of Remote Sensing., vol. 47, no. 5, pp. 677\u2013696, Sep. 2021. This file was automatically generated via lazydocs .","title":"function fusionrs"},{"location":"get-started/","text":"Get Started \u00b6 In these lines, a machine learning approach will be used to classify a satellite image. Example \u00b6 1. Applying Machine Learning \u00b6 Libraries to be used: 1 2 3 4 5 6 7 import rasterio import numpy as np from scikeo.mla import MLA import matplotlib.pyplot as plt from dbfread import DBF import matplotlib as mpl import pandas as pd 2.0 Optical image \u00b6 Landsat-8 OLI (Operational Land Imager) will be used to obtain in order to classify using Random Forest (RF). This image, which is in surface reflectance with bands: - Blue -> B2 - Green -> B3 - Red -> B4 - Nir -> B5 - Swir1 -> B6 - Swir2 -> B7 The image and signatures to be used can be downloaded here : 3.0 Supervised Classification using Random Forest \u00b6 Image and endmembers 1 2 3 4 5 path_raster = r \"C:\\data\\ml\\LC08_232066_20190727_SR.tif\" img = rasterio . open ( path_raster ) path_endm = r \"C:\\data\\ml\\endmembers.dbf\" endm = DBF ( path_endm ) 1 2 3 # endmembers df = pd . DataFrame ( iter ( endm )) df . head () Instance of mla() : 1 inst = MLA ( image = img , endmembers = endm ) Applying Random Forest: 1 rf_class = inst . SVM ( training_split = 0.7 ) 4.0 Results \u00b6 Dictionary of results 1 rf_class . keys () Overall accuracy 1 rf_class . get ( 'Overall_Accuracy' ) Kappa index 1 rf_class . get ( 'Kappa_Index' ) Confusion matrix or error matrix 1 rf_class . get ( 'Confusion_Matrix' ) Preparing the image before plotting 1 2 # Let's define the color palette palette = mpl . colors . ListedColormap ([ \"#2232F9\" , \"#F922AE\" , \"#229954\" , \"#7CED5E\" ]) Applying the plotRGB() algorithm is easy: 1 2 3 4 5 6 7 8 9 10 # Let\u00b4s plot fig , axes = plt . subplots ( nrows = 1 , ncols = 2 , figsize = ( 15 , 9 )) # satellite image plotRGB ( img , title = 'Image in Surface Reflectance' , ax = axes [ 0 ]) # class results axes [ 1 ] . imshow ( svm_class . get ( 'Classification_Map' ), cmap = palette ) axes [ 1 ] . set_title ( \"Classification map\" ) axes [ 1 ] . grid ( False )","title":"Get Started"},{"location":"get-started/#get-started","text":"In these lines, a machine learning approach will be used to classify a satellite image.","title":"Get Started"},{"location":"get-started/#example","text":"","title":"Example"},{"location":"get-started/#1-applying-machine-learning","text":"Libraries to be used: 1 2 3 4 5 6 7 import rasterio import numpy as np from scikeo.mla import MLA import matplotlib.pyplot as plt from dbfread import DBF import matplotlib as mpl import pandas as pd","title":"1. Applying Machine Learning"},{"location":"get-started/#20-optical-image","text":"Landsat-8 OLI (Operational Land Imager) will be used to obtain in order to classify using Random Forest (RF). This image, which is in surface reflectance with bands: - Blue -> B2 - Green -> B3 - Red -> B4 - Nir -> B5 - Swir1 -> B6 - Swir2 -> B7 The image and signatures to be used can be downloaded here :","title":"2.0 Optical image"},{"location":"get-started/#30-supervised-classification-using-random-forest","text":"Image and endmembers 1 2 3 4 5 path_raster = r \"C:\\data\\ml\\LC08_232066_20190727_SR.tif\" img = rasterio . open ( path_raster ) path_endm = r \"C:\\data\\ml\\endmembers.dbf\" endm = DBF ( path_endm ) 1 2 3 # endmembers df = pd . DataFrame ( iter ( endm )) df . head () Instance of mla() : 1 inst = MLA ( image = img , endmembers = endm ) Applying Random Forest: 1 rf_class = inst . SVM ( training_split = 0.7 )","title":"3.0 Supervised Classification using Random Forest"},{"location":"get-started/#40-results","text":"Dictionary of results 1 rf_class . keys () Overall accuracy 1 rf_class . get ( 'Overall_Accuracy' ) Kappa index 1 rf_class . get ( 'Kappa_Index' ) Confusion matrix or error matrix 1 rf_class . get ( 'Confusion_Matrix' ) Preparing the image before plotting 1 2 # Let's define the color palette palette = mpl . colors . ListedColormap ([ \"#2232F9\" , \"#F922AE\" , \"#229954\" , \"#7CED5E\" ]) Applying the plotRGB() algorithm is easy: 1 2 3 4 5 6 7 8 9 10 # Let\u00b4s plot fig , axes = plt . subplots ( nrows = 1 , ncols = 2 , figsize = ( 15 , 9 )) # satellite image plotRGB ( img , title = 'Image in Surface Reflectance' , ax = axes [ 0 ]) # class results axes [ 1 ] . imshow ( svm_class . get ( 'Classification_Map' ), cmap = palette ) axes [ 1 ] . set_title ( \"Classification map\" ) axes [ 1 ] . grid ( False )","title":"4.0 Results"},{"location":"installation/","text":"Installation \u00b6 Stable release \u00b6 To install scikit-eo, run this command in your terminal: 1 pip install scikit-eo This is the preferred method to install scikit-eo, as it will always install the most recent stable release. If you don't have pip installed, this Python installation guide can guide you through the process. From sources \u00b6 The sources for scikit-eo can be downloaded from the Github repo. You can clone the public repository: 1 git clone git://github.com/ytarazona/scikit-eo 1","title":"Installation"},{"location":"installation/#installation","text":"","title":"Installation"},{"location":"installation/#stable-release","text":"To install scikit-eo, run this command in your terminal: 1 pip install scikit-eo This is the preferred method to install scikit-eo, as it will always install the most recent stable release. If you don't have pip installed, this Python installation guide can guide you through the process.","title":"Stable release"},{"location":"installation/#from-sources","text":"The sources for scikit-eo can be downloaded from the Github repo. You can clone the public repository: 1 git clone git://github.com/ytarazona/scikit-eo 1","title":"From sources"},{"location":"linearTrend/","text":"module linearTrend \u00b6 class linearTrend \u00b6 Linear Trend in Remote Sensing method __init__ \u00b6 1 __init__ ( image , nodata =- 99999 ) Parameters: image : Optical images. It must be rasterio.io.DatasetReader with 3d. nodata : The NoData value to replace with -99999. method LN \u00b6 1 LN ( ** kwargs ) Linear trend is useful for mapping forest degradation, land degradation, etc. This algorithm is capable of obtaining the slope of an ordinary least-squares linear regression and its reliability (p-value). Parameters: **kwargs : These will be passed to LN, please see full lists at: https : //docs.scipy.org/doc/scipy/reference/generated/scipy.stats.linregress.html Return: a dictionary with slope, intercept and p-value obtained. All of them in numpy.ndarray with 2d. References: - Crist, E.P., R. Laurin, and R.C. Cicone. 1986. Vegetation and soils information contained in transformed Thematic Mapper data. Pages 1465-1470 Ref. ESA SP-254. - European Space Agency, Paris, France. http : //www.ciesin.org/docs/005-419/005-419.html. Baig, M.H.A., Shuai, T., Tong, Q., 2014. Derivation of a tasseled cap transformation based on Landsat 8 at-satellite reflectance. Remote Sensing Letters, 5(5), 423-431. Li, B., Ti, C., Zhao, Y., Yan, X., 2016. Estimating Soil Moisture with Landsat Data and Its Application in Extracting the Spatial Distribution of Winter Flooded Paddies. Remote Sensing, 8(1), 38. Note: Linear regression is widely used to analyze forest degradation or land degradation. Specifically, the slope and its reliability are used as main parameters and they can be obtained with this function. On the other hand, logistic regression allows obtaining a degradation risk map, in other words, it is a probability map. References: - Tarazona, Y., Maria, Miyasiro-Lopez. (2020). Monitoring tropical forest degradation using remote sensing. Challenges and opportunities in the Madre de Dios region, Peru. Remote Sensing Applications: Society and Environment, 19, 100337. - Wilkinson, G.N., Rogers, C.E., 1973. Symbolic descriptions of factorial models for analysis of variance. Appl. Stat. 22, 392-399. - Chambers, J.M., 1992. Statistical Models in S. CRS Press. method MLN \u00b6 1 MLN ( ** kwargs ) This file was automatically generated via lazydocs .","title":"linearTrend module"},{"location":"linearTrend/#module-lineartrend","text":"","title":"module linearTrend"},{"location":"linearTrend/#class-lineartrend","text":"Linear Trend in Remote Sensing","title":"class linearTrend"},{"location":"linearTrend/#method-__init__","text":"1 __init__ ( image , nodata =- 99999 ) Parameters: image : Optical images. It must be rasterio.io.DatasetReader with 3d. nodata : The NoData value to replace with -99999.","title":"method __init__"},{"location":"linearTrend/#method-ln","text":"1 LN ( ** kwargs ) Linear trend is useful for mapping forest degradation, land degradation, etc. This algorithm is capable of obtaining the slope of an ordinary least-squares linear regression and its reliability (p-value). Parameters: **kwargs : These will be passed to LN, please see full lists at: https : //docs.scipy.org/doc/scipy/reference/generated/scipy.stats.linregress.html Return: a dictionary with slope, intercept and p-value obtained. All of them in numpy.ndarray with 2d. References: - Crist, E.P., R. Laurin, and R.C. Cicone. 1986. Vegetation and soils information contained in transformed Thematic Mapper data. Pages 1465-1470 Ref. ESA SP-254. - European Space Agency, Paris, France. http : //www.ciesin.org/docs/005-419/005-419.html. Baig, M.H.A., Shuai, T., Tong, Q., 2014. Derivation of a tasseled cap transformation based on Landsat 8 at-satellite reflectance. Remote Sensing Letters, 5(5), 423-431. Li, B., Ti, C., Zhao, Y., Yan, X., 2016. Estimating Soil Moisture with Landsat Data and Its Application in Extracting the Spatial Distribution of Winter Flooded Paddies. Remote Sensing, 8(1), 38. Note: Linear regression is widely used to analyze forest degradation or land degradation. Specifically, the slope and its reliability are used as main parameters and they can be obtained with this function. On the other hand, logistic regression allows obtaining a degradation risk map, in other words, it is a probability map. References: - Tarazona, Y., Maria, Miyasiro-Lopez. (2020). Monitoring tropical forest degradation using remote sensing. Challenges and opportunities in the Madre de Dios region, Peru. Remote Sensing Applications: Society and Environment, 19, 100337. - Wilkinson, G.N., Rogers, C.E., 1973. Symbolic descriptions of factorial models for analysis of variance. Appl. Stat. 22, 392-399. - Chambers, J.M., 1992. Statistical Models in S. CRS Press.","title":"method LN"},{"location":"linearTrend/#method-mln","text":"1 MLN ( ** kwargs ) This file was automatically generated via lazydocs .","title":"method MLN"},{"location":"mla/","text":"module mla \u00b6 class MLA \u00b6 Supervised classification in Remote Sensing method __init__ \u00b6 1 __init__ ( image , endmembers , nodata =- 99999 ) Parameter: image: Optical images. It must be rasterio.io.DatasetReader with 3d. endmembers: Endmembers must be a matrix (numpy.ndarray) and with more than one endmember. Rows represent the endmembers and columns represent the spectral bands. The number of bands must be equal to the number of endmembers. E.g. an image with 6 bands, endmembers dimension should be $n*6$, where $n$ is rows with the number of endmembers and 6 is the number of bands (should be equal). In addition, Endmembers must have a field (type int or float) with the names of classes to be predicted. nodata: The NoData value to replace with -99999. method DT \u00b6 1 DT ( training_split = 0.8 , random_state = None , ** kwargs ) Decision Tree is also a supervised non-parametric statistical learning technique, where the input data is divided recursively into branches depending on certain decision thresholds until the data are segmented into homogeneous subgroups. This technique has substantial advantages for remote sensing classification problems due to its flexibility, intuitive simplicity, and computational efficiency. DT support raster data read by rasterio (rasterio.io.DatasetReader) as input. Parameters: training_split : For splitting samples into two subsets, i.e. training data and for testing data. **kwargs : These will be passed to DT, please see full lists at: https : //scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier Return: A dictionary containing labels of classification as numpy object, overall accuracy, kappa index, confusion matrix. method NB \u00b6 1 NB ( training_split = 0.8 , random_state = None , ** kwargs ) Naive Bayes classifier is an effective and simple method for image classification based on probability theory. The NB classifier assumes an underlying probabilistic model and captures the uncertainty about the model in a principled way, that is, by calculating the occurrence probabilities of different attribute values for different classes in a training set. NB support raster data read by rasterio (rasterio.io.DatasetReader) as input. Parameters: training_split : For splitting samples into two subsets, i.e. training data and for testing data. **kwargs : These will be passed to SVM, please see full lists at: https : //scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html Return: A dictionary containing labels of classification as numpy object, overall accuracy, kappa index, confusion matrix. method NN \u00b6 1 NN ( training_split = 0.8 , max_iter = 300 , random_state = None , ** kwargs ) This classification consists of a neural network that is organized into several layers, that is, an input layer of predictor variables, one or more layers of hidden nodes, in which each node represents an activation function acting on a weighted input of the previous layers\u2019 outputs, and an output layer. NN support raster data read by rasterio (rasterio.io.DatasetReader) as input. Parameters: training_split : For splitting samples into two subsets, i.e. training data and for testing data. **kwargs : These will be passed to SVM, please see full lists at: https : //scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html Return: A dictionary containing labels of classification as numpy object, overall accuracy, kappa index, confusion matrix. method RF \u00b6 1 RF ( training_split = 0.8 , random_state = None , ** kwargs ) Random Forest is a derivative of Decision Tree which provides an improvement over DT to overcome the weaknesses of a single DT. The prediction model of the RF classifier only requires two parameters to be identified: the number of classification trees desired, known as \u201cntree,\u201d and the number of prediction variables, known as \u201cmtry,\u201d used in each node to make the tree grow. RF support raster data read by rasterio (rasterio.io.DatasetReader) as input. Parameters: training_split : For splitting samples into two subsets, i.e. training data and for testing data. **kwargs : These will be passed to RF, please see full lists at: https : //scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html Return: A dictionary containing labels of classification as numpy object, overall accuracy, kappa index, confusion matrix. method SVM \u00b6 1 SVM ( training_split = 0.8 , random_state = None , kernel = 'linear' , ** kwargs ) The Support Vector Machine (SVM) classifier is a supervised non-parametric statistical learning technique that does not assume a preliminary distribution of input data. Its discrimination criterion is a hyperplane that separates the classes in the multidimensional space in which the samples that have established the same classes are located, generally some training areas. SVM support raster data read by rasterio (rasterio.io.DatasetReader) as input. Parameters: training_split : For splitting samples into two subsets, i.e. training data and for testing data. kernel : {'linear', 'poly', 'rbf', 'sigmoid', 'precomputed'}, default='rbf' Specifies the kernel type to be used in the algorithm. It must be one of 'linear', 'poly', 'rbf', 'sigmoid', 'precomputed' or a callable. If None is given, 'rbf' will be used. See https : //scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC for more details. **kwargs : These will be passed to SVM, please see full lists at: https : //scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC Return: A dictionary containing labels of classification as numpy object, overall accuracy, kappa index, confusion matrix. This file was automatically generated via lazydocs .","title":"mla module"},{"location":"mla/#module-mla","text":"","title":"module mla"},{"location":"mla/#class-mla","text":"Supervised classification in Remote Sensing","title":"class MLA"},{"location":"mla/#method-__init__","text":"1 __init__ ( image , endmembers , nodata =- 99999 ) Parameter: image: Optical images. It must be rasterio.io.DatasetReader with 3d. endmembers: Endmembers must be a matrix (numpy.ndarray) and with more than one endmember. Rows represent the endmembers and columns represent the spectral bands. The number of bands must be equal to the number of endmembers. E.g. an image with 6 bands, endmembers dimension should be $n*6$, where $n$ is rows with the number of endmembers and 6 is the number of bands (should be equal). In addition, Endmembers must have a field (type int or float) with the names of classes to be predicted. nodata: The NoData value to replace with -99999.","title":"method __init__"},{"location":"mla/#method-dt","text":"1 DT ( training_split = 0.8 , random_state = None , ** kwargs ) Decision Tree is also a supervised non-parametric statistical learning technique, where the input data is divided recursively into branches depending on certain decision thresholds until the data are segmented into homogeneous subgroups. This technique has substantial advantages for remote sensing classification problems due to its flexibility, intuitive simplicity, and computational efficiency. DT support raster data read by rasterio (rasterio.io.DatasetReader) as input. Parameters: training_split : For splitting samples into two subsets, i.e. training data and for testing data. **kwargs : These will be passed to DT, please see full lists at: https : //scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier Return: A dictionary containing labels of classification as numpy object, overall accuracy, kappa index, confusion matrix.","title":"method DT"},{"location":"mla/#method-nb","text":"1 NB ( training_split = 0.8 , random_state = None , ** kwargs ) Naive Bayes classifier is an effective and simple method for image classification based on probability theory. The NB classifier assumes an underlying probabilistic model and captures the uncertainty about the model in a principled way, that is, by calculating the occurrence probabilities of different attribute values for different classes in a training set. NB support raster data read by rasterio (rasterio.io.DatasetReader) as input. Parameters: training_split : For splitting samples into two subsets, i.e. training data and for testing data. **kwargs : These will be passed to SVM, please see full lists at: https : //scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html Return: A dictionary containing labels of classification as numpy object, overall accuracy, kappa index, confusion matrix.","title":"method NB"},{"location":"mla/#method-nn","text":"1 NN ( training_split = 0.8 , max_iter = 300 , random_state = None , ** kwargs ) This classification consists of a neural network that is organized into several layers, that is, an input layer of predictor variables, one or more layers of hidden nodes, in which each node represents an activation function acting on a weighted input of the previous layers\u2019 outputs, and an output layer. NN support raster data read by rasterio (rasterio.io.DatasetReader) as input. Parameters: training_split : For splitting samples into two subsets, i.e. training data and for testing data. **kwargs : These will be passed to SVM, please see full lists at: https : //scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html Return: A dictionary containing labels of classification as numpy object, overall accuracy, kappa index, confusion matrix.","title":"method NN"},{"location":"mla/#method-rf","text":"1 RF ( training_split = 0.8 , random_state = None , ** kwargs ) Random Forest is a derivative of Decision Tree which provides an improvement over DT to overcome the weaknesses of a single DT. The prediction model of the RF classifier only requires two parameters to be identified: the number of classification trees desired, known as \u201cntree,\u201d and the number of prediction variables, known as \u201cmtry,\u201d used in each node to make the tree grow. RF support raster data read by rasterio (rasterio.io.DatasetReader) as input. Parameters: training_split : For splitting samples into two subsets, i.e. training data and for testing data. **kwargs : These will be passed to RF, please see full lists at: https : //scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html Return: A dictionary containing labels of classification as numpy object, overall accuracy, kappa index, confusion matrix.","title":"method RF"},{"location":"mla/#method-svm","text":"1 SVM ( training_split = 0.8 , random_state = None , kernel = 'linear' , ** kwargs ) The Support Vector Machine (SVM) classifier is a supervised non-parametric statistical learning technique that does not assume a preliminary distribution of input data. Its discrimination criterion is a hyperplane that separates the classes in the multidimensional space in which the samples that have established the same classes are located, generally some training areas. SVM support raster data read by rasterio (rasterio.io.DatasetReader) as input. Parameters: training_split : For splitting samples into two subsets, i.e. training data and for testing data. kernel : {'linear', 'poly', 'rbf', 'sigmoid', 'precomputed'}, default='rbf' Specifies the kernel type to be used in the algorithm. It must be one of 'linear', 'poly', 'rbf', 'sigmoid', 'precomputed' or a callable. If None is given, 'rbf' will be used. See https : //scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC for more details. **kwargs : These will be passed to SVM, please see full lists at: https : //scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC Return: A dictionary containing labels of classification as numpy object, overall accuracy, kappa index, confusion matrix. This file was automatically generated via lazydocs .","title":"method SVM"},{"location":"pca/","text":"module pca \u00b6 function PCA \u00b6 1 PCA ( image , stand_varb = True , nodata =- 99999 , ** kwargs ) Runing Principal Component Analysis (PCA) with satellite images. This algorithm allows to obtain Principal Components from images either radar or optical coming from different spectral sensors. It is also possible to obtain the contribution (%) of each variable. Parameters: images : Optical or radar image, it must be rasterio.io.DatasetReader with 3d. stand_varb : Logical. If stand.varb = True , the PCA is calculated using the correlation matrix (standardized variables) instead of the covariance matrix (non-standardized variables). nodata : The NoData value to replace with -99999. **kwargs : These will be passed to scikit-learn PCA, please see full lists at: https : //scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html Return: A dictionary. Note: The contributions of variables in accounting for the variability in a given principal component are expressed in percentage. Variables that are correlated with PC1 (i.e., Dim.1) and PC2 (i.e., Dim.2) are the most important in explaining the variability in the data set. Variables that do not correlated with any PC or correlated with the last dimensions are variables with low contribution and might be removed to simplify the overall analysis. The contribution is a scaled version of the squared correlation between variables and component axes (or the cosine, from a geometrical point of view) --- this is used to assess the quality of the representation of the variables of the principal component, and it is computed as (cos(variable,axis)^2/total cos2 of the component)\u00d7100. This file was automatically generated via lazydocs .","title":"pca module"},{"location":"pca/#module-pca","text":"","title":"module pca"},{"location":"pca/#function-pca","text":"1 PCA ( image , stand_varb = True , nodata =- 99999 , ** kwargs ) Runing Principal Component Analysis (PCA) with satellite images. This algorithm allows to obtain Principal Components from images either radar or optical coming from different spectral sensors. It is also possible to obtain the contribution (%) of each variable. Parameters: images : Optical or radar image, it must be rasterio.io.DatasetReader with 3d. stand_varb : Logical. If stand.varb = True , the PCA is calculated using the correlation matrix (standardized variables) instead of the covariance matrix (non-standardized variables). nodata : The NoData value to replace with -99999. **kwargs : These will be passed to scikit-learn PCA, please see full lists at: https : //scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html Return: A dictionary. Note: The contributions of variables in accounting for the variability in a given principal component are expressed in percentage. Variables that are correlated with PC1 (i.e., Dim.1) and PC2 (i.e., Dim.2) are the most important in explaining the variability in the data set. Variables that do not correlated with any PC or correlated with the last dimensions are variables with low contribution and might be removed to simplify the overall analysis. The contribution is a scaled version of the squared correlation between variables and component axes (or the cosine, from a geometrical point of view) --- this is used to assess the quality of the representation of the variables of the principal component, and it is computed as (cos(variable,axis)^2/total cos2 of the component)\u00d7100. This file was automatically generated via lazydocs .","title":"function PCA"},{"location":"plot/","text":"module plot \u00b6 function plotRGB \u00b6 1 2 3 4 5 6 7 8 9 10 plotRGB ( image , bands = [ 4 , 3 , 2 ], stretch = 'std' , title = None , xlabel = None , ylabel = None , ax = None , ** kwargs ) Plotting an image in RGB This function allows to plot an satellite image in RGB channels. Parameters: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 - <b>`image`</b>: Optical images. It must be rasterio.io.DatasetReader with 3d. - <b>`bands`</b>: A list contain the order of bands to be used in order to plot in RGB. For example, for six bands (blue, green, red, nir, swir1 and swir2), number four (4) indicates the swir1 band, number three (3) indicates the nir band and the number two (2) indicates the red band. - <b>`stretch`</b>: Contrast enhancement using the histogram. There are two options here: i) using standard deviation ('std') and ii) using percentiles ('per'). For default is 'std', which means standard deviation. - <b>`title`</b>: Assigned title. - <b>`xlabel`</b>: X axis title. - <b>`ylabel`</b>: Y axis title. - <b>`ax`</b>: current axes - <b>`**kwargs`</b>: These will be passed to the matplotlib imshow(), please see full lists at: - <b>`https`</b>: //matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.imshow.html Return: 1 - <b>`ax `</b>: Graphic of change detection using the matplotlib plot function. This file was automatically generated via lazydocs .","title":"plot module"},{"location":"plot/#module-plot","text":"","title":"module plot"},{"location":"plot/#function-plotrgb","text":"1 2 3 4 5 6 7 8 9 10 plotRGB ( image , bands = [ 4 , 3 , 2 ], stretch = 'std' , title = None , xlabel = None , ylabel = None , ax = None , ** kwargs ) Plotting an image in RGB This function allows to plot an satellite image in RGB channels. Parameters: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 - <b>`image`</b>: Optical images. It must be rasterio.io.DatasetReader with 3d. - <b>`bands`</b>: A list contain the order of bands to be used in order to plot in RGB. For example, for six bands (blue, green, red, nir, swir1 and swir2), number four (4) indicates the swir1 band, number three (3) indicates the nir band and the number two (2) indicates the red band. - <b>`stretch`</b>: Contrast enhancement using the histogram. There are two options here: i) using standard deviation ('std') and ii) using percentiles ('per'). For default is 'std', which means standard deviation. - <b>`title`</b>: Assigned title. - <b>`xlabel`</b>: X axis title. - <b>`ylabel`</b>: Y axis title. - <b>`ax`</b>: current axes - <b>`**kwargs`</b>: These will be passed to the matplotlib imshow(), please see full lists at: - <b>`https`</b>: //matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.imshow.html Return: 1 - <b>`ax `</b>: Graphic of change detection using the matplotlib plot function. This file was automatically generated via lazydocs .","title":"function plotRGB"},{"location":"rkmeans/","text":"module rkmeans \u00b6 function rkmeans \u00b6 1 rkmeans ( image , k , nodata =- 99999 , ** kwargs ) This function allows to classify satellite images using k-means In principle, this function allows to classify satellite images specifying a k value (clusters), however it is recommended to find the optimal value of k using the calkmeans function embedded in this package. Parameters: image : Optical images. It must be rasterio.io.DatasetReader with 3d. k : The number of clusters to be detected. nodata : The NoData value to replace with -99999. **kwargs : These will be passed to scikit-learn KMeans, please see full lists at: https : //scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html Return: Labels of classification as numpy object with 2d. This file was automatically generated via lazydocs .","title":"rkmeans module"},{"location":"rkmeans/#module-rkmeans","text":"","title":"module rkmeans"},{"location":"rkmeans/#function-rkmeans","text":"1 rkmeans ( image , k , nodata =- 99999 , ** kwargs ) This function allows to classify satellite images using k-means In principle, this function allows to classify satellite images specifying a k value (clusters), however it is recommended to find the optimal value of k using the calkmeans function embedded in this package. Parameters: image : Optical images. It must be rasterio.io.DatasetReader with 3d. k : The number of clusters to be detected. nodata : The NoData value to replace with -99999. **kwargs : These will be passed to scikit-learn KMeans, please see full lists at: https : //scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html Return: Labels of classification as numpy object with 2d. This file was automatically generated via lazydocs .","title":"function rkmeans"},{"location":"sma/","text":"module sma \u00b6 function sma \u00b6 1 sma ( image , endmembers , nodata =- 99999 ) The SMA assumes that the energy received within the field of vision of the remote sensor can be considered as the sum of the energies received from each dominant endmember. This function addresses a Linear Mixing Model. A regression analysis is used to obtain the fractions. In least squares inversion algorithms, the common objective is to estimate abundances that minimize the squared error between the actual spectrum and the estimated spectrum. The values of the fractions will be between 0 and 1. Parameters: image : Optical images. It must be rasterio.io.DatasetReader with 3d. endmembers : Endmembers must be a matrix (numpy.ndarray) and with more than one endmember. Rows represent the endmembers and columns represent the spectral bands. The number of bands must be greater than the number of endmembers. E.g. an image with 6 bands, endmembers dimension should be $n*6$, where $n$ is rows with the number of endmembers and 6 is the number of bands (should be equal). nodata : The NoData value to replace with -99999. Return: numpy.ndarray with 2d. References Adams, J. B., Smith, M. O., & Gillespie, A. R. (1993). Imaging spectroscopy: Interpretation based on spectral mixture analysis. In C. M. Pieters & P. Englert (Eds.), Remote geochemical analysis: Elements and mineralogical composition. NY: Cambridge Univ. Press 145-166 pp. Shimabukuro, Y.E. and Smith, J., (1991). The least squares mixing models to generate fraction images derived from remote sensing multispectral data. IEEE Transactions on Geoscience and Remote Sensing, 29, pp. 16-21. Note: A regression analysis is used to obtain the fractions. In least squares inversion algorithms, the common objective is to estimate abundances that minimize the squared error between the actual spectrum and the estimated spectrum. The values of the fractions will be between 0 and 1. This file was automatically generated via lazydocs .","title":"sma module"},{"location":"sma/#module-sma","text":"","title":"module sma"},{"location":"sma/#function-sma","text":"1 sma ( image , endmembers , nodata =- 99999 ) The SMA assumes that the energy received within the field of vision of the remote sensor can be considered as the sum of the energies received from each dominant endmember. This function addresses a Linear Mixing Model. A regression analysis is used to obtain the fractions. In least squares inversion algorithms, the common objective is to estimate abundances that minimize the squared error between the actual spectrum and the estimated spectrum. The values of the fractions will be between 0 and 1. Parameters: image : Optical images. It must be rasterio.io.DatasetReader with 3d. endmembers : Endmembers must be a matrix (numpy.ndarray) and with more than one endmember. Rows represent the endmembers and columns represent the spectral bands. The number of bands must be greater than the number of endmembers. E.g. an image with 6 bands, endmembers dimension should be $n*6$, where $n$ is rows with the number of endmembers and 6 is the number of bands (should be equal). nodata : The NoData value to replace with -99999. Return: numpy.ndarray with 2d. References Adams, J. B., Smith, M. O., & Gillespie, A. R. (1993). Imaging spectroscopy: Interpretation based on spectral mixture analysis. In C. M. Pieters & P. Englert (Eds.), Remote geochemical analysis: Elements and mineralogical composition. NY: Cambridge Univ. Press 145-166 pp. Shimabukuro, Y.E. and Smith, J., (1991). The least squares mixing models to generate fraction images derived from remote sensing multispectral data. IEEE Transactions on Geoscience and Remote Sensing, 29, pp. 16-21. Note: A regression analysis is used to obtain the fractions. In least squares inversion algorithms, the common objective is to estimate abundances that minimize the squared error between the actual spectrum and the estimated spectrum. The values of the fractions will be between 0 and 1. This file was automatically generated via lazydocs .","title":"function sma"},{"location":"tassCap/","text":"module tassCap \u00b6 function tassCap \u00b6 1 tassCap ( image , sat = 'Landsat8OLI' , nodata =- 99999 , scale = None ) The Tasseled-Cap Transformation is a linear transformation method for various remote sensing data. Not only can it perform volume data compression, but it can also provide parameters associated with the physical characteristics, such as brightness, greenness and wetness indices. Parameters: image : Optical images. It must be rasterio.io.DatasetReader with 3d. sat : Specify satellite and sensor type (Landsat5TM, Landsat7ETM or Landsat8OLI). nodata : The NoData value to replace with -99999. scale : Conversion of coefficients values Return: numpy.ndarray with 3d containing brightness, greenness and wetness indices. References: Crist, E.P., R. Laurin, and R.C. Cicone. 1986. Vegetation and soils information contained in transformed Thematic Mapper data. Pages 1465-1470 Ref. ESA SP-254. European Space Agency, Paris, France. http : //www.ciesin.org/docs/005-419/005-419.html. Baig, M.H.A., Shuai, T., Tong, Q., 2014. Derivation of a tasseled cap transformation based on Landsat 8 at-satellite reflectance. Remote Sensing Letters, 5(5), 423-431. Li, B., Ti, C., Zhao, Y., Yan, X., 2016. Estimating Soil Moisture with Landsat Data and Its Application in Extracting the Spatial Distribution of Winter Flooded Paddies. Remote Sensing, 8(1), 38. Note: Currently implemented for satellites such as Landsat-4 TM, Landsat-5 TM, Landsat-7 ETM+, Landsat-8 OLI and Sentinel2. The input data must be in top of atmosphere reflectance (toa). Bands required as input must be ordered as: Consider using the following satellite bands: =============== ================================ Type of Sensor Name of bands =============== ================================ Landsat4TM :blue, green, red, nir, swir1, swir2 Landsat5TM :blue, green, red, nir, swir1, swir2 Landsat7ETM+ :blue, green, red, nir, swir1, swir2 Landsat8OLI :blue, green, red, nir, swir1, swir2 Landsat8OLI-Li2016 :coastal, blue, green, red, nir, swir1, swir2 Sentinel2MSI :coastal, blue, green, red, nir-1, mir-1, mir-2 This file was automatically generated via lazydocs .","title":"tassCap module"},{"location":"tassCap/#module-tasscap","text":"","title":"module tassCap"},{"location":"tassCap/#function-tasscap","text":"1 tassCap ( image , sat = 'Landsat8OLI' , nodata =- 99999 , scale = None ) The Tasseled-Cap Transformation is a linear transformation method for various remote sensing data. Not only can it perform volume data compression, but it can also provide parameters associated with the physical characteristics, such as brightness, greenness and wetness indices. Parameters: image : Optical images. It must be rasterio.io.DatasetReader with 3d. sat : Specify satellite and sensor type (Landsat5TM, Landsat7ETM or Landsat8OLI). nodata : The NoData value to replace with -99999. scale : Conversion of coefficients values Return: numpy.ndarray with 3d containing brightness, greenness and wetness indices. References: Crist, E.P., R. Laurin, and R.C. Cicone. 1986. Vegetation and soils information contained in transformed Thematic Mapper data. Pages 1465-1470 Ref. ESA SP-254. European Space Agency, Paris, France. http : //www.ciesin.org/docs/005-419/005-419.html. Baig, M.H.A., Shuai, T., Tong, Q., 2014. Derivation of a tasseled cap transformation based on Landsat 8 at-satellite reflectance. Remote Sensing Letters, 5(5), 423-431. Li, B., Ti, C., Zhao, Y., Yan, X., 2016. Estimating Soil Moisture with Landsat Data and Its Application in Extracting the Spatial Distribution of Winter Flooded Paddies. Remote Sensing, 8(1), 38. Note: Currently implemented for satellites such as Landsat-4 TM, Landsat-5 TM, Landsat-7 ETM+, Landsat-8 OLI and Sentinel2. The input data must be in top of atmosphere reflectance (toa). Bands required as input must be ordered as: Consider using the following satellite bands: =============== ================================ Type of Sensor Name of bands =============== ================================ Landsat4TM :blue, green, red, nir, swir1, swir2 Landsat5TM :blue, green, red, nir, swir1, swir2 Landsat7ETM+ :blue, green, red, nir, swir1, swir2 Landsat8OLI :blue, green, red, nir, swir1, swir2 Landsat8OLI-Li2016 :coastal, blue, green, red, nir, swir1, swir2 Sentinel2MSI :coastal, blue, green, red, nir-1, mir-1, mir-2 This file was automatically generated via lazydocs .","title":"function tassCap"},{"location":"usage/","text":"Usage \u00b6 To use scikit-eo in a project: 1 import scikit-eo 1","title":"Usage"},{"location":"usage/#usage","text":"To use scikit-eo in a project: 1 import scikit-eo 1","title":"Usage"},{"location":"writeRaster/","text":"module writeRaster \u00b6 function writeRaster \u00b6 1 writeRaster ( arr , image , filename = None , filepath = None , n = 1 ) This algorithm allows to save array images to raster format (.tif). Parameters: arr : Array object with 2d (rows and cols) or 3d (rows, cols, bands). image : Optical images. It must be read by rasterio.open(). filename : The image name to be saved. filepath : The path which the image will be stored. n : Number of images to be saved. Return: A raster in your filepath. This file was automatically generated via lazydocs .","title":"writeRaster module"},{"location":"writeRaster/#module-writeraster","text":"","title":"module writeRaster"},{"location":"writeRaster/#function-writeraster","text":"1 writeRaster ( arr , image , filename = None , filepath = None , n = 1 ) This algorithm allows to save array images to raster format (.tif). Parameters: arr : Array object with 2d (rows and cols) or 3d (rows, cols, bands). image : Optical images. It must be read by rasterio.open(). filename : The image name to be saved. filepath : The path which the image will be stored. n : Number of images to be saved. Return: A raster in your filepath. This file was automatically generated via lazydocs .","title":"function writeRaster"}]}
=======
{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to \u00b6 scikit-eo: A Python package for Remote Sensing Tools \u00b6 Links of interest: \u00b6 GitHub repo: https://github.com/ytarazona/scikit-eo Documentation: https://ytarazona.github.io/scikit-eo/ PyPI: https://pypi.org/project/scikeo/ Notebooks examples: https://github.com/ytarazona/scikit-eo/tree/main/examples Google Colab examples: https://github.com/ytarazona/scikit-eo/tree/main/examples GitHub repo: https://github.com/ytarazona/scikit-eo Documentation: https://ytarazona.github.io/scikit-eo/ PyPI: https://pypi.org/project/scikeo/ Notebooks examples: https://github.com/ytarazona/scikit-eo/tree/main/examples Google Colab examples: https://github.com/ytarazona/scikit-eo/tree/main/examples Free software: Apache 2.0 Introduction \u00b6 Now a day, remotely sensed data has increased dramatically. Microwaves and optical images with different spatial and temporal resolutions are available and are using to monitor a variaty of environmental issues such as deforestation, land degradation, crop classifications, among other. Although there are efforts (i.e., Python packages, forums, communities, etc.) to make available line-of-code tools for pre-processing, processing and analysis of satellite imagery, there is still a gap that needs to be filled. In other words, too much time is still spent by many users in developing Python lines of code. Algorithms for mapping land degradation through linear trend of vegetation indices (Tarazona and Miyasiro, 2021), fusion optical and radar images to classify vegetation cover, calibration of machine learning lagorithms, among others, are not available yet. Therefore, scikit-eo is a Python package that provides tools for remote sensing. This package was developed to fill the gaps in remotely sensed data processing tools. Most of the tools are based on scientific publications, and others are useful algorithms that will allow processing to be done in a few lines of code. With these tools, the user will be able to invest time in analyzing the results of their data and not spend time on elaborating lines of code, which can sometimes be stressful. Tools for Remote Sensing \u00b6 Name of functions Description mla Machine Learning calmla Calibrating supervised classification in Remote Sensing rkmeans K-means classification calkmeans This function allows to calibrate the kmeans algorithm. It is possible to obtain the best k value and the best embedded algorithm in kmeans. pca Principal Components Analysis atmosCorr Atmospheric Correction of satellite imagery deepLearning Deep Learning algorithms linearTrend Linear trend is useful for mapping forest degradation or land degradation fusionrs This algorithm allows to fusion images coming from different spectral sensors (e.g., optical-optical, optical and SAR or SAR-SAR). Among many of the qualities of this function, it is possible to obtain the contribution (%) of each variable in the fused image sma Spectral Mixture Analysis - Classification sup-pixel tassCap The Tasseled-Cap Transformation You will find more algorithms!. Installation \u00b6 To use scikit-eo it is necessary to install it. There are two options: 1. From PyPI \u00b6 1 pip install scikeo 2. Installing from source \u00b6 It is also possible to install the latest development version directly from the GitHub repository with: 1 pip install git + https : // github . com / ytarazona / scikit - eo","title":"Home"},{"location":"#welcome-to","text":"","title":"Welcome to"},{"location":"#scikit-eo-a-python-package-for-remote-sensing-tools","text":"","title":"scikit-eo: A Python package for Remote Sensing Tools"},{"location":"#links-of-interest","text":"GitHub repo: https://github.com/ytarazona/scikit-eo Documentation: https://ytarazona.github.io/scikit-eo/ PyPI: https://pypi.org/project/scikeo/ Notebooks examples: https://github.com/ytarazona/scikit-eo/tree/main/examples Google Colab examples: https://github.com/ytarazona/scikit-eo/tree/main/examples GitHub repo: https://github.com/ytarazona/scikit-eo Documentation: https://ytarazona.github.io/scikit-eo/ PyPI: https://pypi.org/project/scikeo/ Notebooks examples: https://github.com/ytarazona/scikit-eo/tree/main/examples Google Colab examples: https://github.com/ytarazona/scikit-eo/tree/main/examples Free software: Apache 2.0","title":"Links of interest:"},{"location":"#introduction","text":"Now a day, remotely sensed data has increased dramatically. Microwaves and optical images with different spatial and temporal resolutions are available and are using to monitor a variaty of environmental issues such as deforestation, land degradation, crop classifications, among other. Although there are efforts (i.e., Python packages, forums, communities, etc.) to make available line-of-code tools for pre-processing, processing and analysis of satellite imagery, there is still a gap that needs to be filled. In other words, too much time is still spent by many users in developing Python lines of code. Algorithms for mapping land degradation through linear trend of vegetation indices (Tarazona and Miyasiro, 2021), fusion optical and radar images to classify vegetation cover, calibration of machine learning lagorithms, among others, are not available yet. Therefore, scikit-eo is a Python package that provides tools for remote sensing. This package was developed to fill the gaps in remotely sensed data processing tools. Most of the tools are based on scientific publications, and others are useful algorithms that will allow processing to be done in a few lines of code. With these tools, the user will be able to invest time in analyzing the results of their data and not spend time on elaborating lines of code, which can sometimes be stressful.","title":"Introduction"},{"location":"#tools-for-remote-sensing","text":"Name of functions Description mla Machine Learning calmla Calibrating supervised classification in Remote Sensing rkmeans K-means classification calkmeans This function allows to calibrate the kmeans algorithm. It is possible to obtain the best k value and the best embedded algorithm in kmeans. pca Principal Components Analysis atmosCorr Atmospheric Correction of satellite imagery deepLearning Deep Learning algorithms linearTrend Linear trend is useful for mapping forest degradation or land degradation fusionrs This algorithm allows to fusion images coming from different spectral sensors (e.g., optical-optical, optical and SAR or SAR-SAR). Among many of the qualities of this function, it is possible to obtain the contribution (%) of each variable in the fused image sma Spectral Mixture Analysis - Classification sup-pixel tassCap The Tasseled-Cap Transformation You will find more algorithms!.","title":"Tools for Remote Sensing"},{"location":"#installation","text":"To use scikit-eo it is necessary to install it. There are two options:","title":"Installation"},{"location":"#1-from-pypi","text":"1 pip install scikeo","title":"1. From PyPI"},{"location":"#2-installing-from-source","text":"It is also possible to install the latest development version directly from the GitHub repository with: 1 pip install git + https : // github . com / ytarazona / scikit - eo","title":"2. Installing from source"},{"location":"atmosCorr/","text":"module atmosCorr \u00b6 class atmosCorr \u00b6 Atmospheric Correction in Optical domain method __init__ \u00b6 1 __init__ ( path , nodata =- 99999 ) Parameter: path: String. The folder in which the satellite bands are located. This images could be Landsat Collection 2 Level-1. For example: path = r'/folder/image/raster'. nodata: The NoData value to replace with -99999. method DOS \u00b6 1 DOS ( sat = 'LC08' , mindn = None ) The Dark Object Subtraction Method was proposed by Chavez (1988). This image-based atmospheric correction method considers absolutely critical and valid the existence of a dark object in the scene, which is used in the selection of a minimum value in the haze correction. The most valid dark objects in this kind of correction are areas totally shaded or otherwise areas representing dark water bodies. Parameters: sat : Type of Satellite. It could be Landsat-5 TM, Landsat-8 OLI or Landsat-9 OLI-2. mindn : Min of digital number for each band in a list. Return: An array with Surface Reflectance values with 3d, i.e. (rows, cols, bands). References: - Chavez, P.S. (1988). An Improved Dark-Object Subtraction Technique for Atmospheric Scattering Correction of Multispectral Data. Remote Sensing of Envrironment, 24(3), 459-479. method RAD \u00b6 1 RAD ( sat = 'LC08' ) Conversion to TOA Radiance. Landsat Level-1 data can be converted to TOA spectral radiance using the radiance rescaling factors in the MTL file: L\u03bb = MLQcal + AL where: L\u03bb = TOA spectral radiance (Watts/(m2 srad \u03bcm)) ML = Band-specific multiplicative rescaling factor from the metadata (RADIANCE_MULT_BAND_x, where x is the band number) AL = Band-specific additive rescaling factor from the metadata (RADIANCE_ADD_BAND_x, where x is the band number) Qcal = Quantized and calibrated standard product pixel values (DN) Parameters: sat : Type of Satellite. It could be Landsat-5 TM, Landsat-8 OLI or Landsat-9 OLI-2. Return: An array with radiance values with 3d, i.e. (rows, cols, bands). method TOA \u00b6 1 TOA ( sat = 'LC08' ) A reduction in scene-to-scene variability can be achieved by converting the at-sensor spectral radiance to exoatmospheric TOA reflectance, also known as in-band planetary albedo. Equation to obtain TOA reflectance: \u03c1\u03bb\u2032 = M\u03c1*DN + A\u03c1 \u03c1\u03bb = \u03c1\u03bb\u2032/sin(theta) Parameters: sat : Type of Satellite. It could be Landsat-5 TM, Landsat-8 OLI or Landsat-9 OLI-2. Return: An array with TOA values with 3d, i.e. (rows, cols, bands). This file was automatically generated via lazydocs .","title":"atmosCorr module"},{"location":"atmosCorr/#module-atmoscorr","text":"","title":"module atmosCorr"},{"location":"atmosCorr/#class-atmoscorr","text":"Atmospheric Correction in Optical domain","title":"class atmosCorr"},{"location":"atmosCorr/#method-__init__","text":"1 __init__ ( path , nodata =- 99999 ) Parameter: path: String. The folder in which the satellite bands are located. This images could be Landsat Collection 2 Level-1. For example: path = r'/folder/image/raster'. nodata: The NoData value to replace with -99999.","title":"method __init__"},{"location":"atmosCorr/#method-dos","text":"1 DOS ( sat = 'LC08' , mindn = None ) The Dark Object Subtraction Method was proposed by Chavez (1988). This image-based atmospheric correction method considers absolutely critical and valid the existence of a dark object in the scene, which is used in the selection of a minimum value in the haze correction. The most valid dark objects in this kind of correction are areas totally shaded or otherwise areas representing dark water bodies. Parameters: sat : Type of Satellite. It could be Landsat-5 TM, Landsat-8 OLI or Landsat-9 OLI-2. mindn : Min of digital number for each band in a list. Return: An array with Surface Reflectance values with 3d, i.e. (rows, cols, bands). References: - Chavez, P.S. (1988). An Improved Dark-Object Subtraction Technique for Atmospheric Scattering Correction of Multispectral Data. Remote Sensing of Envrironment, 24(3), 459-479.","title":"method DOS"},{"location":"atmosCorr/#method-rad","text":"1 RAD ( sat = 'LC08' ) Conversion to TOA Radiance. Landsat Level-1 data can be converted to TOA spectral radiance using the radiance rescaling factors in the MTL file: L\u03bb = MLQcal + AL where: L\u03bb = TOA spectral radiance (Watts/(m2 srad \u03bcm)) ML = Band-specific multiplicative rescaling factor from the metadata (RADIANCE_MULT_BAND_x, where x is the band number) AL = Band-specific additive rescaling factor from the metadata (RADIANCE_ADD_BAND_x, where x is the band number) Qcal = Quantized and calibrated standard product pixel values (DN) Parameters: sat : Type of Satellite. It could be Landsat-5 TM, Landsat-8 OLI or Landsat-9 OLI-2. Return: An array with radiance values with 3d, i.e. (rows, cols, bands).","title":"method RAD"},{"location":"atmosCorr/#method-toa","text":"1 TOA ( sat = 'LC08' ) A reduction in scene-to-scene variability can be achieved by converting the at-sensor spectral radiance to exoatmospheric TOA reflectance, also known as in-band planetary albedo. Equation to obtain TOA reflectance: \u03c1\u03bb\u2032 = M\u03c1*DN + A\u03c1 \u03c1\u03bb = \u03c1\u03bb\u2032/sin(theta) Parameters: sat : Type of Satellite. It could be Landsat-5 TM, Landsat-8 OLI or Landsat-9 OLI-2. Return: An array with TOA values with 3d, i.e. (rows, cols, bands). This file was automatically generated via lazydocs .","title":"method TOA"},{"location":"calkmeans/","text":"module calkmeans \u00b6 function calkmeans \u00b6 1 2 3 4 5 6 7 8 9 calkmeans ( image , k = None , algo = ( 'auto' , 'elkan' ), max_iter = 300 , n_iter = 10 , nodata =- 99999 , ** kwargs ) Calibrating kmeans This function allows to calibrate the kmeans algorithm. It is possible to obtain the best 'k' value and the best embedded algorithm in KMmeans. Parameters: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 - <b>`image`</b>: Optical images. It must be rasterio.io.DatasetReader with 3d. - <b>`k`</b>: k This argument is None when the objective is to obtain the best 'k' value. If the objective is to select the best algorithm embedded in kmeans, please specify a 'k' value. - <b>`max_iter`</b>: The maximum number of iterations allowed. Strictly related to KMeans. Please see - <b>`https`</b>: //scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html - <b>`algo`</b>: It can be \"auto\" and 'elkan'. \"auto\" and \"full\" are deprecated and they will be removed in Scikit-Learn 1.3. They are both aliases for \"lloyd\". - <b>`Changed in version 1.1`</b>: Renamed \u201cfull\u201d to \u201clloyd\u201d, and deprecated \u201cauto\u201d and \u201cfull\u201d. Changed \u201cauto\u201d to use \u201clloyd\u201d instead of \u201celkan\u201d. - <b>`n_iter`</b>: Iterations number to obtain the best 'k' value. 'n_iter' must be greater than the number of classes expected to be obtained in the classification. Default is 10. - <b>`nodata`</b>: The NoData value to replace with -99999. - <b>`**kwargs`</b>: These will be passed to scikit-learn KMeans, please see full lists at: - <b>`https`</b>: //scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html. Return: Labels of classification as numpy object with 2d. Note: If the idea is to find the optimal value of 'k' (clusters or classes), k = None as an argument of the function must be put, because the function find 'k' for which the intra-class inertia is stabilized. If the 'k' value is known and the idea is to find the best algorithm embedded in kmeans (that maximizes inter-class distances), k = n, which 'n' is a specific class number, must be put. It can be greater than or equal to 0. This file was automatically generated via lazydocs .","title":"calkmeans module"},{"location":"calkmeans/#module-calkmeans","text":"","title":"module calkmeans"},{"location":"calkmeans/#function-calkmeans","text":"1 2 3 4 5 6 7 8 9 calkmeans ( image , k = None , algo = ( 'auto' , 'elkan' ), max_iter = 300 , n_iter = 10 , nodata =- 99999 , ** kwargs ) Calibrating kmeans This function allows to calibrate the kmeans algorithm. It is possible to obtain the best 'k' value and the best embedded algorithm in KMmeans. Parameters: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 - <b>`image`</b>: Optical images. It must be rasterio.io.DatasetReader with 3d. - <b>`k`</b>: k This argument is None when the objective is to obtain the best 'k' value. If the objective is to select the best algorithm embedded in kmeans, please specify a 'k' value. - <b>`max_iter`</b>: The maximum number of iterations allowed. Strictly related to KMeans. Please see - <b>`https`</b>: //scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html - <b>`algo`</b>: It can be \"auto\" and 'elkan'. \"auto\" and \"full\" are deprecated and they will be removed in Scikit-Learn 1.3. They are both aliases for \"lloyd\". - <b>`Changed in version 1.1`</b>: Renamed \u201cfull\u201d to \u201clloyd\u201d, and deprecated \u201cauto\u201d and \u201cfull\u201d. Changed \u201cauto\u201d to use \u201clloyd\u201d instead of \u201celkan\u201d. - <b>`n_iter`</b>: Iterations number to obtain the best 'k' value. 'n_iter' must be greater than the number of classes expected to be obtained in the classification. Default is 10. - <b>`nodata`</b>: The NoData value to replace with -99999. - <b>`**kwargs`</b>: These will be passed to scikit-learn KMeans, please see full lists at: - <b>`https`</b>: //scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html. Return: Labels of classification as numpy object with 2d. Note: If the idea is to find the optimal value of 'k' (clusters or classes), k = None as an argument of the function must be put, because the function find 'k' for which the intra-class inertia is stabilized. If the 'k' value is known and the idea is to find the best algorithm embedded in kmeans (that maximizes inter-class distances), k = n, which 'n' is a specific class number, must be put. It can be greater than or equal to 0. This file was automatically generated via lazydocs .","title":"function calkmeans"},{"location":"calmla/","text":"module calmla \u00b6 class calmla \u00b6 Calibrating supervised classification in Remote Sensing This module allows to calibrate supervised classification in satellite images through various algorithms and using approaches such as Set-Approach, Leave-One-Out Cross-Validation (LOOCV), Cross-Validation (k-fold) and Monte Carlo Cross-Validation (MCCV) method __init__ \u00b6 1 __init__ ( endmembers ) Parameter: endmembers: Endmembers must be a matrix (numpy.ndarray) and with more than one endmember. Rows represent the endmembers and columns represent the spectral bands. The number of bands must be equal to the number of endmembers. E.g. an image with 6 bands, endmembers dimension should be $n*6$, where $n$ is rows with the number of endmembers and 6 is the number of bands (should be equal). In addition, Endmembers must have a field (type int or float) with the names of classes to be predicted. method CV \u00b6 1 2 3 4 5 6 7 8 CV ( split_data , models = ( 'svm' , 'dt' ), k = 5 , n_iter = 10 , random_state = None , ** kwargs ) This module allows to calibrate supervised classification in satellite images through various algorithms and using Cross-Validation. Parameters: split_data : A dictionary obtained from the splitData method of this package. models : Models to be used such as Support Vector Machine ('svm'), Decision Tree ('dt'), Random Forest ('rf'), Naive Bayes ('nb') and Neural Networks ('nn'). This parameter can be passed like models = ('svm', 'dt', 'rf', 'nb', 'nn'). cv : For splitting samples into two subsets, i.e. training data and for testing data. Following Leave One Out Cross-Validation. n_iter : Number of iterations, i.e number of times the analysis is executed. **kwargs : These will be passed to SVM, DT, RF, NB and NN, please see full lists at: https : //scikit-learn.org/stable/supervised_learning.html#supervised-learning Return: method LOOCV \u00b6 1 LOOCV ( split_data , models = ( 'svm' , 'dt' ), cv = LeaveOneOut (), n_iter = 10 , ** kwargs ) This module allows to calibrate supervised classification in satellite images through various algorithms and using Leave One Out Cross-Validation. Parameters: split_data : A dictionary obtained from the splitData method of this package. models : Models to be used such as Support Vector Machine ('svm'), Decision Tree ('dt'), Random Forest ('rf'), Naive Bayes ('nb') and Neural Networks ('nn'). This parameter can be passed like models = ('svm', 'dt', 'rf', 'nb', 'nn'). cv : For splitting samples into two subsets, i.e. training data and for testing data. Following Leave One Out Cross-Validation. n_iter : Number of iterations, i.e number of times the analysis is executed. **kwargs : These will be passed to SVM, DT, RF, NB and NN, please see full lists at: https : //scikit-learn.org/stable/supervised_learning.html#supervised-learning Return: method MCCV \u00b6 1 2 3 4 5 6 7 8 9 MCCV ( split_data , models = 'svm' , train_size = 0.5 , n_splits = 5 , n_iter = 10 , random_state = None , ** kwargs ) This module allows to calibrate supervised classification in satellite images through various algorithms and using Cross-Validation. Parameters: split_data : A dictionary obtained from the splitData method of this package. models : Models to be used such as Support Vector Machine ('svm'), Decision Tree ('dt'), Random Forest ('rf'), Naive Bayes ('nb') and Neural Networks ('nn'). This parameter can be passed like models = ('svm', 'dt', 'rf', 'nb', 'nn'). cv : For splitting samples into two subsets, i.e. training data and for testing data. Following Leave One Out Cross-Validation. n_iter : Number of iterations, i.e number of times the analysis is executed. **kwargs : These will be passed to SVM, DT, RF, NB and NN, please see full lists at: https : //scikit-learn.org/stable/supervised_learning.html#supervised-learning Return: method SA \u00b6 1 SA ( split_data , models = ( 'svm' , 'dt' , 'rf' ), train_size = 0.5 , n_iter = 10 , ** kwargs ) This module allows to calibrate supervised classification in satellite images through various algorithms and using approaches such as Set-Approach. Parameters: split_data : A dictionary obtained from the splitData method of this package. models : Models to be used such as Support Vector Machine ('svm'), Decision Tree ('dt'), Random Forest ('rf'), Naive Bayes ('nb') and Neural Networks ('nn'). This parameter can be passed like models = ('svm', 'dt', 'rf', 'nb', 'nn'). train_size : For splitting samples into two subsets, i.e. training data and for testing data. n_iter : Number of iterations, i.e number of times the analysis is executed. **kwargs : These will be passed to SVM, DT, RF, NB and NN, please see full lists at: https : //scikit-learn.org/stable/supervised_learning.html#supervised-learning Return: method splitData \u00b6 1 splitData ( random_state = None ) This method is to separate the dataset in predictor variables and the variable to be predicted Parameter: self: Attributes of class calmla. Return: A dictionary with X and y. This file was automatically generated via lazydocs .","title":"calmla module"},{"location":"calmla/#module-calmla","text":"","title":"module calmla"},{"location":"calmla/#class-calmla","text":"Calibrating supervised classification in Remote Sensing This module allows to calibrate supervised classification in satellite images through various algorithms and using approaches such as Set-Approach, Leave-One-Out Cross-Validation (LOOCV), Cross-Validation (k-fold) and Monte Carlo Cross-Validation (MCCV)","title":"class calmla"},{"location":"calmla/#method-__init__","text":"1 __init__ ( endmembers ) Parameter: endmembers: Endmembers must be a matrix (numpy.ndarray) and with more than one endmember. Rows represent the endmembers and columns represent the spectral bands. The number of bands must be equal to the number of endmembers. E.g. an image with 6 bands, endmembers dimension should be $n*6$, where $n$ is rows with the number of endmembers and 6 is the number of bands (should be equal). In addition, Endmembers must have a field (type int or float) with the names of classes to be predicted.","title":"method __init__"},{"location":"calmla/#method-cv","text":"1 2 3 4 5 6 7 8 CV ( split_data , models = ( 'svm' , 'dt' ), k = 5 , n_iter = 10 , random_state = None , ** kwargs ) This module allows to calibrate supervised classification in satellite images through various algorithms and using Cross-Validation. Parameters: split_data : A dictionary obtained from the splitData method of this package. models : Models to be used such as Support Vector Machine ('svm'), Decision Tree ('dt'), Random Forest ('rf'), Naive Bayes ('nb') and Neural Networks ('nn'). This parameter can be passed like models = ('svm', 'dt', 'rf', 'nb', 'nn'). cv : For splitting samples into two subsets, i.e. training data and for testing data. Following Leave One Out Cross-Validation. n_iter : Number of iterations, i.e number of times the analysis is executed. **kwargs : These will be passed to SVM, DT, RF, NB and NN, please see full lists at: https : //scikit-learn.org/stable/supervised_learning.html#supervised-learning Return:","title":"method CV"},{"location":"calmla/#method-loocv","text":"1 LOOCV ( split_data , models = ( 'svm' , 'dt' ), cv = LeaveOneOut (), n_iter = 10 , ** kwargs ) This module allows to calibrate supervised classification in satellite images through various algorithms and using Leave One Out Cross-Validation. Parameters: split_data : A dictionary obtained from the splitData method of this package. models : Models to be used such as Support Vector Machine ('svm'), Decision Tree ('dt'), Random Forest ('rf'), Naive Bayes ('nb') and Neural Networks ('nn'). This parameter can be passed like models = ('svm', 'dt', 'rf', 'nb', 'nn'). cv : For splitting samples into two subsets, i.e. training data and for testing data. Following Leave One Out Cross-Validation. n_iter : Number of iterations, i.e number of times the analysis is executed. **kwargs : These will be passed to SVM, DT, RF, NB and NN, please see full lists at: https : //scikit-learn.org/stable/supervised_learning.html#supervised-learning Return:","title":"method LOOCV"},{"location":"calmla/#method-mccv","text":"1 2 3 4 5 6 7 8 9 MCCV ( split_data , models = 'svm' , train_size = 0.5 , n_splits = 5 , n_iter = 10 , random_state = None , ** kwargs ) This module allows to calibrate supervised classification in satellite images through various algorithms and using Cross-Validation. Parameters: split_data : A dictionary obtained from the splitData method of this package. models : Models to be used such as Support Vector Machine ('svm'), Decision Tree ('dt'), Random Forest ('rf'), Naive Bayes ('nb') and Neural Networks ('nn'). This parameter can be passed like models = ('svm', 'dt', 'rf', 'nb', 'nn'). cv : For splitting samples into two subsets, i.e. training data and for testing data. Following Leave One Out Cross-Validation. n_iter : Number of iterations, i.e number of times the analysis is executed. **kwargs : These will be passed to SVM, DT, RF, NB and NN, please see full lists at: https : //scikit-learn.org/stable/supervised_learning.html#supervised-learning Return:","title":"method MCCV"},{"location":"calmla/#method-sa","text":"1 SA ( split_data , models = ( 'svm' , 'dt' , 'rf' ), train_size = 0.5 , n_iter = 10 , ** kwargs ) This module allows to calibrate supervised classification in satellite images through various algorithms and using approaches such as Set-Approach. Parameters: split_data : A dictionary obtained from the splitData method of this package. models : Models to be used such as Support Vector Machine ('svm'), Decision Tree ('dt'), Random Forest ('rf'), Naive Bayes ('nb') and Neural Networks ('nn'). This parameter can be passed like models = ('svm', 'dt', 'rf', 'nb', 'nn'). train_size : For splitting samples into two subsets, i.e. training data and for testing data. n_iter : Number of iterations, i.e number of times the analysis is executed. **kwargs : These will be passed to SVM, DT, RF, NB and NN, please see full lists at: https : //scikit-learn.org/stable/supervised_learning.html#supervised-learning Return:","title":"method SA"},{"location":"calmla/#method-splitdata","text":"1 splitData ( random_state = None ) This method is to separate the dataset in predictor variables and the variable to be predicted Parameter: self: Attributes of class calmla. Return: A dictionary with X and y. This file was automatically generated via lazydocs .","title":"method splitData"},{"location":"changelog/","text":"Changelog \u00b6 v0.0.1 - Date \u00b6 Improvement : TBD New Features : TBD","title":"Changelog"},{"location":"changelog/#changelog","text":"","title":"Changelog"},{"location":"changelog/#v001-date","text":"Improvement : TBD New Features : TBD","title":"v0.0.1 - Date"},{"location":"contributing/","text":"Contributing \u00b6 Contributions are welcome, and they are greatly appreciated! Every little bit helps, and credit will always be given. You can contribute in many ways: Types of Contributions \u00b6 Report Bugs \u00b6 Report bugs at https://github.com/ytarazona/scikit-eo/issues . If you are reporting a bug, please include: Your operating system name and version. Any details about your local setup that might be helpful in troubleshooting. Detailed steps to reproduce the bug. Fix Bugs \u00b6 Look through the GitHub issues for bugs. Anything tagged with bug and help wanted is open to whoever wants to implement it. Implement Features \u00b6 Look through the GitHub issues for features. Anything tagged with enhancement and help wanted is open to whoever wants to implement it. Write Documentation \u00b6 scikit-eo could always use more documentation, whether as part of the official scikit-eo docs, in docstrings, or even on the web in blog posts, articles, and such. Submit Feedback \u00b6 The best way to send feedback is to file an issue at https://github.com/ytarazona/scikit-eo/issues . If you are proposing a feature: Explain in detail how it would work. Keep the scope as narrow as possible, to make it easier to implement. Remember that this is a volunteer-driven project, and that contributions are welcome :) Get Started! \u00b6 Ready to contribute? Here's how to set up scikit-eo for local development. Fork the scikit-eo repo on GitHub. Clone your fork locally: 1 $ git clone git@github.com:your_name_here/scikit-eo.git Install your local copy into a virtualenv. Assuming you have virtualenvwrapper installed, this is how you set up your fork for local development: 1 2 3 $ mkvirtualenv scikit-eo $ cd scikit-eo/ $ python setup.py develop Create a branch for local development: 1 $ git checkout -b name-of-your-bugfix-or-feature Now you can make your changes locally. When you're done making changes, check that your changes pass flake8 and the tests, including testing other Python versions with tox: 1 2 3 $ flake8 scikit-eo tests $ python setup.py test or pytest $ tox To get flake8 and tox, just pip install them into your virtualenv. Commit your changes and push your branch to GitHub: 1 2 3 $ git add . $ git commit -m \"Your detailed description of your changes.\" $ git push origin name-of-your-bugfix-or-feature Submit a pull request through the GitHub website. Pull Request Guidelines \u00b6 Before you submit a pull request, check that it meets these guidelines: The pull request should include tests. If the pull request adds functionality, the docs should be updated. Put your new functionality into a function with a docstring, and add the feature to the list in README.rst. The pull request should work for Python 3.5, 3.6, 3.7 and 3.8, and for PyPy. Check https://github.com/ytarazona/scikit-eo/pull_requests and make sure that the tests pass for all supported Python versions. 1","title":"Contributing"},{"location":"contributing/#contributing","text":"Contributions are welcome, and they are greatly appreciated! Every little bit helps, and credit will always be given. You can contribute in many ways:","title":"Contributing"},{"location":"contributing/#types-of-contributions","text":"","title":"Types of Contributions"},{"location":"contributing/#report-bugs","text":"Report bugs at https://github.com/ytarazona/scikit-eo/issues . If you are reporting a bug, please include: Your operating system name and version. Any details about your local setup that might be helpful in troubleshooting. Detailed steps to reproduce the bug.","title":"Report Bugs"},{"location":"contributing/#fix-bugs","text":"Look through the GitHub issues for bugs. Anything tagged with bug and help wanted is open to whoever wants to implement it.","title":"Fix Bugs"},{"location":"contributing/#implement-features","text":"Look through the GitHub issues for features. Anything tagged with enhancement and help wanted is open to whoever wants to implement it.","title":"Implement Features"},{"location":"contributing/#write-documentation","text":"scikit-eo could always use more documentation, whether as part of the official scikit-eo docs, in docstrings, or even on the web in blog posts, articles, and such.","title":"Write Documentation"},{"location":"contributing/#submit-feedback","text":"The best way to send feedback is to file an issue at https://github.com/ytarazona/scikit-eo/issues . If you are proposing a feature: Explain in detail how it would work. Keep the scope as narrow as possible, to make it easier to implement. Remember that this is a volunteer-driven project, and that contributions are welcome :)","title":"Submit Feedback"},{"location":"contributing/#get-started","text":"Ready to contribute? Here's how to set up scikit-eo for local development. Fork the scikit-eo repo on GitHub. Clone your fork locally: 1 $ git clone git@github.com:your_name_here/scikit-eo.git Install your local copy into a virtualenv. Assuming you have virtualenvwrapper installed, this is how you set up your fork for local development: 1 2 3 $ mkvirtualenv scikit-eo $ cd scikit-eo/ $ python setup.py develop Create a branch for local development: 1 $ git checkout -b name-of-your-bugfix-or-feature Now you can make your changes locally. When you're done making changes, check that your changes pass flake8 and the tests, including testing other Python versions with tox: 1 2 3 $ flake8 scikit-eo tests $ python setup.py test or pytest $ tox To get flake8 and tox, just pip install them into your virtualenv. Commit your changes and push your branch to GitHub: 1 2 3 $ git add . $ git commit -m \"Your detailed description of your changes.\" $ git push origin name-of-your-bugfix-or-feature Submit a pull request through the GitHub website.","title":"Get Started!"},{"location":"contributing/#pull-request-guidelines","text":"Before you submit a pull request, check that it meets these guidelines: The pull request should include tests. If the pull request adds functionality, the docs should be updated. Put your new functionality into a function with a docstring, and add the feature to the list in README.rst. The pull request should work for Python 3.5, 3.6, 3.7 and 3.8, and for PyPy. Check https://github.com/ytarazona/scikit-eo/pull_requests and make sure that the tests pass for all supported Python versions. 1","title":"Pull Request Guidelines"},{"location":"crop/","text":"module crop \u00b6 function crop \u00b6 1 crop ( image , shp , filename = None , filepath = None ) This algorithm allows to clip a raster (.tif) including a satellite image using a shapefile. Parameters: image : This parameter can be a string with the raster path (e.g., r'/home/image/b3.tif') or it can be a rasterio.io.DatasetReader type. shp : vector file, tipically shapefile. filename : The image name to be saved. filepath : The path which the image will be stored. Return: A raster in your filepath. This file was automatically generated via lazydocs .","title":"crop module"},{"location":"crop/#module-crop","text":"","title":"module crop"},{"location":"crop/#function-crop","text":"1 crop ( image , shp , filename = None , filepath = None ) This algorithm allows to clip a raster (.tif) including a satellite image using a shapefile. Parameters: image : This parameter can be a string with the raster path (e.g., r'/home/image/b3.tif') or it can be a rasterio.io.DatasetReader type. shp : vector file, tipically shapefile. filename : The image name to be saved. filepath : The path which the image will be stored. Return: A raster in your filepath. This file was automatically generated via lazydocs .","title":"function crop"},{"location":"deeplearning/","text":"module deeplearning \u00b6 class DL \u00b6 Deep Learning classification in Remote Sensing method __init__ \u00b6 1 __init__ ( image , endmembers , nodata =- 99999 ) Parameter: image: Optical images. It must be rasterio.io.DatasetReader with 3d. endmembers: Endmembers must be a matrix (numpy.ndarray) and with more than one endmember. Rows represent the endmembers and columns represent the spectral bands. The number of bands must be equal to the number of endmembers. E.g. an image with 6 bands, endmembers dimension should be $n*6$, where $n$ is rows with the number of endmembers and 6 is the number of bands (should be equal). In addition, Endmembers must have a field (type int or float) with the names of classes to be predicted. nodata: The NoData value to replace with -99999. method CNN \u00b6 1 CNN () method FullyConnected \u00b6 1 2 3 4 5 6 7 8 9 10 FullyConnected ( hidden_layers = 3 , hidden_units = [ 64 , 32 , 16 ], output_units = 10 , input_shape = ( 6 ,), epochs = 300 , batch_size = 32 , training_split = 0.8 , random_state = None ) This algorithm consiste of a network with a sequence of Dense layers, which area densely connnected (also called fully connected ) neural layers. This is the simplest of deep learning. Parameters: hidden_layers : Number of hidden layers to be used. 3 is for default. hidden_units : Number of units to be used. This is related to 'neurons' in each hidden layers. output_units : Number of clases to be obtained. input_shape : The input shape is generally the shape of the input data provided to the Keras model while training. The model cannot know the shape of the training data. The shape of other tensors(layers) is computed automatically. epochs : Number of iteration, the network will compute the gradients of the weights with regard to the loss on the batch, and update the weights accordingly. batch_size : This break the data into small batches. In deep learning, models do not process antire dataset at once. training_split : For splitting samples into two subsets, i.e. training data and for testing data. random_state : Random state ensures that the splits that you generate are reproducible. Please, see for more details https : //scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html Return: A dictionary with Labels of classification as numpy object, overall accuracy, among others results. This file was automatically generated via lazydocs .","title":"deeplearning module"},{"location":"deeplearning/#module-deeplearning","text":"","title":"module deeplearning"},{"location":"deeplearning/#class-dl","text":"Deep Learning classification in Remote Sensing","title":"class DL"},{"location":"deeplearning/#method-__init__","text":"1 __init__ ( image , endmembers , nodata =- 99999 ) Parameter: image: Optical images. It must be rasterio.io.DatasetReader with 3d. endmembers: Endmembers must be a matrix (numpy.ndarray) and with more than one endmember. Rows represent the endmembers and columns represent the spectral bands. The number of bands must be equal to the number of endmembers. E.g. an image with 6 bands, endmembers dimension should be $n*6$, where $n$ is rows with the number of endmembers and 6 is the number of bands (should be equal). In addition, Endmembers must have a field (type int or float) with the names of classes to be predicted. nodata: The NoData value to replace with -99999.","title":"method __init__"},{"location":"deeplearning/#method-cnn","text":"1 CNN ()","title":"method CNN"},{"location":"deeplearning/#method-fullyconnected","text":"1 2 3 4 5 6 7 8 9 10 FullyConnected ( hidden_layers = 3 , hidden_units = [ 64 , 32 , 16 ], output_units = 10 , input_shape = ( 6 ,), epochs = 300 , batch_size = 32 , training_split = 0.8 , random_state = None ) This algorithm consiste of a network with a sequence of Dense layers, which area densely connnected (also called fully connected ) neural layers. This is the simplest of deep learning. Parameters: hidden_layers : Number of hidden layers to be used. 3 is for default. hidden_units : Number of units to be used. This is related to 'neurons' in each hidden layers. output_units : Number of clases to be obtained. input_shape : The input shape is generally the shape of the input data provided to the Keras model while training. The model cannot know the shape of the training data. The shape of other tensors(layers) is computed automatically. epochs : Number of iteration, the network will compute the gradients of the weights with regard to the loss on the batch, and update the weights accordingly. batch_size : This break the data into small batches. In deep learning, models do not process antire dataset at once. training_split : For splitting samples into two subsets, i.e. training data and for testing data. random_state : Random state ensures that the splits that you generate are reproducible. Please, see for more details https : //scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html Return: A dictionary with Labels of classification as numpy object, overall accuracy, among others results. This file was automatically generated via lazydocs .","title":"method FullyConnected"},{"location":"faq/","text":"FAQ \u00b6","title":"FAQ"},{"location":"faq/#faq","text":"","title":"FAQ"},{"location":"fusionrs/","text":"module fusionrs \u00b6 function fusionrs \u00b6 1 fusionrs ( optical , radar , stand_varb = True , nodata =- 99999 , ** kwargs ) Fusion of images with different observation geometries through Principal Component Analysis (PCA). This algorithm allows to fusion images coming from different spectral sensors (e.g., optical-optical, optical and SAR, or SAR-SAR). It is also possible to obtain the contribution (%) of each variable in the fused image. Parameters: optical : Optical image. It must be rasterio.io.DatasetReader with 3d. radar : Radar image. It must be rasterio.io.DatasetReader with 3d. stand_varb : Logical. If stand.varb = True , the PCA is calculated using the correlation matrix (standardized variables) instead of the covariance matrix (non-standardized variables). nodata : The NoData value to replace with -99999. **kwargs : These will be passed to scikit-learn PCA, please see full lists at: https : //scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html Return: A dictionary. Note: Before executing the function, it is recommended that images coming from different sensors or from the same sensor have a co-registration. The contributions of variables in accounting for the variability in a given principal component are expressed in percentage. Variables that are correlated with PC1 (i.e., Dim.1) and PC2 (i.e., Dim.2) are the most important in explaining the variability in the data set. Variables that do not correlated with any PC or correlated with the last dimensions are variables with low contribution and might be removed to simplify the overall analysis. The contribution is a scaled version of the squared correlation between variables and component axes (or the cosine, from a geometrical point of view) --- this is used to assess the quality of the representation of the variables of the principal component, and it is computed as (cos(variable,axis)^2/total cos2 of the component)\u00d7100. References: - Tarazona, Y., Zabala, A., Pons, X., Broquetas, A., Nowosad, J., and Zurqani, H.A. Fusing Landsat and SAR data for mapping tropical deforestation through machine learning classification and the PVts-\u03b2 non-seasonal detection approach, Canadian Journal of Remote Sensing., vol. 47, no. 5, pp. 677\u2013696, Sep. 2021. This file was automatically generated via lazydocs .","title":"fusionrs module"},{"location":"fusionrs/#module-fusionrs","text":"","title":"module fusionrs"},{"location":"fusionrs/#function-fusionrs","text":"1 fusionrs ( optical , radar , stand_varb = True , nodata =- 99999 , ** kwargs ) Fusion of images with different observation geometries through Principal Component Analysis (PCA). This algorithm allows to fusion images coming from different spectral sensors (e.g., optical-optical, optical and SAR, or SAR-SAR). It is also possible to obtain the contribution (%) of each variable in the fused image. Parameters: optical : Optical image. It must be rasterio.io.DatasetReader with 3d. radar : Radar image. It must be rasterio.io.DatasetReader with 3d. stand_varb : Logical. If stand.varb = True , the PCA is calculated using the correlation matrix (standardized variables) instead of the covariance matrix (non-standardized variables). nodata : The NoData value to replace with -99999. **kwargs : These will be passed to scikit-learn PCA, please see full lists at: https : //scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html Return: A dictionary. Note: Before executing the function, it is recommended that images coming from different sensors or from the same sensor have a co-registration. The contributions of variables in accounting for the variability in a given principal component are expressed in percentage. Variables that are correlated with PC1 (i.e., Dim.1) and PC2 (i.e., Dim.2) are the most important in explaining the variability in the data set. Variables that do not correlated with any PC or correlated with the last dimensions are variables with low contribution and might be removed to simplify the overall analysis. The contribution is a scaled version of the squared correlation between variables and component axes (or the cosine, from a geometrical point of view) --- this is used to assess the quality of the representation of the variables of the principal component, and it is computed as (cos(variable,axis)^2/total cos2 of the component)\u00d7100. References: - Tarazona, Y., Zabala, A., Pons, X., Broquetas, A., Nowosad, J., and Zurqani, H.A. Fusing Landsat and SAR data for mapping tropical deforestation through machine learning classification and the PVts-\u03b2 non-seasonal detection approach, Canadian Journal of Remote Sensing., vol. 47, no. 5, pp. 677\u2013696, Sep. 2021. This file was automatically generated via lazydocs .","title":"function fusionrs"},{"location":"get-started/","text":"Get Started \u00b6 In these lines, a machine learning approach will be used to classify a satellite image. Example \u00b6 1. Applying Machine Learning \u00b6 Libraries to be used: 1 2 3 4 5 6 7 import rasterio import numpy as np from scikeo.mla import MLA import matplotlib.pyplot as plt from dbfread import DBF import matplotlib as mpl import pandas as pd 2.0 Optical image \u00b6 Landsat-8 OLI (Operational Land Imager) will be used to obtain in order to classify using Random Forest (RF). This image, which is in surface reflectance with bands: - Blue -> B2 - Green -> B3 - Red -> B4 - Nir -> B5 - Swir1 -> B6 - Swir2 -> B7 The image and signatures to be used can be downloaded here : 3.0 Supervised Classification using Random Forest \u00b6 Image and endmembers 1 2 3 4 5 path_raster = r \"C:\\data\\ml\\LC08_232066_20190727_SR.tif\" img = rasterio . open ( path_raster ) path_endm = r \"C:\\data\\ml\\endmembers.dbf\" endm = DBF ( path_endm ) 1 2 3 # endmembers df = pd . DataFrame ( iter ( endm )) df . head () Instance of mla() : 1 inst = MLA ( image = img , endmembers = endm ) Applying Random Forest: 1 rf_class = inst . SVM ( training_split = 0.7 ) 4.0 Results \u00b6 Dictionary of results 1 rf_class . keys () Overall accuracy 1 rf_class . get ( 'Overall_Accuracy' ) Kappa index 1 rf_class . get ( 'Kappa_Index' ) Confusion matrix or error matrix 1 rf_class . get ( 'Confusion_Matrix' ) Preparing the image before plotting 1 2 # Let's define the color palette palette = mpl . colors . ListedColormap ([ \"#2232F9\" , \"#F922AE\" , \"#229954\" , \"#7CED5E\" ]) Applying the plotRGB() algorithm is easy: 1 2 3 4 5 6 7 8 9 10 # Let\u00b4s plot fig , axes = plt . subplots ( nrows = 1 , ncols = 2 , figsize = ( 15 , 9 )) # satellite image plotRGB ( img , title = 'Image in Surface Reflectance' , ax = axes [ 0 ]) # class results axes [ 1 ] . imshow ( svm_class . get ( 'Classification_Map' ), cmap = palette ) axes [ 1 ] . set_title ( \"Classification map\" ) axes [ 1 ] . grid ( False )","title":"Get Started"},{"location":"get-started/#get-started","text":"In these lines, a machine learning approach will be used to classify a satellite image.","title":"Get Started"},{"location":"get-started/#example","text":"","title":"Example"},{"location":"get-started/#1-applying-machine-learning","text":"Libraries to be used: 1 2 3 4 5 6 7 import rasterio import numpy as np from scikeo.mla import MLA import matplotlib.pyplot as plt from dbfread import DBF import matplotlib as mpl import pandas as pd","title":"1. Applying Machine Learning"},{"location":"get-started/#20-optical-image","text":"Landsat-8 OLI (Operational Land Imager) will be used to obtain in order to classify using Random Forest (RF). This image, which is in surface reflectance with bands: - Blue -> B2 - Green -> B3 - Red -> B4 - Nir -> B5 - Swir1 -> B6 - Swir2 -> B7 The image and signatures to be used can be downloaded here :","title":"2.0 Optical image"},{"location":"get-started/#30-supervised-classification-using-random-forest","text":"Image and endmembers 1 2 3 4 5 path_raster = r \"C:\\data\\ml\\LC08_232066_20190727_SR.tif\" img = rasterio . open ( path_raster ) path_endm = r \"C:\\data\\ml\\endmembers.dbf\" endm = DBF ( path_endm ) 1 2 3 # endmembers df = pd . DataFrame ( iter ( endm )) df . head () Instance of mla() : 1 inst = MLA ( image = img , endmembers = endm ) Applying Random Forest: 1 rf_class = inst . SVM ( training_split = 0.7 )","title":"3.0 Supervised Classification using Random Forest"},{"location":"get-started/#40-results","text":"Dictionary of results 1 rf_class . keys () Overall accuracy 1 rf_class . get ( 'Overall_Accuracy' ) Kappa index 1 rf_class . get ( 'Kappa_Index' ) Confusion matrix or error matrix 1 rf_class . get ( 'Confusion_Matrix' ) Preparing the image before plotting 1 2 # Let's define the color palette palette = mpl . colors . ListedColormap ([ \"#2232F9\" , \"#F922AE\" , \"#229954\" , \"#7CED5E\" ]) Applying the plotRGB() algorithm is easy: 1 2 3 4 5 6 7 8 9 10 # Let\u00b4s plot fig , axes = plt . subplots ( nrows = 1 , ncols = 2 , figsize = ( 15 , 9 )) # satellite image plotRGB ( img , title = 'Image in Surface Reflectance' , ax = axes [ 0 ]) # class results axes [ 1 ] . imshow ( svm_class . get ( 'Classification_Map' ), cmap = palette ) axes [ 1 ] . set_title ( \"Classification map\" ) axes [ 1 ] . grid ( False )","title":"4.0 Results"},{"location":"installation/","text":"Installation \u00b6 Stable release \u00b6 To install scikit-eo, run this command in your terminal: 1 pip install scikit-eo This is the preferred method to install scikit-eo, as it will always install the most recent stable release. If you don't have pip installed, this Python installation guide can guide you through the process. From sources \u00b6 The sources for scikit-eo can be downloaded from the Github repo. You can clone the public repository: 1 git clone git://github.com/ytarazona/scikit-eo 1","title":"Installation"},{"location":"installation/#installation","text":"","title":"Installation"},{"location":"installation/#stable-release","text":"To install scikit-eo, run this command in your terminal: 1 pip install scikit-eo This is the preferred method to install scikit-eo, as it will always install the most recent stable release. If you don't have pip installed, this Python installation guide can guide you through the process.","title":"Stable release"},{"location":"installation/#from-sources","text":"The sources for scikit-eo can be downloaded from the Github repo. You can clone the public repository: 1 git clone git://github.com/ytarazona/scikit-eo 1","title":"From sources"},{"location":"linearTrend/","text":"module linearTrend \u00b6 class linearTrend \u00b6 Linear Trend in Remote Sensing method __init__ \u00b6 1 __init__ ( image , nodata =- 99999 ) Parameters: image : Optical images. It must be rasterio.io.DatasetReader with 3d. nodata : The NoData value to replace with -99999. method LN \u00b6 1 LN ( ** kwargs ) Linear trend is useful for mapping forest degradation, land degradation, etc. This algorithm is capable of obtaining the slope of an ordinary least-squares linear regression and its reliability (p-value). Parameters: **kwargs : These will be passed to LN, please see full lists at: https : //docs.scipy.org/doc/scipy/reference/generated/scipy.stats.linregress.html Return: a dictionary with slope, intercept and p-value obtained. All of them in numpy.ndarray with 2d. References: - Crist, E.P., R. Laurin, and R.C. Cicone. 1986. Vegetation and soils information contained in transformed Thematic Mapper data. Pages 1465-1470 Ref. ESA SP-254. - European Space Agency, Paris, France. http : //www.ciesin.org/docs/005-419/005-419.html. Baig, M.H.A., Shuai, T., Tong, Q., 2014. Derivation of a tasseled cap transformation based on Landsat 8 at-satellite reflectance. Remote Sensing Letters, 5(5), 423-431. Li, B., Ti, C., Zhao, Y., Yan, X., 2016. Estimating Soil Moisture with Landsat Data and Its Application in Extracting the Spatial Distribution of Winter Flooded Paddies. Remote Sensing, 8(1), 38. Note: Linear regression is widely used to analyze forest degradation or land degradation. Specifically, the slope and its reliability are used as main parameters and they can be obtained with this function. On the other hand, logistic regression allows obtaining a degradation risk map, in other words, it is a probability map. References: - Tarazona, Y., Maria, Miyasiro-Lopez. (2020). Monitoring tropical forest degradation using remote sensing. Challenges and opportunities in the Madre de Dios region, Peru. Remote Sensing Applications: Society and Environment, 19, 100337. - Wilkinson, G.N., Rogers, C.E., 1973. Symbolic descriptions of factorial models for analysis of variance. Appl. Stat. 22, 392-399. - Chambers, J.M., 1992. Statistical Models in S. CRS Press. method MLN \u00b6 1 MLN ( ** kwargs ) This file was automatically generated via lazydocs .","title":"linearTrend module"},{"location":"linearTrend/#module-lineartrend","text":"","title":"module linearTrend"},{"location":"linearTrend/#class-lineartrend","text":"Linear Trend in Remote Sensing","title":"class linearTrend"},{"location":"linearTrend/#method-__init__","text":"1 __init__ ( image , nodata =- 99999 ) Parameters: image : Optical images. It must be rasterio.io.DatasetReader with 3d. nodata : The NoData value to replace with -99999.","title":"method __init__"},{"location":"linearTrend/#method-ln","text":"1 LN ( ** kwargs ) Linear trend is useful for mapping forest degradation, land degradation, etc. This algorithm is capable of obtaining the slope of an ordinary least-squares linear regression and its reliability (p-value). Parameters: **kwargs : These will be passed to LN, please see full lists at: https : //docs.scipy.org/doc/scipy/reference/generated/scipy.stats.linregress.html Return: a dictionary with slope, intercept and p-value obtained. All of them in numpy.ndarray with 2d. References: - Crist, E.P., R. Laurin, and R.C. Cicone. 1986. Vegetation and soils information contained in transformed Thematic Mapper data. Pages 1465-1470 Ref. ESA SP-254. - European Space Agency, Paris, France. http : //www.ciesin.org/docs/005-419/005-419.html. Baig, M.H.A., Shuai, T., Tong, Q., 2014. Derivation of a tasseled cap transformation based on Landsat 8 at-satellite reflectance. Remote Sensing Letters, 5(5), 423-431. Li, B., Ti, C., Zhao, Y., Yan, X., 2016. Estimating Soil Moisture with Landsat Data and Its Application in Extracting the Spatial Distribution of Winter Flooded Paddies. Remote Sensing, 8(1), 38. Note: Linear regression is widely used to analyze forest degradation or land degradation. Specifically, the slope and its reliability are used as main parameters and they can be obtained with this function. On the other hand, logistic regression allows obtaining a degradation risk map, in other words, it is a probability map. References: - Tarazona, Y., Maria, Miyasiro-Lopez. (2020). Monitoring tropical forest degradation using remote sensing. Challenges and opportunities in the Madre de Dios region, Peru. Remote Sensing Applications: Society and Environment, 19, 100337. - Wilkinson, G.N., Rogers, C.E., 1973. Symbolic descriptions of factorial models for analysis of variance. Appl. Stat. 22, 392-399. - Chambers, J.M., 1992. Statistical Models in S. CRS Press.","title":"method LN"},{"location":"linearTrend/#method-mln","text":"1 MLN ( ** kwargs ) This file was automatically generated via lazydocs .","title":"method MLN"},{"location":"mla/","text":"module mla \u00b6 class MLA \u00b6 Supervised classification in Remote Sensing method __init__ \u00b6 1 __init__ ( image , endmembers , nodata =- 99999 ) Parameter: image: Optical images. It must be rasterio.io.DatasetReader with 3d. endmembers: Endmembers must be a matrix (numpy.ndarray) and with more than one endmember. Rows represent the endmembers and columns represent the spectral bands. The number of bands must be equal to the number of endmembers. E.g. an image with 6 bands, endmembers dimension should be $n*6$, where $n$ is rows with the number of endmembers and 6 is the number of bands (should be equal). In addition, Endmembers must have a field (type int or float) with the names of classes to be predicted. nodata: The NoData value to replace with -99999. method DT \u00b6 1 DT ( training_split = 0.8 , random_state = None , ** kwargs ) Decision Tree is also a supervised non-parametric statistical learning technique, where the input data is divided recursively into branches depending on certain decision thresholds until the data are segmented into homogeneous subgroups. This technique has substantial advantages for remote sensing classification problems due to its flexibility, intuitive simplicity, and computational efficiency. DT support raster data read by rasterio (rasterio.io.DatasetReader) as input. Parameters: training_split : For splitting samples into two subsets, i.e. training data and for testing data. **kwargs : These will be passed to DT, please see full lists at: https : //scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier Return: A dictionary containing labels of classification as numpy object, overall accuracy, kappa index, confusion matrix. method NB \u00b6 1 NB ( training_split = 0.8 , random_state = None , ** kwargs ) Naive Bayes classifier is an effective and simple method for image classification based on probability theory. The NB classifier assumes an underlying probabilistic model and captures the uncertainty about the model in a principled way, that is, by calculating the occurrence probabilities of different attribute values for different classes in a training set. NB support raster data read by rasterio (rasterio.io.DatasetReader) as input. Parameters: training_split : For splitting samples into two subsets, i.e. training data and for testing data. **kwargs : These will be passed to SVM, please see full lists at: https : //scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html Return: A dictionary containing labels of classification as numpy object, overall accuracy, kappa index, confusion matrix. method NN \u00b6 1 NN ( training_split = 0.8 , max_iter = 300 , random_state = None , ** kwargs ) This classification consists of a neural network that is organized into several layers, that is, an input layer of predictor variables, one or more layers of hidden nodes, in which each node represents an activation function acting on a weighted input of the previous layers\u2019 outputs, and an output layer. NN support raster data read by rasterio (rasterio.io.DatasetReader) as input. Parameters: training_split : For splitting samples into two subsets, i.e. training data and for testing data. **kwargs : These will be passed to SVM, please see full lists at: https : //scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html Return: A dictionary containing labels of classification as numpy object, overall accuracy, kappa index, confusion matrix. method RF \u00b6 1 RF ( training_split = 0.8 , random_state = None , ** kwargs ) Random Forest is a derivative of Decision Tree which provides an improvement over DT to overcome the weaknesses of a single DT. The prediction model of the RF classifier only requires two parameters to be identified: the number of classification trees desired, known as \u201cntree,\u201d and the number of prediction variables, known as \u201cmtry,\u201d used in each node to make the tree grow. RF support raster data read by rasterio (rasterio.io.DatasetReader) as input. Parameters: training_split : For splitting samples into two subsets, i.e. training data and for testing data. **kwargs : These will be passed to RF, please see full lists at: https : //scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html Return: A dictionary containing labels of classification as numpy object, overall accuracy, kappa index, confusion matrix. method SVM \u00b6 1 SVM ( training_split = 0.8 , random_state = None , kernel = 'linear' , ** kwargs ) The Support Vector Machine (SVM) classifier is a supervised non-parametric statistical learning technique that does not assume a preliminary distribution of input data. Its discrimination criterion is a hyperplane that separates the classes in the multidimensional space in which the samples that have established the same classes are located, generally some training areas. SVM support raster data read by rasterio (rasterio.io.DatasetReader) as input. Parameters: training_split : For splitting samples into two subsets, i.e. training data and for testing data. kernel : {'linear', 'poly', 'rbf', 'sigmoid', 'precomputed'}, default='rbf' Specifies the kernel type to be used in the algorithm. It must be one of 'linear', 'poly', 'rbf', 'sigmoid', 'precomputed' or a callable. If None is given, 'rbf' will be used. See https : //scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC for more details. **kwargs : These will be passed to SVM, please see full lists at: https : //scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC Return: A dictionary containing labels of classification as numpy object, overall accuracy, kappa index, confusion matrix. This file was automatically generated via lazydocs .","title":"mla module"},{"location":"mla/#module-mla","text":"","title":"module mla"},{"location":"mla/#class-mla","text":"Supervised classification in Remote Sensing","title":"class MLA"},{"location":"mla/#method-__init__","text":"1 __init__ ( image , endmembers , nodata =- 99999 ) Parameter: image: Optical images. It must be rasterio.io.DatasetReader with 3d. endmembers: Endmembers must be a matrix (numpy.ndarray) and with more than one endmember. Rows represent the endmembers and columns represent the spectral bands. The number of bands must be equal to the number of endmembers. E.g. an image with 6 bands, endmembers dimension should be $n*6$, where $n$ is rows with the number of endmembers and 6 is the number of bands (should be equal). In addition, Endmembers must have a field (type int or float) with the names of classes to be predicted. nodata: The NoData value to replace with -99999.","title":"method __init__"},{"location":"mla/#method-dt","text":"1 DT ( training_split = 0.8 , random_state = None , ** kwargs ) Decision Tree is also a supervised non-parametric statistical learning technique, where the input data is divided recursively into branches depending on certain decision thresholds until the data are segmented into homogeneous subgroups. This technique has substantial advantages for remote sensing classification problems due to its flexibility, intuitive simplicity, and computational efficiency. DT support raster data read by rasterio (rasterio.io.DatasetReader) as input. Parameters: training_split : For splitting samples into two subsets, i.e. training data and for testing data. **kwargs : These will be passed to DT, please see full lists at: https : //scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier Return: A dictionary containing labels of classification as numpy object, overall accuracy, kappa index, confusion matrix.","title":"method DT"},{"location":"mla/#method-nb","text":"1 NB ( training_split = 0.8 , random_state = None , ** kwargs ) Naive Bayes classifier is an effective and simple method for image classification based on probability theory. The NB classifier assumes an underlying probabilistic model and captures the uncertainty about the model in a principled way, that is, by calculating the occurrence probabilities of different attribute values for different classes in a training set. NB support raster data read by rasterio (rasterio.io.DatasetReader) as input. Parameters: training_split : For splitting samples into two subsets, i.e. training data and for testing data. **kwargs : These will be passed to SVM, please see full lists at: https : //scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html Return: A dictionary containing labels of classification as numpy object, overall accuracy, kappa index, confusion matrix.","title":"method NB"},{"location":"mla/#method-nn","text":"1 NN ( training_split = 0.8 , max_iter = 300 , random_state = None , ** kwargs ) This classification consists of a neural network that is organized into several layers, that is, an input layer of predictor variables, one or more layers of hidden nodes, in which each node represents an activation function acting on a weighted input of the previous layers\u2019 outputs, and an output layer. NN support raster data read by rasterio (rasterio.io.DatasetReader) as input. Parameters: training_split : For splitting samples into two subsets, i.e. training data and for testing data. **kwargs : These will be passed to SVM, please see full lists at: https : //scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html Return: A dictionary containing labels of classification as numpy object, overall accuracy, kappa index, confusion matrix.","title":"method NN"},{"location":"mla/#method-rf","text":"1 RF ( training_split = 0.8 , random_state = None , ** kwargs ) Random Forest is a derivative of Decision Tree which provides an improvement over DT to overcome the weaknesses of a single DT. The prediction model of the RF classifier only requires two parameters to be identified: the number of classification trees desired, known as \u201cntree,\u201d and the number of prediction variables, known as \u201cmtry,\u201d used in each node to make the tree grow. RF support raster data read by rasterio (rasterio.io.DatasetReader) as input. Parameters: training_split : For splitting samples into two subsets, i.e. training data and for testing data. **kwargs : These will be passed to RF, please see full lists at: https : //scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html Return: A dictionary containing labels of classification as numpy object, overall accuracy, kappa index, confusion matrix.","title":"method RF"},{"location":"mla/#method-svm","text":"1 SVM ( training_split = 0.8 , random_state = None , kernel = 'linear' , ** kwargs ) The Support Vector Machine (SVM) classifier is a supervised non-parametric statistical learning technique that does not assume a preliminary distribution of input data. Its discrimination criterion is a hyperplane that separates the classes in the multidimensional space in which the samples that have established the same classes are located, generally some training areas. SVM support raster data read by rasterio (rasterio.io.DatasetReader) as input. Parameters: training_split : For splitting samples into two subsets, i.e. training data and for testing data. kernel : {'linear', 'poly', 'rbf', 'sigmoid', 'precomputed'}, default='rbf' Specifies the kernel type to be used in the algorithm. It must be one of 'linear', 'poly', 'rbf', 'sigmoid', 'precomputed' or a callable. If None is given, 'rbf' will be used. See https : //scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC for more details. **kwargs : These will be passed to SVM, please see full lists at: https : //scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC Return: A dictionary containing labels of classification as numpy object, overall accuracy, kappa index, confusion matrix. This file was automatically generated via lazydocs .","title":"method SVM"},{"location":"pca/","text":"module pca \u00b6 function PCA \u00b6 1 PCA ( image , stand_varb = True , nodata =- 99999 , ** kwargs ) Runing Principal Component Analysis (PCA) with satellite images. This algorithm allows to obtain Principal Components from images either radar or optical coming from different spectral sensors. It is also possible to obtain the contribution (%) of each variable. Parameters: images : Optical or radar image, it must be rasterio.io.DatasetReader with 3d. stand_varb : Logical. If stand.varb = True , the PCA is calculated using the correlation matrix (standardized variables) instead of the covariance matrix (non-standardized variables). nodata : The NoData value to replace with -99999. **kwargs : These will be passed to scikit-learn PCA, please see full lists at: https : //scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html Return: A dictionary. Note: The contributions of variables in accounting for the variability in a given principal component are expressed in percentage. Variables that are correlated with PC1 (i.e., Dim.1) and PC2 (i.e., Dim.2) are the most important in explaining the variability in the data set. Variables that do not correlated with any PC or correlated with the last dimensions are variables with low contribution and might be removed to simplify the overall analysis. The contribution is a scaled version of the squared correlation between variables and component axes (or the cosine, from a geometrical point of view) --- this is used to assess the quality of the representation of the variables of the principal component, and it is computed as (cos(variable,axis)^2/total cos2 of the component)\u00d7100. This file was automatically generated via lazydocs .","title":"pca module"},{"location":"pca/#module-pca","text":"","title":"module pca"},{"location":"pca/#function-pca","text":"1 PCA ( image , stand_varb = True , nodata =- 99999 , ** kwargs ) Runing Principal Component Analysis (PCA) with satellite images. This algorithm allows to obtain Principal Components from images either radar or optical coming from different spectral sensors. It is also possible to obtain the contribution (%) of each variable. Parameters: images : Optical or radar image, it must be rasterio.io.DatasetReader with 3d. stand_varb : Logical. If stand.varb = True , the PCA is calculated using the correlation matrix (standardized variables) instead of the covariance matrix (non-standardized variables). nodata : The NoData value to replace with -99999. **kwargs : These will be passed to scikit-learn PCA, please see full lists at: https : //scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html Return: A dictionary. Note: The contributions of variables in accounting for the variability in a given principal component are expressed in percentage. Variables that are correlated with PC1 (i.e., Dim.1) and PC2 (i.e., Dim.2) are the most important in explaining the variability in the data set. Variables that do not correlated with any PC or correlated with the last dimensions are variables with low contribution and might be removed to simplify the overall analysis. The contribution is a scaled version of the squared correlation between variables and component axes (or the cosine, from a geometrical point of view) --- this is used to assess the quality of the representation of the variables of the principal component, and it is computed as (cos(variable,axis)^2/total cos2 of the component)\u00d7100. This file was automatically generated via lazydocs .","title":"function PCA"},{"location":"plot/","text":"module plot \u00b6 function plotRGB \u00b6 1 2 3 4 5 6 7 8 9 10 plotRGB ( image , bands = [ 4 , 3 , 2 ], stretch = 'std' , title = None , xlabel = None , ylabel = None , ax = None , ** kwargs ) Plotting an image in RGB This function allows to plot an satellite image in RGB channels. Parameters: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 - <b>`image`</b>: Optical images. It must be rasterio.io.DatasetReader with 3d. - <b>`bands`</b>: A list contain the order of bands to be used in order to plot in RGB. For example, for six bands (blue, green, red, nir, swir1 and swir2), number four (4) indicates the swir1 band, number three (3) indicates the nir band and the number two (2) indicates the red band. - <b>`stretch`</b>: Contrast enhancement using the histogram. There are two options here: i) using standard deviation ('std') and ii) using percentiles ('per'). For default is 'std', which means standard deviation. - <b>`title`</b>: Assigned title. - <b>`xlabel`</b>: X axis title. - <b>`ylabel`</b>: Y axis title. - <b>`ax`</b>: current axes - <b>`**kwargs`</b>: These will be passed to the matplotlib imshow(), please see full lists at: - <b>`https`</b>: //matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.imshow.html Return: 1 - <b>`ax `</b>: Graphic of change detection using the matplotlib plot function. This file was automatically generated via lazydocs .","title":"plot module"},{"location":"plot/#module-plot","text":"","title":"module plot"},{"location":"plot/#function-plotrgb","text":"1 2 3 4 5 6 7 8 9 10 plotRGB ( image , bands = [ 4 , 3 , 2 ], stretch = 'std' , title = None , xlabel = None , ylabel = None , ax = None , ** kwargs ) Plotting an image in RGB This function allows to plot an satellite image in RGB channels. Parameters: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 - <b>`image`</b>: Optical images. It must be rasterio.io.DatasetReader with 3d. - <b>`bands`</b>: A list contain the order of bands to be used in order to plot in RGB. For example, for six bands (blue, green, red, nir, swir1 and swir2), number four (4) indicates the swir1 band, number three (3) indicates the nir band and the number two (2) indicates the red band. - <b>`stretch`</b>: Contrast enhancement using the histogram. There are two options here: i) using standard deviation ('std') and ii) using percentiles ('per'). For default is 'std', which means standard deviation. - <b>`title`</b>: Assigned title. - <b>`xlabel`</b>: X axis title. - <b>`ylabel`</b>: Y axis title. - <b>`ax`</b>: current axes - <b>`**kwargs`</b>: These will be passed to the matplotlib imshow(), please see full lists at: - <b>`https`</b>: //matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.imshow.html Return: 1 - <b>`ax `</b>: Graphic of change detection using the matplotlib plot function. This file was automatically generated via lazydocs .","title":"function plotRGB"},{"location":"rkmeans/","text":"module rkmeans \u00b6 function rkmeans \u00b6 1 rkmeans ( image , k , nodata =- 99999 , ** kwargs ) This function allows to classify satellite images using k-means In principle, this function allows to classify satellite images specifying a k value (clusters), however it is recommended to find the optimal value of k using the calkmeans function embedded in this package. Parameters: image : Optical images. It must be rasterio.io.DatasetReader with 3d. k : The number of clusters to be detected. nodata : The NoData value to replace with -99999. **kwargs : These will be passed to scikit-learn KMeans, please see full lists at: https : //scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html Return: Labels of classification as numpy object with 2d. This file was automatically generated via lazydocs .","title":"rkmeans module"},{"location":"rkmeans/#module-rkmeans","text":"","title":"module rkmeans"},{"location":"rkmeans/#function-rkmeans","text":"1 rkmeans ( image , k , nodata =- 99999 , ** kwargs ) This function allows to classify satellite images using k-means In principle, this function allows to classify satellite images specifying a k value (clusters), however it is recommended to find the optimal value of k using the calkmeans function embedded in this package. Parameters: image : Optical images. It must be rasterio.io.DatasetReader with 3d. k : The number of clusters to be detected. nodata : The NoData value to replace with -99999. **kwargs : These will be passed to scikit-learn KMeans, please see full lists at: https : //scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html Return: Labels of classification as numpy object with 2d. This file was automatically generated via lazydocs .","title":"function rkmeans"},{"location":"sma/","text":"module sma \u00b6 function sma \u00b6 1 sma ( image , endmembers , nodata =- 99999 ) The SMA assumes that the energy received within the field of vision of the remote sensor can be considered as the sum of the energies received from each dominant endmember. This function addresses a Linear Mixing Model. A regression analysis is used to obtain the fractions. In least squares inversion algorithms, the common objective is to estimate abundances that minimize the squared error between the actual spectrum and the estimated spectrum. The values of the fractions will be between 0 and 1. Parameters: image : Optical images. It must be rasterio.io.DatasetReader with 3d. endmembers : Endmembers must be a matrix (numpy.ndarray) and with more than one endmember. Rows represent the endmembers and columns represent the spectral bands. The number of bands must be greater than the number of endmembers. E.g. an image with 6 bands, endmembers dimension should be $n*6$, where $n$ is rows with the number of endmembers and 6 is the number of bands (should be equal). nodata : The NoData value to replace with -99999. Return: numpy.ndarray with 2d. References Adams, J. B., Smith, M. O., & Gillespie, A. R. (1993). Imaging spectroscopy: Interpretation based on spectral mixture analysis. In C. M. Pieters & P. Englert (Eds.), Remote geochemical analysis: Elements and mineralogical composition. NY: Cambridge Univ. Press 145-166 pp. Shimabukuro, Y.E. and Smith, J., (1991). The least squares mixing models to generate fraction images derived from remote sensing multispectral data. IEEE Transactions on Geoscience and Remote Sensing, 29, pp. 16-21. Note: A regression analysis is used to obtain the fractions. In least squares inversion algorithms, the common objective is to estimate abundances that minimize the squared error between the actual spectrum and the estimated spectrum. The values of the fractions will be between 0 and 1. This file was automatically generated via lazydocs .","title":"sma module"},{"location":"sma/#module-sma","text":"","title":"module sma"},{"location":"sma/#function-sma","text":"1 sma ( image , endmembers , nodata =- 99999 ) The SMA assumes that the energy received within the field of vision of the remote sensor can be considered as the sum of the energies received from each dominant endmember. This function addresses a Linear Mixing Model. A regression analysis is used to obtain the fractions. In least squares inversion algorithms, the common objective is to estimate abundances that minimize the squared error between the actual spectrum and the estimated spectrum. The values of the fractions will be between 0 and 1. Parameters: image : Optical images. It must be rasterio.io.DatasetReader with 3d. endmembers : Endmembers must be a matrix (numpy.ndarray) and with more than one endmember. Rows represent the endmembers and columns represent the spectral bands. The number of bands must be greater than the number of endmembers. E.g. an image with 6 bands, endmembers dimension should be $n*6$, where $n$ is rows with the number of endmembers and 6 is the number of bands (should be equal). nodata : The NoData value to replace with -99999. Return: numpy.ndarray with 2d. References Adams, J. B., Smith, M. O., & Gillespie, A. R. (1993). Imaging spectroscopy: Interpretation based on spectral mixture analysis. In C. M. Pieters & P. Englert (Eds.), Remote geochemical analysis: Elements and mineralogical composition. NY: Cambridge Univ. Press 145-166 pp. Shimabukuro, Y.E. and Smith, J., (1991). The least squares mixing models to generate fraction images derived from remote sensing multispectral data. IEEE Transactions on Geoscience and Remote Sensing, 29, pp. 16-21. Note: A regression analysis is used to obtain the fractions. In least squares inversion algorithms, the common objective is to estimate abundances that minimize the squared error between the actual spectrum and the estimated spectrum. The values of the fractions will be between 0 and 1. This file was automatically generated via lazydocs .","title":"function sma"},{"location":"tassCap/","text":"module tassCap \u00b6 function tassCap \u00b6 1 tassCap ( image , sat = 'Landsat8OLI' , nodata =- 99999 , scale = None ) The Tasseled-Cap Transformation is a linear transformation method for various remote sensing data. Not only can it perform volume data compression, but it can also provide parameters associated with the physical characteristics, such as brightness, greenness and wetness indices. Parameters: image : Optical images. It must be rasterio.io.DatasetReader with 3d. sat : Specify satellite and sensor type (Landsat5TM, Landsat7ETM or Landsat8OLI). nodata : The NoData value to replace with -99999. scale : Conversion of coefficients values Return: numpy.ndarray with 3d containing brightness, greenness and wetness indices. References: Crist, E.P., R. Laurin, and R.C. Cicone. 1986. Vegetation and soils information contained in transformed Thematic Mapper data. Pages 1465-1470 Ref. ESA SP-254. European Space Agency, Paris, France. http : //www.ciesin.org/docs/005-419/005-419.html. Baig, M.H.A., Shuai, T., Tong, Q., 2014. Derivation of a tasseled cap transformation based on Landsat 8 at-satellite reflectance. Remote Sensing Letters, 5(5), 423-431. Li, B., Ti, C., Zhao, Y., Yan, X., 2016. Estimating Soil Moisture with Landsat Data and Its Application in Extracting the Spatial Distribution of Winter Flooded Paddies. Remote Sensing, 8(1), 38. Note: Currently implemented for satellites such as Landsat-4 TM, Landsat-5 TM, Landsat-7 ETM+, Landsat-8 OLI and Sentinel2. The input data must be in top of atmosphere reflectance (toa). Bands required as input must be ordered as: Consider using the following satellite bands: =============== ================================ Type of Sensor Name of bands =============== ================================ Landsat4TM :blue, green, red, nir, swir1, swir2 Landsat5TM :blue, green, red, nir, swir1, swir2 Landsat7ETM+ :blue, green, red, nir, swir1, swir2 Landsat8OLI :blue, green, red, nir, swir1, swir2 Landsat8OLI-Li2016 :coastal, blue, green, red, nir, swir1, swir2 Sentinel2MSI :coastal, blue, green, red, nir-1, mir-1, mir-2 This file was automatically generated via lazydocs .","title":"tassCap module"},{"location":"tassCap/#module-tasscap","text":"","title":"module tassCap"},{"location":"tassCap/#function-tasscap","text":"1 tassCap ( image , sat = 'Landsat8OLI' , nodata =- 99999 , scale = None ) The Tasseled-Cap Transformation is a linear transformation method for various remote sensing data. Not only can it perform volume data compression, but it can also provide parameters associated with the physical characteristics, such as brightness, greenness and wetness indices. Parameters: image : Optical images. It must be rasterio.io.DatasetReader with 3d. sat : Specify satellite and sensor type (Landsat5TM, Landsat7ETM or Landsat8OLI). nodata : The NoData value to replace with -99999. scale : Conversion of coefficients values Return: numpy.ndarray with 3d containing brightness, greenness and wetness indices. References: Crist, E.P., R. Laurin, and R.C. Cicone. 1986. Vegetation and soils information contained in transformed Thematic Mapper data. Pages 1465-1470 Ref. ESA SP-254. European Space Agency, Paris, France. http : //www.ciesin.org/docs/005-419/005-419.html. Baig, M.H.A., Shuai, T., Tong, Q., 2014. Derivation of a tasseled cap transformation based on Landsat 8 at-satellite reflectance. Remote Sensing Letters, 5(5), 423-431. Li, B., Ti, C., Zhao, Y., Yan, X., 2016. Estimating Soil Moisture with Landsat Data and Its Application in Extracting the Spatial Distribution of Winter Flooded Paddies. Remote Sensing, 8(1), 38. Note: Currently implemented for satellites such as Landsat-4 TM, Landsat-5 TM, Landsat-7 ETM+, Landsat-8 OLI and Sentinel2. The input data must be in top of atmosphere reflectance (toa). Bands required as input must be ordered as: Consider using the following satellite bands: =============== ================================ Type of Sensor Name of bands =============== ================================ Landsat4TM :blue, green, red, nir, swir1, swir2 Landsat5TM :blue, green, red, nir, swir1, swir2 Landsat7ETM+ :blue, green, red, nir, swir1, swir2 Landsat8OLI :blue, green, red, nir, swir1, swir2 Landsat8OLI-Li2016 :coastal, blue, green, red, nir, swir1, swir2 Sentinel2MSI :coastal, blue, green, red, nir-1, mir-1, mir-2 This file was automatically generated via lazydocs .","title":"function tassCap"},{"location":"usage/","text":"Usage \u00b6 To use scikit-eo in a project: 1 import scikit-eo 1","title":"Usage"},{"location":"usage/#usage","text":"To use scikit-eo in a project: 1 import scikit-eo 1","title":"Usage"},{"location":"writeRaster/","text":"module writeRaster \u00b6 function writeRaster \u00b6 1 writeRaster ( arr , image , filename = None , filepath = None , n = 1 ) This algorithm allows to save array images to raster format (.tif). Parameters: arr : Array object with 2d (rows and cols) or 3d (rows, cols, bands). image : Optical images. It must be read by rasterio.open(). filename : The image name to be saved. filepath : The path which the image will be stored. n : Number of images to be saved. Return: A raster in your filepath. This file was automatically generated via lazydocs .","title":"writeRaster module"},{"location":"writeRaster/#module-writeraster","text":"","title":"module writeRaster"},{"location":"writeRaster/#function-writeraster","text":"1 writeRaster ( arr , image , filename = None , filepath = None , n = 1 ) This algorithm allows to save array images to raster format (.tif). Parameters: arr : Array object with 2d (rows and cols) or 3d (rows, cols, bands). image : Optical images. It must be read by rasterio.open(). filename : The image name to be saved. filepath : The path which the image will be stored. n : Number of images to be saved. Return: A raster in your filepath. This file was automatically generated via lazydocs .","title":"function writeRaster"}]}
>>>>>>> Stashed changes
